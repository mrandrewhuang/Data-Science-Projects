{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Used Car Estimator (Numerical Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rusty Bargain, a used car sales service, is developing an app to attract new customers. In that app, users can quickly find out the market value of their car. We have access to historical data: technical specifications, trim versions, and prices. We need to build the model to determine the value.\n",
    "\n",
    "Our client is interested in:\n",
    "\n",
    "- the quality of the prediction;\n",
    "- the speed of the prediction;\n",
    "- the time required for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('datasets/car_data.csv')\n",
    "except:\n",
    "    df = pd.read_csv('/datasets/car_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateCrawled</th>\n",
       "      <th>Price</th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Gearbox</th>\n",
       "      <th>Power</th>\n",
       "      <th>Model</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <th>FuelType</th>\n",
       "      <th>Brand</th>\n",
       "      <th>NotRepaired</th>\n",
       "      <th>DateCreated</th>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>LastSeen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24/03/2016 11:52</td>\n",
       "      <td>480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>70435</td>\n",
       "      <td>07/04/2016 03:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24/03/2016 10:58</td>\n",
       "      <td>18300</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manual</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125000</td>\n",
       "      <td>5</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>audi</td>\n",
       "      <td>yes</td>\n",
       "      <td>24/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>66954</td>\n",
       "      <td>07/04/2016 01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14/03/2016 12:52</td>\n",
       "      <td>9800</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>auto</td>\n",
       "      <td>163</td>\n",
       "      <td>grand</td>\n",
       "      <td>125000</td>\n",
       "      <td>8</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>jeep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>90480</td>\n",
       "      <td>05/04/2016 12:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17/03/2016 16:54</td>\n",
       "      <td>1500</td>\n",
       "      <td>small</td>\n",
       "      <td>2001</td>\n",
       "      <td>manual</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>6</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "      <td>17/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>91074</td>\n",
       "      <td>17/03/2016 17:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31/03/2016 17:25</td>\n",
       "      <td>3600</td>\n",
       "      <td>small</td>\n",
       "      <td>2008</td>\n",
       "      <td>manual</td>\n",
       "      <td>69</td>\n",
       "      <td>fabia</td>\n",
       "      <td>90000</td>\n",
       "      <td>7</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>skoda</td>\n",
       "      <td>no</td>\n",
       "      <td>31/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>60437</td>\n",
       "      <td>06/04/2016 10:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>04/04/2016 17:36</td>\n",
       "      <td>650</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1995</td>\n",
       "      <td>manual</td>\n",
       "      <td>102</td>\n",
       "      <td>3er</td>\n",
       "      <td>150000</td>\n",
       "      <td>10</td>\n",
       "      <td>petrol</td>\n",
       "      <td>bmw</td>\n",
       "      <td>yes</td>\n",
       "      <td>04/04/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>33775</td>\n",
       "      <td>06/04/2016 19:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01/04/2016 20:48</td>\n",
       "      <td>2200</td>\n",
       "      <td>convertible</td>\n",
       "      <td>2004</td>\n",
       "      <td>manual</td>\n",
       "      <td>109</td>\n",
       "      <td>2_reihe</td>\n",
       "      <td>150000</td>\n",
       "      <td>8</td>\n",
       "      <td>petrol</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>no</td>\n",
       "      <td>01/04/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>67112</td>\n",
       "      <td>05/04/2016 18:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21/03/2016 18:54</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1980</td>\n",
       "      <td>manual</td>\n",
       "      <td>50</td>\n",
       "      <td>other</td>\n",
       "      <td>40000</td>\n",
       "      <td>7</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "      <td>21/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>19348</td>\n",
       "      <td>25/03/2016 16:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>04/04/2016 23:42</td>\n",
       "      <td>14500</td>\n",
       "      <td>bus</td>\n",
       "      <td>2014</td>\n",
       "      <td>manual</td>\n",
       "      <td>125</td>\n",
       "      <td>c_max</td>\n",
       "      <td>30000</td>\n",
       "      <td>8</td>\n",
       "      <td>petrol</td>\n",
       "      <td>ford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04/04/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>94505</td>\n",
       "      <td>04/04/2016 23:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17/03/2016 10:53</td>\n",
       "      <td>999</td>\n",
       "      <td>small</td>\n",
       "      <td>1998</td>\n",
       "      <td>manual</td>\n",
       "      <td>101</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/03/2016 00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>27472</td>\n",
       "      <td>31/03/2016 17:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DateCrawled  Price  VehicleType  RegistrationYear Gearbox  Power  \\\n",
       "0  24/03/2016 11:52    480          NaN              1993  manual      0   \n",
       "1  24/03/2016 10:58  18300        coupe              2011  manual    190   \n",
       "2  14/03/2016 12:52   9800          suv              2004    auto    163   \n",
       "3  17/03/2016 16:54   1500        small              2001  manual     75   \n",
       "4  31/03/2016 17:25   3600        small              2008  manual     69   \n",
       "5  04/04/2016 17:36    650        sedan              1995  manual    102   \n",
       "6  01/04/2016 20:48   2200  convertible              2004  manual    109   \n",
       "7  21/03/2016 18:54      0        sedan              1980  manual     50   \n",
       "8  04/04/2016 23:42  14500          bus              2014  manual    125   \n",
       "9  17/03/2016 10:53    999        small              1998  manual    101   \n",
       "\n",
       "     Model  Mileage  RegistrationMonth  FuelType       Brand NotRepaired  \\\n",
       "0     golf   150000                  0    petrol  volkswagen         NaN   \n",
       "1      NaN   125000                  5  gasoline        audi         yes   \n",
       "2    grand   125000                  8  gasoline        jeep         NaN   \n",
       "3     golf   150000                  6    petrol  volkswagen          no   \n",
       "4    fabia    90000                  7  gasoline       skoda          no   \n",
       "5      3er   150000                 10    petrol         bmw         yes   \n",
       "6  2_reihe   150000                  8    petrol     peugeot          no   \n",
       "7    other    40000                  7    petrol  volkswagen          no   \n",
       "8    c_max    30000                  8    petrol        ford         NaN   \n",
       "9     golf   150000                  0       NaN  volkswagen         NaN   \n",
       "\n",
       "        DateCreated  NumberOfPictures  PostalCode          LastSeen  \n",
       "0  24/03/2016 00:00                 0       70435  07/04/2016 03:16  \n",
       "1  24/03/2016 00:00                 0       66954  07/04/2016 01:46  \n",
       "2  14/03/2016 00:00                 0       90480  05/04/2016 12:47  \n",
       "3  17/03/2016 00:00                 0       91074  17/03/2016 17:40  \n",
       "4  31/03/2016 00:00                 0       60437  06/04/2016 10:17  \n",
       "5  04/04/2016 00:00                 0       33775  06/04/2016 19:17  \n",
       "6  01/04/2016 00:00                 0       67112  05/04/2016 18:18  \n",
       "7  21/03/2016 00:00                 0       19348  25/03/2016 16:47  \n",
       "8  04/04/2016 00:00                 0       94505  04/04/2016 23:42  \n",
       "9  17/03/2016 00:00                 0       27472  31/03/2016 17:17  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 354369 entries, 0 to 354368\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   DateCrawled        354369 non-null  object\n",
      " 1   Price              354369 non-null  int64 \n",
      " 2   VehicleType        316879 non-null  object\n",
      " 3   RegistrationYear   354369 non-null  int64 \n",
      " 4   Gearbox            334536 non-null  object\n",
      " 5   Power              354369 non-null  int64 \n",
      " 6   Model              334664 non-null  object\n",
      " 7   Mileage            354369 non-null  int64 \n",
      " 8   RegistrationMonth  354369 non-null  int64 \n",
      " 9   FuelType           321474 non-null  object\n",
      " 10  Brand              354369 non-null  object\n",
      " 11  NotRepaired        283215 non-null  object\n",
      " 12  DateCreated        354369 non-null  object\n",
      " 13  NumberOfPictures   354369 non-null  int64 \n",
      " 14  PostalCode         354369 non-null  int64 \n",
      " 15  LastSeen           354369 non-null  object\n",
      "dtypes: int64(7), object(9)\n",
      "memory usage: 43.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateCrawled: 100.00%\n",
      "Price: 100.00%\n",
      "VehicleType: 89.42%\n",
      "RegistrationYear: 100.00%\n",
      "Gearbox: 94.40%\n",
      "Power: 100.00%\n",
      "Model: 94.44%\n",
      "Mileage: 100.00%\n",
      "RegistrationMonth: 100.00%\n",
      "FuelType: 90.72%\n",
      "Brand: 100.00%\n",
      "NotRepaired: 79.92%\n",
      "DateCreated: 100.00%\n",
      "NumberOfPictures: 100.00%\n",
      "PostalCode: 100.00%\n",
      "LastSeen: 100.00%\n"
     ]
    }
   ],
   "source": [
    "total_count = len(df)\n",
    "\n",
    "for column in df.columns:\n",
    "    column_count = df[column].count()\n",
    "    count_percentage = (column_count / total_count) * 100\n",
    "    print(f'{column}: {count_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Power</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <th>PostalCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.0</td>\n",
       "      <td>354369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4416.656776</td>\n",
       "      <td>2004.234448</td>\n",
       "      <td>110.094337</td>\n",
       "      <td>128211.172535</td>\n",
       "      <td>5.714645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50508.689087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4514.158514</td>\n",
       "      <td>90.227958</td>\n",
       "      <td>189.850405</td>\n",
       "      <td>37905.341530</td>\n",
       "      <td>3.726421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25783.096248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>125000.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2003.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49413.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6400.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99998.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Price  RegistrationYear          Power        Mileage  \\\n",
       "count  354369.000000     354369.000000  354369.000000  354369.000000   \n",
       "mean     4416.656776       2004.234448     110.094337  128211.172535   \n",
       "std      4514.158514         90.227958     189.850405   37905.341530   \n",
       "min         0.000000       1000.000000       0.000000    5000.000000   \n",
       "25%      1050.000000       1999.000000      69.000000  125000.000000   \n",
       "50%      2700.000000       2003.000000     105.000000  150000.000000   \n",
       "75%      6400.000000       2008.000000     143.000000  150000.000000   \n",
       "max     20000.000000       9999.000000   20000.000000  150000.000000   \n",
       "\n",
       "       RegistrationMonth  NumberOfPictures     PostalCode  \n",
       "count      354369.000000          354369.0  354369.000000  \n",
       "mean            5.714645               0.0   50508.689087  \n",
       "std             3.726421               0.0   25783.096248  \n",
       "min             0.000000               0.0    1067.000000  \n",
       "25%             3.000000               0.0   30165.000000  \n",
       "50%             6.000000               0.0   49413.000000  \n",
       "75%             9.000000               0.0   71083.000000  \n",
       "max            12.000000               0.0   99998.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in inspecting the data, we see some discrepancies in the counts against the expected totals (i.e. missing values for VehicleType, Gearbox, Model, FuelType, NotRepaired). Also, the min and max values for RegistrationYear (1000 and 9999, respectively) don't seem to be valid. We'll want to take care of this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find duplicated values\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicated values\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop data that can't be used for prediction\n",
    "df = df.drop(['DateCrawled', 'DateCreated', 'PostalCode', 'LastSeen'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='RegistrationYear'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB9sAAAMJCAYAAACjplZ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAACpBUlEQVR4nOzde5hd870/8M9czOTCTAjJJBLinhC3BhG3aqUmTHtKcUrdBT89iV9JG6LVUL249UJJm6Pq0qccoU85JRpyQqIIKjU/EqSqSUOZUCRDkETy/f3hmX2yZRL5bslk4/V6nv0ks9b3u9dnrb3Wd6+933vtXZFSSgEAAAAAAAAArLHK9V0AAAAAAAAAAHzcCNsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyVa/vAtan5cuXx0svvRQbbbRRVFRUrO9yAAAAAAAAAFiPUkrx5ptvRu/evaOycvXXrn+qw/aXXnop+vbtu77LAAAAAAAAAKCMvPDCC9GnT5/VtvlUh+0bbbRRRLy/oerq6tZzNQAAAAAAAACsT62trdG3b99Clrw6n+qwve2r4+vq6oTtAAAAAAAAAERErNHPkK/+S+YBAAAAAAAAgJUI2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgU1bYfvHFF8eee+4ZG220UfTo0SMOO+ywmD17dlGbAw88MCoqKopuZ5xxRlGbefPmRVNTU3Tp0iV69OgRo0ePjvfee6+ozdSpU+Mzn/lM1NbWxrbbbhs33HDDSvWMGzcu+vXrF506dYrBgwfHY489lrM6AAAAAAAAAFCSrLB92rRpMWLEiHjkkUdi8uTJsXTp0jj44INj0aJFRe1OO+20ePnllwu3yy67rDBv2bJl0dTUFEuWLImHH344brzxxrjhhhti7NixhTZz5syJpqam+NznPhfNzc1x1llnxamnnhr33HNPoc2ECRNi1KhRccEFF8Rf/vKX2HXXXaOxsTFeeeWVUrcFAAAAAAAAAKyRipRSKrXzq6++Gj169Ihp06bFAQccEBHvX9m+2267xRVXXNFunz/+8Y/xxS9+MV566aXo2bNnRESMHz8+zj333Hj11VejpqYmzj333Jg4cWLMnDmz0O/oo4+OBQsWxKRJkyIiYvDgwbHnnnvG1VdfHRERy5cvj759+8aZZ54ZY8aMWaP6W1tbo76+PhYuXBh1dXWlbgYAAAAAAAAAPgFyMuSP9JvtCxcujIiITTbZpGj6TTfdFJtuumkMHDgwzjvvvHj77bcL86ZPnx4777xzIWiPiGhsbIzW1taYNWtWoc3QoUOL7rOxsTGmT58eERFLliyJGTNmFLWprKyMoUOHFtq0Z/HixdHa2lp0AwAAAAAAAIBc1aV2XL58eZx11lmx7777xsCBAwvTv/a1r8WWW24ZvXv3jieffDLOPffcmD17dvz+97+PiIiWlpaioD0iCn+3tLSstk1ra2u888478cYbb8SyZcvabfPss8+usuaLL744vve975W6ygAAAAAAAAAQER8hbB8xYkTMnDkzHnzwwaLpp59+euH/O++8c/Tq1SsOOuigeP7552ObbbYpvdK14LzzzotRo0YV/m5tbY2+ffuux4oAAAAAAAAA+DgqKWwfOXJk3HXXXfHAAw9Enz59Vtt28ODBERHxt7/9LbbZZptoaGiIxx57rKjN/PnzIyKioaGh8G/btBXb1NXVRefOnaOqqiqqqqrabdN2H+2pra2N2traNVtJAAAAAAAAAFiFrN9sTynFyJEj4/bbb4/77rsvttpqqw/t09zcHBERvXr1ioiIIUOGxFNPPRWvvPJKoc3kyZOjrq4udtxxx0KbKVOmFN3P5MmTY8iQIRERUVNTE4MGDSpqs3z58pgyZUqhDQAAAAAAAACsK1lXto8YMSJuvvnm+O///u/YaKONCr+xXl9fH507d47nn38+br755jj00EOje/fu8eSTT8bZZ58dBxxwQOyyyy4REXHwwQfHjjvuGMcff3xcdtll0dLSEueff36MGDGicNX5GWecEVdffXWcc845ccopp8R9990Xt956a0ycOLFQy6hRo+LEE0+MPfbYI/baa6+44oorYtGiRXHyySevrW0DAAAAAAAAAO2qSCmlNW5cUdHu9Ouvvz5OOumkeOGFF+K4446LmTNnxqJFi6Jv375x+OGHx/nnnx91dXWF9v/4xz/i61//ekydOjW6du0aJ554YlxyySVRXf2/2f/UqVPj7LPPjqeffjr69OkT3/3ud+Okk04qWu7VV18dl19+ebS0tMRuu+0WP//5zwtfW78mWltbo76+PhYuXFhUHwAAAAAAAACfPjkZclbY/kkjbAcAAAAAAACgTU6GnPWb7QAAAAAAAACAsB0AAAAAAAAAsgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACBT9fouoJz0GzOx3elzL2nq4EoAAAAAAAAAKGeubAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATNXru4CPu35jJrY7fe4lTR1cCQAAAAAAAAAdxZXtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJCpen0X8GnTb8zEVc6be0lTB1YCAAAAAAAAQKlc2Q4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJApK2y/+OKLY88994yNNtooevToEYcddljMnj27qM27774bI0aMiO7du8eGG24YRxxxRMyfP7+ozbx586KpqSm6dOkSPXr0iNGjR8d7771X1Gbq1Knxmc98Jmpra2PbbbeNG264YaV6xo0bF/369YtOnTrF4MGD47HHHstZHQAAAAAAAAAoSXVO42nTpsWIESNizz33jPfeey++/e1vx8EHHxxPP/10dO3aNSIizj777Jg4cWLcdtttUV9fHyNHjoyvfOUr8dBDD0VExLJly6KpqSkaGhri4YcfjpdffjlOOOGE2GCDDeJHP/pRRETMmTMnmpqa4owzzoibbroppkyZEqeeemr06tUrGhsbIyJiwoQJMWrUqBg/fnwMHjw4rrjiimhsbIzZs2dHjx491uY2Wu/6jZm4ynlzL2nqwEoAAAAAAAAAiIioSCmlUju/+uqr0aNHj5g2bVoccMABsXDhwthss83i5ptvjiOPPDIiIp599tkYMGBATJ8+Pfbee+/44x//GF/84hfjpZdeip49e0ZExPjx4+Pcc8+NV199NWpqauLcc8+NiRMnxsyZMwvLOvroo2PBggUxadKkiIgYPHhw7LnnnnH11VdHRMTy5cujb9++ceaZZ8aYMWPWqP7W1taor6+PhQsXRl1d3SpD7dUF2rl9SgnOhe0AAAAAAAAA694HM+TV+Ui/2b5w4cKIiNhkk00iImLGjBmxdOnSGDp0aKFN//79Y4sttojp06dHRMT06dNj5513LgTtERGNjY3R2toas2bNKrRZ8T7a2rTdx5IlS2LGjBlFbSorK2Po0KGFNu1ZvHhxtLa2Ft0AAAAAAAAAIFfJYfvy5cvjrLPOin333TcGDhwYEREtLS1RU1MT3bp1K2rbs2fPaGlpKbRZMWhvm982b3VtWltb45133ol//etfsWzZsnbbtN1Hey6++OKor68v3Pr27Zu/4gAAAAAAAAB86pUcto8YMSJmzpwZt9xyy9qsZ50677zzYuHChYXbCy+8sL5LAgAAAAAAAOBjqLqUTiNHjoy77rorHnjggejTp09hekNDQyxZsiQWLFhQdHX7/Pnzo6GhodDmscceK7q/+fPnF+a1/ds2bcU2dXV10blz56iqqoqqqqp227TdR3tqa2ujtrY2f4UBAAAAAAAAYAVZYXtKKc4888y4/fbbY+rUqbHVVlsVzR80aFBssMEGMWXKlDjiiCMiImL27Nkxb968GDJkSEREDBkyJH74wx/GK6+8Ej169IiIiMmTJ0ddXV3suOOOhTZ333130X1Pnjy5cB81NTUxaNCgmDJlShx22GER8f7X2k+ZMiVGjhyZuQk+mfqNmbjKeXMvaerASgAAAAAAAAA+ebLC9hEjRsTNN98c//3f/x0bbbRR4ffR6+vro3PnzlFfXx/Dhw+PUaNGxSabbBJ1dXVx5plnxpAhQ2LvvfeOiIiDDz44dtxxxzj++OPjsssui5aWljj//PNjxIgRhavOzzjjjLj66qvjnHPOiVNOOSXuu+++uPXWW2PixP8NkEeNGhUnnnhi7LHHHrHXXnvFFVdcEYsWLYqTTz55bW0bAAAAAAAAAGhXVtj+y1/+MiIiDjzwwKLp119/fZx00kkREfGzn/0sKisr44gjjojFixdHY2Nj/OIXvyi0raqqirvuuiu+/vWvx5AhQ6Jr165x4oknxkUXXVRos9VWW8XEiRPj7LPPjiuvvDL69OkT1157bTQ2NhbafPWrX41XX301xo4dGy0tLbHbbrvFpEmTomfPnrnbAAAAAAAAAACyZH+N/Ifp1KlTjBs3LsaNG7fKNltuueVKXxP/QQceeGA88cQTq20zcuRIXxsPAAAAAAAAQIfLCtv5ZFvV77z7jXcAAAAAAACAYpXruwAAAAAAAAAA+LgRtgMAAAAAAABAJmE7AAAAAAAAAGTym+18JH7nHQAAAAAAAPg0cmU7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGSqXt8F8OnTb8zEdqfPvaSpgysBAAAAAAAAKI0r2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgU/X6LgA+TL8xE1c5b+4lTR1YCQAAAAAAAMD7XNkOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQqXp9FwDrQr8xE1c5b+4lTR1YCQAAAAAAAPBJ5Mp2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATNXruwAoF/3GTFzlvLmXNHVgJQAAAAAAAEC5c2U7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGTKDtsfeOCB+NKXvhS9e/eOioqKuOOOO4rmn3TSSVFRUVF0GzZsWFGb119/PY499tioq6uLbt26xfDhw+Ott94qavPkk0/G/vvvH506dYq+ffvGZZddtlItt912W/Tv3z86deoUO++8c9x99925qwMAAAAAAAAA2bLD9kWLFsWuu+4a48aNW2WbYcOGxcsvv1y4/dd//VfR/GOPPTZmzZoVkydPjrvuuiseeOCBOP300wvzW1tb4+CDD44tt9wyZsyYEZdffnlceOGFcc011xTaPPzww3HMMcfE8OHD44knnojDDjssDjvssJg5c2buKgEAAAAAAABAlurcDoccckgccsghq21TW1sbDQ0N7c575plnYtKkSfHnP/859thjj4iIuOqqq+LQQw+NH//4x9G7d++46aabYsmSJXHddddFTU1N7LTTTtHc3Bw//elPC6H8lVdeGcOGDYvRo0dHRMT3v//9mDx5clx99dUxfvz43NUCAAAAAAAAgDW2Tn6zferUqdGjR4/YYYcd4utf/3q89tprhXnTp0+Pbt26FYL2iIihQ4dGZWVlPProo4U2BxxwQNTU1BTaNDY2xuzZs+ONN94otBk6dGjRchsbG2P69OmrrGvx4sXR2tpadAMAAAAAAACAXGs9bB82bFj85je/iSlTpsSll14a06ZNi0MOOSSWLVsWEREtLS3Ro0ePoj7V1dWxySabREtLS6FNz549i9q0/f1hbdrmt+fiiy+O+vr6wq1v374fbWUBAAAAAAAA+FTK/hr5D3P00UcX/r/zzjvHLrvsEttss01MnTo1DjrooLW9uCznnXdejBo1qvB3a2urwB0AAAAAAACAbOvka+RXtPXWW8emm24af/vb3yIioqGhIV555ZWiNu+99168/vrrhd95b2hoiPnz5xe1afv7w9qs6rfiI97/Lfm6urqiGwAAAAAAAADkWudh+4svvhivvfZa9OrVKyIihgwZEgsWLIgZM2YU2tx3332xfPnyGDx4cKHNAw88EEuXLi20mTx5cuywww6x8cYbF9pMmTKlaFmTJ0+OIUOGrOtVAgAAAAAAAOBTLjtsf+utt6K5uTmam5sjImLOnDnR3Nwc8+bNi7feeitGjx4djzzySMydOzemTJkSX/7yl2PbbbeNxsbGiIgYMGBADBs2LE477bR47LHH4qGHHoqRI0fG0UcfHb17946IiK997WtRU1MTw4cPj1mzZsWECRPiyiuvLPoK+G984xsxadKk+MlPfhLPPvtsXHjhhfH444/HyJEj18JmAQAAAAAAAIBVyw7bH3/88dh9991j9913j4iIUaNGxe677x5jx46NqqqqePLJJ+Pf/u3fYvvtt4/hw4fHoEGD4k9/+lPU1tYW7uOmm26K/v37x0EHHRSHHnpo7LfffnHNNdcU5tfX18e9994bc+bMiUGDBsU3v/nNGDt2bJx++umFNvvss0/cfPPNcc0118Suu+4av/vd7+KOO+6IgQMHfpTtAQAAAAAAAAAfqjq3w4EHHhgppVXOv+eeez70PjbZZJO4+eabV9tml112iT/96U+rbXPUUUfFUUcd9aHLAwAAAAAAAIC1aZ3/ZjsAAAAAAAAAfNII2wEAAAAAAAAgU/bXyAP/q9+Yie1On3tJUwdXAgAAAAAAAHQkV7YDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCa/2Q4dzO+8AwAAAAAAwMefK9sBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyVa/vAoAP12/MxHanz72kqYMrAQAAAAAAACJc2Q4AAAAAAAAA2VzZDp9Aq7oSPsLV8AAAAAAAALA2uLIdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgU/X6LgAoD/3GTFzlvLmXNHVgJQAAAAAAAFD+XNkOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmyw/YHHnggvvSlL0Xv3r2joqIi7rjjjqL5KaUYO3Zs9OrVKzp37hxDhw6N5557rqjN66+/Hscee2zU1dVFt27dYvjw4fHWW28VtXnyySdj//33j06dOkXfvn3jsssuW6mW2267Lfr37x+dOnWKnXfeOe6+++7c1QEAAAAAAACAbNlh+6JFi2LXXXeNcePGtTv/sssui5///Ocxfvz4ePTRR6Nr167R2NgY7777bqHNscceG7NmzYrJkyfHXXfdFQ888ECcfvrphfmtra1x8MEHx5ZbbhkzZsyIyy+/PC688MK45pprCm0efvjhOOaYY2L48OHxxBNPxGGHHRaHHXZYzJw5M3eVAAAAAAAAACBLdW6HQw45JA455JB256WU4oorrojzzz8/vvzlL0dExG9+85vo2bNn3HHHHXH00UfHM888E5MmTYo///nPsccee0RExFVXXRWHHnpo/PjHP47evXvHTTfdFEuWLInrrrsuampqYqeddorm5ub46U9/Wgjlr7zyyhg2bFiMHj06IiK+//3vx+TJk+Pqq6+O8ePHl7QxAAAAAAAAAGBNrNXfbJ8zZ060tLTE0KFDC9Pq6+tj8ODBMX369IiImD59enTr1q0QtEdEDB06NCorK+PRRx8ttDnggAOipqam0KaxsTFmz54db7zxRqHNistpa9O2nPYsXrw4Wltbi24AAAAAAAAAkGuthu0tLS0REdGzZ8+i6T179izMa2lpiR49ehTNr66ujk022aSoTXv3seIyVtWmbX57Lr744qivry/c+vbtm7uKAAAAAAAAALB2w/Zyd95558XChQsLtxdeeGF9lwQAAAAAAADAx1D2b7avTkNDQ0REzJ8/P3r16lWYPn/+/Nhtt90KbV555ZWifu+99168/vrrhf4NDQ0xf/78ojZtf39Ym7b57amtrY3a2toS1gxoT78xE1c5b+4lTR1YCQAAAAAAAHSstXpl+1ZbbRUNDQ0xZcqUwrTW1tZ49NFHY8iQIRERMWTIkFiwYEHMmDGj0Oa+++6L5cuXx+DBgwttHnjggVi6dGmhzeTJk2OHHXaIjTfeuNBmxeW0tWlbDgAAAAAAAACsK9lh+1tvvRXNzc3R3NwcERFz5syJ5ubmmDdvXlRUVMRZZ50VP/jBD+IPf/hDPPXUU3HCCSdE796947DDDouIiAEDBsSwYcPitNNOi8ceeyweeuihGDlyZBx99NHRu3fviIj42te+FjU1NTF8+PCYNWtWTJgwIa688soYNWpUoY5vfOMbMWnSpPjJT34Szz77bFx44YXx+OOPx8iRIz/6VgEAAAAAAACA1cj+GvnHH388Pve5zxX+bgvATzzxxLjhhhvinHPOiUWLFsXpp58eCxYsiP322y8mTZoUnTp1KvS56aabYuTIkXHQQQdFZWVlHHHEEfHzn/+8ML++vj7uvffeGDFiRAwaNCg23XTTGDt2bJx++umFNvvss0/cfPPNcf7558e3v/3t2G677eKOO+6IgQMHlrQhAAAAAAAAAGBNZYftBx54YKSUVjm/oqIiLrroorjoootW2WaTTTaJm2++ebXL2WWXXeJPf/rTatscddRRcdRRR62+YAAAAAAAAABYy9bqb7YDAAAAAAAAwKeBsB0AAAAAAAAAMgnbAQAAAAAAACBT9m+2A3wU/cZMbHf63Eua1mofAAAAAAAAWJdc2Q4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmarXdwEA60K/MRPbnT73kqYOrgQAAAAAAIBPIle2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZFrrYfuFF14YFRUVRbf+/fsX5r/77rsxYsSI6N69e2y44YZxxBFHxPz584vuY968edHU1BRdunSJHj16xOjRo+O9994rajN16tT4zGc+E7W1tbHtttvGDTfcsLZXBQAAAAAAAADatU6ubN9pp53i5ZdfLtwefPDBwryzzz477rzzzrjtttti2rRp8dJLL8VXvvKVwvxly5ZFU1NTLFmyJB5++OG48cYb44YbboixY8cW2syZMyeampric5/7XDQ3N8dZZ50Vp556atxzzz3rYnUAAAAAAAAAoEj1OrnT6upoaGhYafrChQvj17/+ddx8883x+c9/PiIirr/++hgwYEA88sgjsffee8e9994bTz/9dPzP//xP9OzZM3bbbbf4/ve/H+eee25ceOGFUVNTE+PHj4+tttoqfvKTn0RExIABA+LBBx+Mn/3sZ9HY2LjKuhYvXhyLFy8u/N3a2rqW1xwAAAAAAACAT4N1cmX7c889F717946tt946jj322Jg3b15ERMyYMSOWLl0aQ4cOLbTt379/bLHFFjF9+vSIiJg+fXrsvPPO0bNnz0KbxsbGaG1tjVmzZhXarHgfbW3a7mNVLr744qivry/c+vbtu1bWFwAAAAAAAIBPl7Uetg8ePDhuuOGGmDRpUvzyl7+MOXPmxP777x9vvvlmtLS0RE1NTXTr1q2oT8+ePaOlpSUiIlpaWoqC9rb5bfNW16a1tTXeeeedVdZ23nnnxcKFCwu3F1544aOuLgAAAAAAAACfQmv9a+QPOeSQwv932WWXGDx4cGy55ZZx6623RufOndf24rLU1tZGbW3teq0BAAAAAAAAgI+/dfI18ivq1q1bbL/99vG3v/0tGhoaYsmSJbFgwYKiNvPnzy/8xntDQ0PMnz9/pflt81bXpq6ubr0H+gAAAAAAAAB88q3zsP2tt96K559/Pnr16hWDBg2KDTbYIKZMmVKYP3v27Jg3b14MGTIkIiKGDBkSTz31VLzyyiuFNpMnT466urrYcccdC21WvI+2Nm33AQAAAAAAAADr0lr/Gvlvfetb8aUvfSm23HLLeOmll+KCCy6IqqqqOOaYY6K+vj6GDx8eo0aNik022STq6urizDPPjCFDhsTee+8dEREHH3xw7LjjjnH88cfHZZddFi0tLXH++efHiBEjCl8Bf8YZZ8TVV18d55xzTpxyyilx3333xa233hoTJ05c26sDfEr0G7Pq8WPuJU0dWAkAAAAAAAAfB2s9bH/xxRfjmGOOiddeey0222yz2G+//eKRRx6JzTbbLCIifvazn0VlZWUcccQRsXjx4mhsbIxf/OIXhf5VVVVx1113xde//vUYMmRIdO3aNU488cS46KKLCm222mqrmDhxYpx99tlx5ZVXRp8+feLaa6+NxsbGtb06AAAAAAAAALCStR6233LLLaud36lTpxg3blyMGzdulW223HLLuPvuu1d7PwceeGA88cQTJdUIAAAAAAAAAB/FOv/NdgAAAAAAAAD4pBG2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAECm6vVdAMDHVb8xE1c5b+4lTR1YCQAAAAAAAB3Nle0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkMlvtgN0IL/zDgAAAAAA8MngynYAAAAAAAAAyCRsBwAAAAAAAIBMwnYAAAAAAAAAyCRsBwAAAAAAAIBMwnYAAAAAAAAAyCRsBwAAAAAAAIBMwnYAAAAAAAAAyCRsBwAAAAAAAIBMwnYAAAAAAAAAyCRsBwAAAAAAAIBMwnYAAAAAAAAAyCRsBwAAAAAAAIBMwnYAAAAAAAAAyCRsBwAAAAAAAIBM1eu7AABWr9+Yie1On3tJUwdXAgAAAAAAQBtXtgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGSqXt8FALD29Rszsd3pcy9p6uBKAAAAAAAAPplc2Q4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmarXdwEAlId+Yya2O33uJU0dXAkAAAAAAED5E7YDUJJVhfMRAnoAAAAAAOCTz9fIAwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAECm6vVdAACfHv3GTFzlvLmXNHVgJQAAAAAAAB+NK9sBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyCdsBAAAAAAAAIJOwHQAAAAAAAAAyVa/vAgBgdfqNmdju9LmXNK3VPgAAAAAAADlc2Q4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmarXdwEAUA76jZnY7vS5lzR1cCUAAAAAAMDHgSvbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMlWv7wIA4OOq35iJ7U6fe0lTB1cCAAAAAAB0NFe2AwAAAAAAAEAmV7YDQAdZ1ZXwEa6GBwAAAACAjxtXtgMAAAAAAABAJle2A0AZczU8AAAAAACUJ1e2AwAAAAAAAEAmV7YDwCfMqq6GdyU8AAAAAACsPa5sBwAAAAAAAIBMwnYAAAAAAAAAyORr5AGAkr563tfVAwAAAADwaebKdgAAAAAAAADI5Mp2AKDDuBoeAAAAAIBPCmE7AFC2VhXORwjoAQAAAABYv4TtAMAnSikBvd+sBwAAAAAgl7AdAKCDCOgBAAAAAD45hO0AAGVMQA8AAAAAUJ4+9mH7uHHj4vLLL4+WlpbYdddd46qrroq99tprfZcFALBerM2v0S+lz9r+6n1f8Q8AAAAAlKuPddg+YcKEGDVqVIwfPz4GDx4cV1xxRTQ2Nsbs2bOjR48e67s8AAA+Bj5pH1AAAAAAADrGxzps/+lPfxqnnXZanHzyyRERMX78+Jg4cWJcd911MWbMmPVcHQAAdDyhPgAAAAB0jI9t2L5kyZKYMWNGnHfeeYVplZWVMXTo0Jg+fXq7fRYvXhyLFy8u/L1w4cKIiGhtbY2IiOWL3263X9v89uT2WVX7cu6zNte/lD7re/1L6WOb2Wa22Sd//UvpY5vZZrZZefaxzfL7DLzgnlXe18zvNa7zPqtqX859Sll/AAAAADpe23tiKaUPbVuR1qRVGXrppZdi8803j4cffjiGDBlSmH7OOefEtGnT4tFHH12pz4UXXhjf+973OrJMAAAAAAAAAD5mXnjhhejTp89q23xsr2wvxXnnnRejRo0q/L18+fJ4/fXXo3v37lFRUVGY3traGn379o0XXngh6urq1ui+y7VPudbVUX3Kta5y7lOudZVzn3Ktq5z7lGtd5dynXOsq5z7lWlc59ynXusq5T7nWVc59yrWucu5TrnWVc59yrauc+5RrXeXcp1zr6qg+5VpXOfcp17rKuU+51lXOfcq1rnLuU651lXOfcq2rnPuUa13l3Kdc6yrnPuVaVzn3Kde6yrlPuda1tvuklOLNN9+M3r17f+h9fGzD9k033TSqqqpi/vz5RdPnz58fDQ0N7fapra2N2traomndunVb5TLq6urW+MEo9z7lWldH9SnXusq5T7nWVc59yrWucu5TrnWVc59yrauc+5RrXeXcp1zrKuc+5VpXOfcp17rKuU+51lXOfcq1rnLuU651lXOfcq2ro/qUa13l3Kdc6yrnPuVaVzn3Kde6yrlPudZVzn3Kta5y7lOudZVzn3Ktq5z7lGtd5dynXOsq5z7lWtfa7FNfX79GfSuzllRGampqYtCgQTFlypTCtOXLl8eUKVOKvlYeAAAAAAAAANa2j+2V7RERo0aNihNPPDH22GOP2GuvveKKK66IRYsWxcknn7y+SwMAAAAAAADgE+xjHbZ/9atfjVdffTXGjh0bLS0tsdtuu8WkSZOiZ8+eH+l+a2tr44ILLljpK+c/jn3Kta6O6lOudZVzn3Ktq5z7lGtd5dynXOsq5z7lWlc59ynXusq5T7nWVc59yrWucu5TrnWVc59yrauc+5RrXeXcp1zrKuc+5VpXR/Up17rKuU+51lXOfcq1rnLuU651lXOfcq2rnPuUa13l3Kdc6yrnPuVaVzn3Kde6yrlPudZVzn3Kta6O7PNBFSmlVHJvAAAAAAAAAPgU+tj+ZjsAAAAAAAAArC/CdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIVL2+Cyg3ixcvjoiI2tra9VzJ2rF48eJ48cUXo0+fPut8naZOnRqDBw+Ozp07r9PldIT58+dHSikaGhrW+n0vW7Ys/vWvf0VlZWVsttlma/3+2yxcuDBaWloiIqKhoSHq6+vX2bI6ynPPPRfz5s2LLbfcMrbddtv1XU6RlFIsX748qqqqVtlm2bJlRfMfe+yxWL58eey+++6fmDEnxw033BCHH374J2LfBIC17e9//3s8+OCD8fLLL0dlZWVsvfXW8YUvfCHq6urW+rLmzZtXtJzu3buv9WVAOSvltdMn7b0D8nySHv9yfe+gXOuidLnHjX2gNAsWLIjbbrut8P7ZUUcdZdutRZ+08+aOOM4+zcfyK6+8EjNnzoxBgwZFfX19zJ8/P2688cZYvnx5NDU1xc4777za/qWcb5Tj++efpH3g074uHdWnFOv9sUmke++9Nx1yyCGpW7duqbKyMlVWVqZu3bqlQw45JE2ePHmV/Z5//vl04403pksuuSRddtll6Xe/+11auHBhu23nz59f9PcTTzyRTjjhhLTPPvukI444It1///3ZdT/99NNpq622Kvx9/fXXp4cffjillNI777yTTjnllFRVVZUqKytTdXV1+j//5/+kd999d6X7+dWvfpVOOOGEdN1116WUUrrllltS//7901ZbbZXGjh27xvVssMEG6emnn17l/Obm5vT9738/jRs3Lr366qtF8xYuXJhOPvnklfrMnz8/TZkyJS1YsCCllFJLS0u69NJL08UXX5yefPLJorYDBw5MF110UZo3b94a1/zaa6+lI444IvXt2zedccYZ6b333kvDhw9PFRUVqbKyMg0ZMiS99NJLK/V7/PHH13gZbe666660//77p9ra2sJ+Vl9fn4477rj0j3/840P7v/HGG+maa65J559/fvrVr35V2CYf9Ktf/SoNGDCgsIy224ABA9K1115b1HbDDTdMp5xySnrooYey12dVmpubU2Vl5Rq1XbJkSfrrX//a7rr86Ec/Sv/zP/+TUkrp9ddfTwcddFCqqKgoPDbDhg1Lb7zxxkr9Jk6cmIYPH55Gjx6dnnnmmaJ5r7/+evrc5z63RrWddNJJ6Z///OdK05cuXZq+853vpAMOOKBwfFx22WWpS5cuqaamJp1wwglp8eLFRX3mzp2bBg0alKqqqtKwYcPSwoUL09ChQwvrs/XWW6fZs2evspb33nuv6O9HHnkkTZs2LS1ZsmSN1mVNvPzyy+mOO+5I48ePT+PHj0933HFHevnll0u6r7feeitNmzbtQ9t92JjRZk3H2kcffbRoW915553pgAMOSL17906DBg1KN9544yqXkTPWtGd1+3JKpY0ZbT7KY7Om40ZKH30/W9Uxs6KlS5em5ubmNGnSpDRp0qTU3Ny8VvfjlN5fj+effz4tW7YspZTSu+++myZMmJD+67/+K7W0tKyyX9t+e8stt6Rbb701Pf7442n58uUl1bCmx8Ca+CiP/9KlS9O9996brr322jR58uSVHuOU0krPxx9FS0vLGj2frejCCy8sqYalS5eu8bLWZB/LOaf7oDU9zkodZ9Z0H/go48yKPmw8W5U1GQM64vlsdVZ3bC5YsCA9++yz6dlnn12jdf8oY8aHHZtvvfVWOvLII4vOexoaGlJVVVXacMMN09VXX73K+84dA8eNG5e22GKLlc4b991339XuUx9lPH/33XfbfU2yJkrZV1Y3Nq3NMbDNB8entbmMUsfM1Sn1ebOtbc5jWcpxk3ts5u6bOa+dUir9vYM2H2X/Tyn/GLj++uuzx/O1fZytTWuynHV5vvlRH//2rOq56aMcmymt2Xlg7v6/Kmt7bFpbda1odeeOa2OfWZN9M2c8K+X9s9x9Zm0/n6xqG5dy3HTkvlnq4/+Pf/wjPfLII+mxxx5L//rXvz60fe7zWU6fww8/PN12220ppZRmzpyZNt1007TZZpulwYMHp549e6aGhoaV3nv5KO+frMqavkZb09cbHfH+QUprvp1LPW9eUSnnAevqdX2px1nOa+e1OZ4vX7683XX/oFKOtRWtzdc0999/f+ratWuqqKhIDQ0Nqbm5OfXp0ydtt912aYcddki1tbXpnnvuWalfzrhZ6vvnbUp9Xbum+/JH2QfW5biZUv57NOvi/OTD5I4Za7r+paxLR/UpxUddzkd9jdbmUx+233DDDam6ujodffTR6frrr0933313uvvuu9P111+fjjnmmLTBBhuk3/zmN0V9SnkDrLKyshC4P/TQQ2mDDTZIn/3sZ9Po0aPTF77whVRdXZ39xvwHQ82tttoqPfLIIymllL71rW+lfv36pd///vfpmWeeSXfccUfafvvt0+jRo4vu42c/+1nq2rVr+spXvpJ69eqVfvCDH6Tu3bunH/zgB+l73/teqqurS//5n/9Z1Gf33Xdv91ZRUZEGDBhQ+HtF99xzT6qpqUk77bRT2mKLLVL37t3TfffdV5jf0tKyUkCb+4RUUVGRunfvnqqqqlJjY2P63e9+l5YuXbrabXjKKaekgQMHpquuuip99rOfTV/+8pfTLrvskh588MH08MMPpz333DOdcMIJK/WrqKhI22yzTfrhD3/4oW8sp5TSb37zm7TRRhulb37zm+k73/lOamhoSGPGjEm//OUv02c/+9m06aabpr/+9a9FfUo5WW4LfceMGZPuv//+9PTTT6enn3463X///em8885LXbt2TZdffnnReuy0006poqIi9e/fP/34xz9Or7zyyoeuz+o0NzenioqKlaZfeuml6e23304pvf8i8Jvf/GaqqakpfBjk5JNPLjqh6dOnT/rLX/6SUkrp1FNPTbvvvnv6y1/+kt55553U3Nyc9t577zR8+PCiZdx0002pqqoqNTU1pf322y916tQp/fa3vy3Mb28/+3//7/+1e9tggw3S7bffXvi7zfnnn5969uyZRo0alXbcccd0xhlnpL59+6bf/va36cYbb0ybb755uvTSS4uWccQRR6TPfvaz6c4770z//u//nvbdd9904IEHphdffDG99NJLqbGxMR122GErbbOXXnop7bvvvqmqqiodcMAB6fXXX09NTU2FsWf77bdf6cMgS5YsSaNHj07bbLNN2nPPPdOvf/3rovkf3AZvvfVWOvbYY1NVVVWqrq5OPXr0SD169EjV1dWpqqoqHXfccWnRokUr1bY6HxybNt5443ZvFRUVqb6+vvD3B+WOtSuOs3/4wx9SZWVlOuGEE9K4cePSqaeemqqrq9Pvf//7lZaTO9bk7ssp5Y8Zbeuf+9iUMm7k7me5x0xKKS1btix95zvfSd26dSvcb9utW7du6fzzzy+8KdQmd19uq61Xr16psrIyDRw4MM2bNy8NHDgwde3aNW244YZp4403To899thKtY0ePTp16dKlcDLWVtuWW26Z/vCHP6zR47Wi9j50NG7cuHTQQQelo446qvBCqM2rr75a9OG5lEp7/EeOHJnuvPPOlFJKL7zwQurfv3+qqqpKPXv2TFVVVWnnnXdOL774YlGfysrK9PnPfz7ddNNNa3xi2dramo499ti0xRZbFD5g9B//8R+F4/SAAw5Y6cX2woULV7otWLAgbbDBBunRRx8tTFtT7W3jCRMmFH3Y6aqrriq8GdK9e/f0ve99b6X7KeWcrpTjrJQX2bn7QCnjTCnjWSljQEc8n62J9vab3BdlpYwZucfm6aefnvbdd9/01FNPpeeeey4deeSR6ZxzzkmLFi1Kv/71r1OXLl3STTfdtNJycsfAyy+/PPXu3TtdddVVhe1w0UUXpT/+8Y/p+OOPT126dEl//vOfV1r/3PE8pfw32ks5nksZm0oZAz/MB/ezUpZR6piZ+1xTyvNmKaFJKcdNKcdm7r6Z+9qplPcOSt1mpRwD7Vndh1s76jhLKX/fLGU5pewDOXWV+vh/mPaem0o5NnOfa3L3/5RKG5tyLzwppa5St3Mp+0wp+2bueFbK+2el7DNr+zmwvW1cynHTUftmqec0uYFrR4QTG2+8ceFik0MOOSR97WtfK4zvS5YsScOHD08HH3xwUZ9S3z9Znfb2gVJeb6zr8bxNznYu5by5Te55QEe8ri/lOMt97VzqeF7KRU4p5R83pZwH5T6W++23XxoxYkR688030+WXX54233zzNGLEiML8b33rW2mfffYp6pM7bpby/nlKpZ2f565/qftAR4ybue/RlLoupXywaV1/SK2UdemoPmvig881pS5nXXyI9lMftm+33XarvTpk3Lhxadttty2aVsobYBUVFYWTmC984QvplFNOKZr/jW98I33+858vmnb22Wev9nbccccV7Vi1tbWFTxBuv/326Y9//GPR/U2bNi1tscUWRdP69+9fqPUvf/lLqq6uLjoAr7322jRo0KCiPtXV1WnYsGHpwgsvLNwuuOCCVFlZmf7jP/6jMG1FQ4YMSd/+9rdTSu9/Eu3SSy9NG264YaHG9l4s5D4hVVRUpH/+85/p9ttvT1/60pdSdXV12myzzdI3v/nNVb7B0KtXr8JV3S0tLamioiLde++9hfkPPvhg2nzzzVfqV1FRkU477bTCm95NTU3p9ttvX+Un7Pr3759uueWWwt9//vOfU58+fQqfFPvqV7+aDj/88KI+pZwsb7HFFmnChAnt1pDS+99a0Ldv36L1mD9/fmpubk4jR45Mm2yySaqpqUlf+cpX0t13393uJ9kOP/zw1d4+//nPt/vm94on8pdffnnaeOON03XXXZdmzZqVfvvb36YePXoUhdS1tbVp7ty5KaWU+vXrt9KHUR5//PHUq1evomm77bZbuvLKKwt/T5gwIXXt2rWwT7e3n7WdIH7wBH7F6Sv22XrrrQsnvc8991yqrKwsemwnTJiQBg4cWLSMzTbbLD3xxBMppfc/YVZRUZH+9Kc/FebPmDEj9ezZc6Vtdvzxx6d99tkn/eEPf0hf/epX0z777JP233//9OKLL6Z//OMfad999y06JlJK6YILLkg9e/ZMl19+efrOd76T6uvr0+mnn16Y37aftxk+fHjabrvt0qRJk4r23/feey/dc889afvtt0+nnnrqSrWtzgef9DbccMPU1NSUbrjhhsLt+uuvT1VVVemHP/xhYdoH5Y61K46z++23XxozZkzR/f3whz9Me++990rLyR1rcvflttpyxoyUSntsShk3cvez3GMmpZRGjx6dNttsszR+/Pg0Z86c9Pbbb6e33347zZkzJ/3nf/5n6tGjRzrnnHOK+uTuyyml1NjYmI488sj01FNPpW984xtpwIAB6aijjkpLlixJS5cuTccdd1waOnRoUZ9zzz03DRgwIN15551p8uTJ6YADDkiXXnppeuaZZ9J3v/vdVX7SeHU+eAxceeWVqUuXLmnEiBHpuOOOSzU1NelHP/pR0bp8cJuV8vj37NkzPfXUUymllP793/89DR06tPAJ99deey198YtfTEceeWRRn4qKijRs2LBUU1OTNt544zRy5MjCeLUqI0eOTP37908///nP04EHHpi+/OUvp4EDB6YHH3wwTZs2Le24446F5/w2HzzhX/HF3Kr2m9Vp742cFY/N6667LnXq1CmNHTs2TZw4Mf3gBz9IXbt2Tb/61a+K+pRyTlfKcVbKi+zcfaCUcabU8Sx3DOiI57M1sTZelJUyZuQem5tuumnRm7Wvv/566tSpU+HDFVdffXXabbfdVlq/3DGwX79+6e677y78PXv27NS9e/fCh1X/7//9v+kLX/hC0TJKGc9LeaO9lOO5lLGplDHww3xwPytlGaWMmaU81+TuM6WGjbnHTSnHZin7Zu5rp1LeOyh1m+UeA6V8uLWjjrNS9s1SlpO7D+TWVcrjvybaO6cp5Zw297kmd/9PqbSxacX9bE0uPCmlrjXR3nYuZdzI3TdLGc9Kef+slH1mbT8HtreNSzluOmrfLOXxzw1cOyqc6Ny5c/rb3/6WUnr/fc624K3N7NmzU319fdG0Ut8/WZ0Pe422pq831vV4nlL+di7lvDml0s4DOuJ1fSnHWe5r51LH81IucirluMk9Dyrlsayrqyscm0uXLk3V1dVFj8tf//rXlY7N3HGzlPfPU8o/Py9l/UvZBzpq3Mx9j6bU/Tn3g00d8SG1Utalo/qsiQ9e6FnKctbVh2g/9WF7bW1tevbZZ1c5/9lnn02dOnUqmlbKG2ArnsT06tUrTZ8+vWh+25VRK6qsrEyf+cxn0oEHHtjubY899ig6Wdhyyy0Ln3bdfPPNV/pE3dNPP526du1aNK1z585FX/FTW1ubZs6cWfj7ueeeS926dSvq8+CDD6ZtttkmjR07tuiThNXV1WnWrFmpPSs+ubS56aabUteuXdOdd97Z7olP7hPSits4pfevovrRj36Utttuu1RZ+f5Xwn/wqqguXboUnpBSev+T/20nNCml9Pe//32lbbbispYuXZp+97vfpUMPPbTwKcNzzjlnpa8E79y5c5ozZ07RtOrq6sKVZ48++uhK27mUk+VOnTqt9mu5Z82alTp37rzSerR59913080335wOOuigVFlZmfr06ZO++93vrlT3IYcckk466aR2b//2b//WbmCy4rJ23333lb4x4be//W3aaaedCn9vv/326a677kopvf+tDR/8qvsnnngi1dXVFU3r2rVr+vvf/1407b777ksbbrhh+uUvf9nufrbrrrumpqam9Mwzz6S5c+emuXPnpjlz5qTq6uo0efLkwrQ2nTp1Kvqpgk6dOhV9Xf3f//73tNFGGxUtY6ONNirUtWzZslRdXZ2am5sL85977rmV+qRUPFa89tprqaKiougTulOmTElbb711UZ9tt9228GGAtvvedttt00knnZSWL1++0jbo1q3ban9G4MEHH1xp31zVm3ltt7q6uqJlPPfcc4VviXjzzTcL01c3ZqSUP9auuI/16NFjpU+VP/vssyutS0ofbaxZk315xT5rOmakVNpjU8q4kbuf5R4zKb3/YnHSpEmrXJdJkyalHj16FE3L3ZdTen/fbBsD33777VRVVZUeffTRwvyZM2em7t27r7T+DzzwQOHvF198MW244YaFT4NfdNFFaciQISstJ+cY2HHHHYtedD700ENps802K4yv7a1LKY9/p06dCmNNnz59itY9pZSeeuqplc412vbNV199Nf34xz9OO+64Y+H84xe/+EW7V6f17du3cL7xz3/+M1VUVBQ9VnfddVfaYYcdivpsvvnmqampKd13331p6tSpaerUqen+++9PVVVV6frrry9Ma7Oqb9Fpu/Xv37/dD0+1HZt77bVXuuyyy4rm/+IXv1jpm3dKOacr5Tgr5UV27j5QyjhTynhWyhjQEc9nKeUfm6W8KCtlzMg9Nrt161b0jUdLlixJ1dXVhW8f+utf/7rS65O29c8ZA7t06VJ0frp8+fJUXV1d+JaB5ubmtOGGGxYto5TxvJQ32ks5nksZm0oZA3PHp1KWkTtmplTac03uPlNq2Jh73JRybJayb+a+dirlvYNSt1nuMVDKh1s76jgrZd8sZTm5+0BuXaU8/inlPze19ck9p819rsnd/1MqbWzKvfCklLpSKu3csZRxI3ffLGU8K+X9s1L2mdznp1K2cSnHTUftm6U8/rmBa0eFE4MHD07XXHNNSun9x+n2228vmn/vvfemhoaGommlvH/yUV+jrenrjXU9nqf/396Zx1VVdf9/ncuMgCiioAg4oZipSKJgiZqKWk75qOWAWjmV+WiWpqmp5VyPmeaQPU6l1mNppaXmgBWamZg4kgioOeasgDJ+fn/4u+fL4VyQvYHjCdf79eL14t5z1l1rr7322vvsfc4+EPezzLgZkBsHGHFdL9PORK+dZfO5zENOMu1GdBwkU5eVKlVS11nS0tJgsVg0a0Lx8fG6uhTNmzLz54D4+Fw2lkVjwKi8KTpHIxvPojc2GXGTmkxZjJIBxB/0lNFTWjfRPvKL7U2aNNFtrZ6XsWPHokmTJprvZCbAFEXBqVOncOvWLdSoUUM3MXvq1Cm4urpqvgsKCsJnn31WoG1//PGHJrAmTJiA8PBw3LhxA2+99RY6d+6sLmylpaWhV69euietvLy8NMHo5+enmSBNTEy0OVi4efMmnn/+eTRr1kxNTIUtnHl7e9vcTmndunVwdXXF4sWLdQMf0Q4p751C+YmJiUG/fv10C+eNGjVSG9YPP/wAd3d3fPDBB+rxxYsX6zpwQL9IDdzvlKZNm4aaNWvCYrHgqaeeUo8FBwerW84C959kdnR0VJ82S0xM1NkmM1h+6qmnEB0dbXP7/OzsbERHR6Nly5bqd4X5LCUlBRMnTtR1Ro8//nih77rIH5dWFEVR24iXl5fmpgbg/iJ13jYwd+5cBAcHIzExER988AHCw8PVWEtOTkarVq10d3LaupEFAHbv3g03Nze8/fbbOtsyMjLw73//G/Xr19e0y4LiuUqVKpr3tkRERGi2bzpx4oRuENO8eXNMnDgRwP27JatUqaLpXKdNm6bbQQLQL+yXK1cOiYmJ6uczZ87oOgtbN3acO3cOQUFB6Nu3L86fP6/xgYeHR4FbXQHA/v37deVxdXXFmDFjNJN5ef+mTp2q83NWVhbGjh2LWrVqITY2FsCDF9tFc62iKIiJiUF8fDwCAgJ02+QlJCTYzGeiuUY0lq0yIjkDkKsbmbwhGmeibQa4HzOFvZM6Pj7e5s1gIrEMaGMmMzMTdnZ2iIuLU4+fOHFC91SXu7s7kpKS1M/WG2Ks78Q+duyYrj5F24Ctshw5ckTNBQXdcCZa/w0bNlQvQoODg3XbHu3duxcVK1bUfGcrNvfu3YsXX3wR7u7ucHV1Rf/+/TXHnZycNDHj6uqqWcg9ffq0zmfXrl1Dt27d0Lp1a03OLChunJycMGDAAM0uOnn/hg4danMix9o2K1WqpLmpCbg/1sp/Y5PMmE6mnclcZIvGgEyekclnMjnAiP4MEG+bMhdlMjlDtG22a9dOc1f93LlzNU8jHDx4UBcvgHgObNy4sRrLwP2bHlxdXdWdjRISEnRtRiafy0y0y7RnmdwkmwNF8pOMDtGcCcj1NaIxI7vYKNpuZNqmTGyKXjvJzB3I+ky0Dcjc3GpUO5OJTRk9ojEgapdM/VvtEr12khnTivY1ovEPyOUm0QdPZOwC5MaOsn2aSGzK5DOZ+TOZmBHtn2R8LNNujIpNmfoXXXA1anFi8+bNqFixIlasWIEVK1YgMDAQn376Kfbs2YPly5ejevXqunqQmT8p7jVaUa83SjufA+J+lhk3A3LjACOu62Xamei1s2w+l3nISabdiI6DZOqya9euePbZZxEbG4shQ4bgiSeewDPPPIPU1FSkpaXhX//6Fzp06KCREc2bMvPngPj4XKb8MjFgVN4UnaORjWfRG5uMuElNpixGyQDiD3rK6JG9RnsQj/xiu/X9DI8//jhGjx6NWbNmYdasWRg9ejQaNmwINzc33fYbMhNg1i2LrNsX5e2gAeDbb7/V3S3Rp08fjBo1qkDb82+ZkJGRgS5duqBChQpo164dnJ2d4erqijp16qBcuXLw9/fXPdXUokULzd1h+dm0aZPNxWYry5cvh4+PD5YuXQoHB4cCB7Ht2rUr8B0Ma9euhYODg27gI9oh2RpY5Cf/nXyff/457OzsULt2bTg5OWH9+vWoWrUqevXqheeffx6Ojo4273IpbJEaAHbs2IE+ffqonxcuXIjy5ctj7NixmDx5MqpWrap5X8rnn3+ue2pAZrAcHx8PHx8feHl5oXv37hg2bBiGDRuG7t27w8vLC76+vpqBbVF8ln8r+YEDB+KVV14p8Pzjx48jMDBQ972iKJg+fTrmz58PX19fXbuKj4/XXfy99tprcHBwQL169eDs7AyLxaK+2+mJJ55QBwBWunbtqr7PJz/Wtl7QNsU//PAD/Pz8MGPGDHWAYSueW7dubXPLcyv/+9//dAvnW7duhbOzMxwdHeHs7IyffvoJQUFBCAsLQ/PmzWFnZ2fzDjR/f3/NHazjxo3DtWvX1M+HDh3S5ZoaNWro3k8F3L/rPigoCO3atdP4oE+fPur7fPJz8OBBhIaGom/fvprvIyIi8OGHHxboA1tbh1nZuXMn/P39MX78+EJzBiCea/NvbTxv3jzN761btw7169fX6ZHJNaKxLJozALm6kckbMnEGFL3NAECnTp3Qvn17deuzvFy5cgUdOnTAM888o/leNJYB4Omnn8ZLL72Ec+fOYerUqahduzYGDRqkHn/llVd0i40RERF477331M/r1q3TDHSPHDmiq0/RNlC9enXNHcNWjh07hipVqiA6OlpXFpn6X7FiBfz8/BATE4PVq1cjODgYO3bswPnz57Fr1y48/vjjuq3nC4vN1NRUfPrpp7rtzatWraqZvHvhhRc0v3H06FGdz6wsWrQIVatWxdq1awEUPAEWGhqKRYsW2fwNwPaNXYqiYPXq1fj222/h5+eHvXv3ao4fPXpUd4OCzJhOpp3JXGSLxoBMnpHJZ1ZEcoAR/Rkg3jZlLspkcoZo24yLi0PFihXh4+MDf39/ODo6Yt26derxhQsXIjo6WmezaA788ssv4eDggF69eiE6Ohpubm6amwGXLFmie0pfJp/LTLTLtGeZ3CSTA0Xzk4wOK0XNmYBcXyMaM7KLjaLtRqZtysSm6LWTzNyBrM9k2oDoza1GtTOZ2JTRIxoDonbJ1D8gd+0kM6YV7WtE4z8vIrlJ9METWbtkxo4yeUM0NmXymcz8mUzMiPZPMj6WaTdGxaZM/YsuuBq5OPHVV1/Bz89PMyeiKAqcnZ0xatQo3aulZOZPZK/RRK83SjufA+J+lhk3A3LjACOu62Xamei1s2xblnnISabdiI6DZOry5MmTqFOnDhRFQXBwMM6dO4cuXbrA3t5eff1t3j4FkMubovPngPj4XKb8MjFgVN4UnaORjWfRG5uMuElNpixGyQDiD3rK6JG9RnsQj/xiO3D/Cd6xY8eiZcuWCAoKQlBQEFq2bIlx48bp7owD5CbArFsWWf/yL3p/+OGHuu1KLl68qNuGsyhs2bIFr7zyCjp06ID27dtjwIAB+OSTT5Camqo7NzY2ttB3uHz88cdYsGBBofpOnjyJpk2bQlGUAgexGzZsKPTGgTVr1qBVq1a63xXpkAYOHIjbt28XaqstYmNj8f7776vbrBw7dgz9+/dHjx49ClxULcoidX4WLVqEiIgIhIaGYsKECbh7966mrHnv0rMiOlgGgNu3b2PRokWIjo5G+/bt0b59e0RHR2Px4sW6mw2mTJmibvdTVO7duycsA9x/zUFgYKD6l38g/+GHH9p8H9Tx48cxZ84cDBs2DEOGDME777yDH3/80eb75Hfv3q15L1N+du3ahYEDBxZ4/NKlS+jYsSOeeuqpAi/K/vzzT91W9XlZs2aNzYXzlJQUfPXVV2pOuXTpEiZNmoQxY8ao29Dlp0uXLoVOzCxcuFCz5R5w/x2/+bfms3Lu3DnUrl1b0yFdv34dHTp0gKIoqFixIurVq4d69eqhYsWKsFgs6NixI27cuKH5nenTp2PKlCkF2nX27NlC/Xz16lV0794dnp6ehd5FJpprrVsXW/+uXr2q+b1Vq1Zh1apVOj2iuUYmlmVyhkzdAOJ5QybOrBSlzQD3Y6JBgwawt7dHSEgIOnTogA4dOiAkJAT29vZo2LCh5s5lQDyWgftP+np5ecFiscDb2xtHjx5Fs2bN4OPjg6pVq8LFxUW3eLdjxw44OTkhLCwMLVu2hL29vaZO586dqyu/aBt44YUXCuwDjx49Cm9vb11ZZOv/gw8+gKurK1xcXNSLK+tft27dNE+7AXKx2aFDByxZsqTA4ytWrChw4Qi43882atQIL7zwQoFxM3LkSPz73/8u8DdOnTqlGzfkf3943gtHAPj00091N7bJLmqKtjOZi2zRGJCpS9m+2UpRc4AR/Rkg3jZlLspkcgYg3jYvXLiATz75BAsWLCj0xrS8yOTAH374AX369EGPHj10NwNfvXpV15fK5HOZCSOZ9iyTm2TajWh+ktGRl6LkTECurxGNGdnFRtF2I9M2ZWITELt2AsTnDmR9JtMGrBT15laj2plMbMroEY0BGbtE6x+Qu3aSyeeAeF8jGv95KWpuknnwRMYumbGjTN4QjU2ZfCYzfyYTM6L9k4yPAbl2Y0RsytS/6IKrkYsTwP1Fld9++w1ffPEF1q5di5iYmALnSWXmT2RiQOZ6w4h8LuNn0XEzID8OMOK6XrSdyVw7y7RlmYecZOpTdBwkW5cAdLGxY8cObNq0yWbMAHJ5U2T+3GqDyPhctvyiMWBU3pSZo5GJZ9Ebm4y6SU2mLEbJyDzoKaqnOO25MBQAIEaYixcv0ubNmykjI4PatGlD9evXf9gmPVRyc3Ppzp075OHhQYqilOhvX7t2jby8vNTPO3fupLt371J4eLjmeyP56aefqEWLFmRvb1/qunJyciguLo5SUlIoNzeXfH19KTQ0lNzd3Utdt5Hs27ePnJycKCQk5GGbQh999BHFxMTQggULyM/P72GbUyD79+8nV1dXatCggfrdmTNnKCEhgaKiomzKXLhwgbZv304DBgzQfH/ixAnat28fXbp0iYiIfHx8KDw8nOrVq1d6BSgCRubakso1tmK5ODlDpm5ycnLo4MGDlJycXOy8YSvO8lOUNpObm0vbtm2zWZb27duTxWLRnC8by2lpaZSQkEB169YlNzc3unfvHq1Zs4bu3r1L7dq1o7p16+p+Kz4+nv73v/9RRkYGRUVFUbt27Qr1iSiHDx+muLg4GjRokM3jR48epa+//preeecd3bGEhAT69ddfher/5s2btH37dk39t2jRgurUqaM7d9WqVfT888+Tk5NTkctz/fp1slgs5OnpafP4li1byMXFhVq1alXgb2RmZtJbb71FMTExtGHDBqpRo0aR9cuyefNmcnBw0MWUbJ6RaWcyeaaoMVAaY5Oi9s3F7TdLsj8T5c6dO/T555/bzE19+vQhDw8PnYxszhBpm7LI5EBRRPM5EdHp06dp8eLFNmWGDRtGgYGBQjbYas8yuUkmB4pSEjqKkjNl+xrRmJGtS9F2I9M2ZWLTCEo6/okK7tOsXLt2jQYPHkwxMTG0b98+qbZfUu1MJjZlxxoiMVCc8ZkRyOZzI/oaK0XJTT/99JPms6+vLwUFBamf58+fT5mZmfTmm2+WuH1FQTRvyMSmTD6TQTRmjOgDHxZFvdaQ6Te2bNlCn3/+udqfDR48WD127do1IiLNuF6m/o2KGTNQ0PWGEfncKD/LjgNK+7peBiPm6E6ePEkODg4Fttu1a9eSvb099erVS/N9SdenrXFQaYzpHiai43Ojym9k3iztdaczZ85oPru5uWl+d/Xq1UREFB0drX4n4+ey1G9kZGRQTk4Oubq6lqqe0ohnXmz//2RnZ9OxY8dUx/r6+lJwcDA5ODiUqh4fHx+qX79+oXpEZYzQYaSMKGa1y0g9BZGVlUUXL14kf3//Ip2fnZ1NFy5cKPL5sjIyiJaFyBjbzOwzs2JU+R91PzMMwzD/LHbt2kWxsbF08eJFslgsVLNmTerSpUupLJjYgvtN5lHh0qVL9Ntvv2nmAsLCwsjHx8fm+UbNHTDmpKzVv2j8P+p2MXLItBuOAXGKO3bMysqi06dPU+XKlal8+fKlbC1jBoxoZ496W46Pj6e4uDhq1aoV1axZk44dO0Yff/wx5ebmUvfu3Qu8cVJ2vJGdnU0xMTF09uxZCggIoNatW5OdnV2Jl0uEshQDj3pZZGTS0tIoLi5O7Ztq1apFISEhJf7Abl4eVn/2yC+25+bm0uTJk+njjz+mW7duaY6VL1+eRowYQVOnTrV5N6PIIEZGj6iMETqMlBH1s9F2lbZtojqKQnx8PDVp0oRycnJK5fyiyJRUeUratvx21apVizp37vzQ7bJlW1F8JipjhA6Z8lsHpa1bt6YaNWoUeVAqq6eog1+ZwXJZkhE9/+uvv6aOHTsK35VohB4jbJPRkZGRQRaLRb2QSkpKouXLl6sXTC+99JLuTm+jym+EjFF2yfjZKJkHcePGDdq0aZN6B7SZy2JmGaLiXzCnpKTQqVOnyNfXt9BdQIoq8/fff1Pnzp3pwIEDZLFYKDc3l0JCQuj8+fN05coVev3112nOnDmlZpuVoo5piqqjuAtUJe1nI2UA0OnTp6l69epkb29PmZmZtHHjRsrIyKBOnTpRpUqVHlpZZGyTrcvc3Fyb1zu5ubl07tw53Y0dMm1z//79ut1AIiIiqGnTprpz09LSaOjQofTFF1+QoihUsWJFIrr/5CoAeuGFF2jp0qVqv1Kca7qSWqBt06YNrVixggICAmwez8nJ0Uyo/vbbb5SRkUHh4eE2dZVEbBbFLhk9RskUtSzFnTsoiPz9uYxtVkTKLxr/MjoKIysry2ZMytol6zNZmYLyGQD666+/ip3PFi1aRBs2bKCKFSvS0KFD6emnn1aPXb16lcLCwig5ObnIdhWUZ20h4zOi/5tMb9mypUavaLspyRgoSllE+g0rorm2tBc0ZMaOc+bModdee41cXFwoJyeHxo0bRwsWLKDs7GyyWCzUv39/Wrp0qa48JZUDrBSWA0Xj2VZdhoeHU1hYWIH6i+rnrKwsevvtt9V2OWzYMHrxxRfV45cvX6aqVasWOG4uiXFAcXJTfp+VdK4l0tdlSeoo6sKZTFsTzecidblhwwbq1asXeXp6UkZGBm3cuJF69uxJTzzxBNnZ2dGOHTto9erV1KdPH409Innztddeo6ioKHr22Wfp3Llz1K5dO0pMTKRKlSrR1atXqX79+rRlyxaqVq1agT7Iy4PiTKT8xYkBIxaCReb1jI7n0rxJTaYsMjK5ubn01ltv0cKFCykjI4OI7rcrIiJ/f39asGABde7cucjl8fHxoWbNmunKI9ufyfq5UIQ3ni9jvPnmm/D29saSJUuQkpKC9PR0pKenIyUlBUuXLkXlypUxduxYjczly5cRFhYGi8UCe3t7WCwWhIaGwsfHB3Z2dnjzzTdLRI+ojBE6jJQR9bNRdhllm0ycPYhDhw7p3lNUkucXJlPS5Skp28xql6xtojJG6JAt/9dffw07Ozt4eXnBzc0N27dvh6enJ9q2bYuoqCjY2dlhzZo1huuRsassycjoUBQFHh4eGDx4MPbt21ek+jJKjxG2yeiIjIzE+vXrAQCxsbFwcnJCw4YN0bt3b4SEhMDV1RV79+59KOU3QsYou2T8bJTMg8if08xcFrPKpKamom/fvrCzs4O9vT0qV66MypUrw97eHnZ2dujXrx/S0tI0OoYPH66+IzE9PR09evRQ38FmsVjQunVr3TsUbclY39lmS6Z3797o1q0bbt26hXv37mHEiBHq+w937twJLy8vfPjhh8iPjG2FYavfFC0LAOTk5ODtt9+Gp6en7t2Inp6emDhxInJyckrFz2aQSUhIQEBAACwWC2rXro3k5GSEhoaiXLlycHV1RaVKlXDy5Mli+1mmLKK2ydQlANy6dQs9e/aEs7MzKleujEmTJiE7O1s9funSJU2sybTNy5cv48knn4SiKAgICEBYWBjCwsIQEBAARVHw5JNP6t5l+tJLL6FOnTrYunWrxp7s7Gxs27YNQUFBePnll9XvZa7pZH327bff2vyzs7PDwoUL1c9WLly4gBYtWsDOzg4tW7bE9evX8cwzz6i6goKCcOHChWLVv4xdVj3+/v5CemRsE5URLYvs3MGDsJVrZf0sUn7R+Jetly+//BIZGRnq5wULFqjx4OXlhalTp2rOl7FL1mcyMkbks/nz58PV1RWvvvoq+vXrB0dHR8yYMaNAHTJ2yZa/MGzFsky7kYkBmbLI9BuiuVam/mVkZMaOFotFLd/cuXNRoUIFLF++HMeOHcPnn3+OypUrY/bs2RoZmRzwIGzFjWg8X758GS1atBCqS1E/v/POO6hSpQrmzp2Lt99+G+XLl8eQIUM0NimKoiufzDjAiNwkm2sLI39dyuqYPXs20tPT1XPHjBmjvrfe3t4egwYNQmZmpkZGpt2I+kymLps0aaK+C37dunXw9PTEtGnT1OPvv/8+GjdurJERzZtVqlRR38fdq1cvtG3bFleuXAEAXLt2Dc8++yz+9a9/6fwsGmcy5ZeJAaPypui8nlHxLONn0fLLlEVGZty4cQgODsamTZuwfft2tGzZErNnz8aJEycwadIkODk5Ydu2bTqfiZZHpj+TvUZ7EI/8YnuVKlWwdevWAo9v3boVlStX1nwnM4iR0SMqY4QOI2VE/WyUXUbZJhNnISEhhf7Vq1dPM1AQPV9WRqY8RtlmVrtkY0BUxggdsuUXHZQapUdmsFyWZGR0KIqCadOmISQkBIqi4LHHHsO8efNw9epVFIRReoywTUaHh4eHOlERGRmJ0aNHa45PnDgRLVq0eCjlN0LGKLtk/GyUzK1btwr9++WXXzQ5zcxlMauMzAVj3gu58ePHw8/PD7t27UJaWhpiY2NRq1YtvPXWW8WS8fDwwNGjR9XPqampcHBwwK1btwAAn332GerWrYv8iOqR6Tdlyi8z0W6En42S6dq1K7p06YLDhw9j1KhRCA4ORteuXZGZmYl79+6hc+fO6Nev30Mpi6htsouNI0eORFBQENavX49ly5YhICAAzzzzjLoAl3+CWqZt9ujRA+Hh4UhISNDpT0hIQEREhG6i0dPTE3v27NGdbyU2Nhaenp7qZ5lrOlmfWW+SyD/5k/cvb/vs378/IiIi8N1336F3796IiIjAU089hXPnzuHMmTNo0aIFXn31VY0OmdgUtUtWjxEyomWRnTsQ7c+N8rNo/MvWS97ctHz5cjg7O2Py5Mn4/vvv8d5776FcuXJYtmxZseyS9ZmMjBH5rH79+poJ/j179sDb2xuTJk1SdRTXLtnyF4atRVOZdiMTAzJlkek3RHOtUQsaMmNHRVHUthkSEoKlS5dqjn/++ed47LHHNN/J5ACZHCgazzJ1Kern2rVrY9OmTernxMRE1K5dGwMHDkRubq7NdgnIjQOMyE0y7Uy0LmXzuczCmUy7EfWZTF2WK1cOKSkpAIDc3Fw4ODjg8OHD6vGkpCS4ublpZETzprOzM5KTkwEAfn5++O233zTnHzlyBJUqVdL9jmicyZRfJgaMypui83pGxbMRN6nJlEVGxtfXFz///LP6+dy5c3Bzc8O9e/cAANOmTUN4eLjut0TLI9OfldZNtI/8Yrurq6smyeUnPj4e5cqV03wnM4iR0SMqY4QOI2VE/WyUXUbZJhNnTk5OGDBgAKZMmWLzb+jQoZqOUvR8WRmZ8hhlm1ntkrFNRsYIHbLlFx2UGqVHZrBclmRkdOQd+Bw4cADDhw+Hp6cnnJyc0LNnT/z444/Ij1F6jLBNVseJEycA3L/YOnTokOb4qVOnHlr5jZAxsi5F/WyUjPXitqC//Be/Zi6LWWVkJ3OtcdagQQOsXbtWc/zbb79FUFBQsWS8vb1x7Ngx9XN6ejosFguuXbsG4H6OcXJy0tkrqkem35Qpv8xEuxF+NkrG29sbf/zxB4D7YyZFUfDLL7+ox/fs2QN/f/+HUhZR22QXG/39/RETE6N+vnLlCsLCwtC+fXvcu3dPN0Et0zbd3Nxw8ODBAmUOHDigyxkeHh74/fffC5TZv38/PDw81M8y13SyPuvQoQOeeeYZ3ZN49vb2mvxgxdfXF7/++iuA+08xKYqCHTt2qMd37tyJmjVramRkYlPULlk9RsiIlkV27kC0P5exTab8ovEvo8Nafms5wsLCMGfOHM3xRYsWISQkpFh2AXI+k5ExIp+5uLio1xpWjhw5gipVquCtt96yuagnapdM+StUqFDon4eHh06H7FyYaAzI1KVMvyGaa41a0JAZOyqKgr///hsA4OXlpT4ZayU5ORmurq46PTI5QDQHisazTF2K+tlWuzx37hyCgoLQt29fnD9/3uZ8k8w4wIjcJNPOROtSNp/LLJzJtBtRn8nUpY+PDw4cOAAAuH79OhRF0ejcv38/fHx8NDKiebNhw4b44osvAADBwcHYvn275vy9e/eiYsWKut8RjTOZ8svEgFF5U3Rez6h4NuImNZmyyMi4u7sjKSlJ/ZyTkwN7e3tcvHgRAHDs2DFdPyNTHpn+TPYa7UE88ovtnTp1Qvv27dXtNfJy5coVNfHkRWYQI6NHVMYIHUbKiPrZKLuMsk0mzkJDQ7Fo0SKdDit//PGHZqAger6sjEx5jLLNrHbJ2CYjY4QO2fKLDkqN0iMzWC5LMjI68g4urdy9exerV69Gq1atYLFYEBgYWOyyyOgxwjYZHW3atFEnIyMiIrBq1SrN8a+++qrQyczSLL8RMkbZJeNno2Q8PDwwe/Zs7N692+bfsmXLNDnNzGUxq4zsJJP1Qq5SpUqam88A4PTp03BxcSmWTPfu3dGjRw+kpqYiMzMTo0aNQu3atdXj+/bt0+U/GT0y/aZM+WUm2o3ws1EyLi4uOHPmjPrZzc0Np06dUj+fPXu20Anw0iyLqG2yi40uLi7qEzdWbt++jfDwcLRp0wbJycnFnpz18vLC7t27C5SJiYmBl5eX5rs+ffogJCTE5gT9wYMHERoair59+6rfyVzTyfoMAP7zn/+gevXqmifpCpoAdXZ2xtmzZ9XP5cqVQ2Jiovr5zJkzxa5/Gbtk9RglI1IW2bkD0f5cxjaZ8ovGv4wOQJ+bbN0I5+7uXiy7rIj6TEbGiHxWvXp1zZNgVo4dO4YqVaogOjpaFzOidlkRKb+rqyvGjBmDlStX2vybOnWqTodMu5GNAdG6lOk3RHOtUQsaMmNHRVEwffp0zJ8/H76+vvjpp580x+Pj41GhQgXNdzI5QCYHisazTF2K+rlGjRqaGyusnD9/HkFBQWjXrp3NNiY7Dijt3CTTzkTrUrYtyyycybQbUZ/J1GW/fv3QrFkzfP755+jcuTOioqLQvHlznDhxAgkJCYiMjNTtuiCaN1esWAE/Pz/ExMRg9erVCA4Oxo4dO3D+/Hns2rULjz/+eIGvBBDtA0TLLxtnRuRN0Xk9o+LZiJvUZMoiIxMREaHuHgD83w4CVo4cOaLrZ2TKI9OfFecarTAe+cX2s2fPokGDBrC3t0dISAg6dOiADh06ICQkBPb29mjYsKFmIAXIDWJk9IjKGKHDSBlRPxtll1G2ycTZyJEj8e9//1tnr5VTp06hVatW0ufLysiUxyjbzGqXjG0yMkbokC2/6KDUKD0yg+WyJCOjI++2SbZITEzEhAkTil0WGT1G2CajY+/evShfvjzeeecdLFiwAJUqVcLEiROxZs0aTJ48GZ6enrqt04wqvxEyRtkl42ejZFq1aqX7Li+HDh3SbGtn5rKYVUbmglFRFAwdOhSjR49G5cqVdTsmxMXF6bbpE5VJSkpCrVq1YG9vDwcHB3h6emqeUFixYoVuO3AZPTL9pkz5ZSbajfCzUTK1atXSPPW1aNEi3L59W3O+rQlwI8oiapvsYmPdunXx/fff676/c+cOwsPD0ahRo2JPzr7yyisICAjAhg0b1N2WgPtbnm7YsAGBgYEYMWKERub69evo0KEDFEVBxYoVUa9ePdSrVw8VK1aExWJBx44dcePGDfV8mWs6WZ9Z+eOPP1C/fn0MGTIEaWlpBU6A+vv7a7YNHTdunHojLHC/zyhu/cvYJavHKBmRssjOHYj25zK2yZRfNP5ldAD3c9Pq1avx7bffws/PD3v37tUcP3r0qGbCVMYuWZ/JyBiRz1544QWMGjXKpv6jR4/C29tbt6gnaldeilr+iIgI3Svm8mJrG3mZdlOcGBCpS5l+QzTXGrWgITN2DAgIQGBgoPo3b948zfEPP/wQzZs313wnkwNkcqBoPMvUpaifX3rpJbz44os2y3Du3DnUrl3bZhsrzjigNHOTTDsTrUvZtiyzcCbTbkR9JlOXly5dQrt27eDm5oaoqCjcvHkTI0aMUHcBqFOnjuaGFUAub37wwQdwdXWFi4uL+j5w61+3bt1w584dnc1WihpnMuWXiQGj8qbovJ5R8WzETWoyZZGR2bFjB5ycnBAWFoaWLVvC3t5e09fMnTsXbdq00dksWh6Z/qy412gF8cgvtgP3tzD44YcfMHnyZAwZMgRDhgzB5MmTsWXLFuTk5OjOl50AE9UjI2OEDqNkZPxsVFmMsE02zsyKWctjVrtkbROVMUKHLDKDUiP0yNhVlmRkdNh64rik60VWjxG2yegA7i8cNm/eXPferGrVqtmc6DKq/EbIGGUXIO5no2Q++eQTzJ8/v0C7L126hClTpvwjymJWGZkLxsjISLRq1Ur9y/ueWQB49913ERkZWWyZtLQ0bNu2DZs2bbJ5AWgLGT2iyOiQmTAyys9GyAwdOlR3Tl5mzpyJTp06PZSyiNomu9j42muv6W6Qs3L79m00a9as2BPA9+7dw7Bhw9QJRmdnZzg7O8NiscDR0RHDhw9X3w+Yn+PHj2P58uWYMWMGZsyYgeXLl6uvpMiP6DWdrM/ykp6ejqFDh6JOnTqws7OzOQHapUuXQhfBFi5cqJvMkolNUbtk9RglY6WoZZGZO5Dpz2Vsky2/SPzL6MjfH+d9wgkAPv30U8028jJ25aeoPpORMSKfxcfHY/ny5QXaeuTIEV3MiNqVn6KUf/r06YXG6tmzZzFw4EDd9zLtBpCPgaLWpUy/IZprjVrQAOTGjoXx66+/6hY6ZHKATA4UjWeZuhT18+nTpwvdcvj8+fNYuXKl7vvijgNKKzdZEWlnsv3ZiRMnhNqyzMKZTLsR9VlJjOmsJCUl4ciRI8jKyrJ5XCZv3rhxA19++SVmzZqFGTNmYMWKFTh58mSR7ClKnBWn/CJxZlTelJ1zLu14NvImNZl+VrT8hw4dwoQJEzBmzBibr3a0RXFvvMyPrf6sJNtzXhQAIEaY9PR0io2NpczMTGrevDlVqlTpYZtUJjGzn42wzczll8Gs5TGrXURytonKGKGjJElOTqb09HSqV68e2dvbm0aPjF1lSaaw88+cOUP+/v6kKEqRdBupxwjbiqvjypUrlJycTLm5ueTr60uBgYE2zzOq/EbIPIy6LKqfH4aMKGYuixllTpw4Qfv27aNLly4REZGPjw+Fh4dTvXr1HqgnP8nJyeTo6Eh+fn6lKiODEXoK0pGbm0vbtm2z6ef27duTxWIpET3/RJmUlBRydnYmX19fU9lVkG0ydXnjxg26cOECPfbYYzb13Llzhw4ePEiRkZGa7xMSEujXX38Vapu3b9+muLg4jUxoaCh5eHgUudwlTUnF/3fffUcxMTE0fvx4qly5spAN+/fvJ1dXV2rQoEGRZYoam8WxS0SPETLFLUtp8jD8bISOzZs3k4ODA0VFRZW4PTI+e5CMbD4rybGGLWTtyo+Z24AoRS1LSfYbBeVamf5MRuZhU1J5RjaeZeqytNsmUcmMA0orN/0T2bdvHzk5OVFISIjumEi7kfFZSV/TmI0HxZmR5ZdpmyXRnpOSkuju3bulPudsxVY8y/rZiHxmJKVdntKIZ15s///s379fl4wjIiKoadOmD12PqIwROoyUEcWsdj1sPeHh4RQWFlYi58vKGFEWo2wzq11mxswxwzAMwzBm4u7du7Ru3TqKjY2lixcvksVioZo1a1K3bt3o6aefLlFd3G8yjzKZmZn0zTff2LxG69q1Kzk6OhYqn5KSQqdOnSJfX1+hhWzmn4tR1/RGUNz4Ly3i4+MpLi6OWrVqRTVr1qRjx47Rxx9/TLm5udS9e/dSWZg3I7m5uTYnenNzc+ncuXPk7+//EKwqPkXJm2aNTTNTUmPHmjVr0rZt26hOnTqlaO0/l3PnzpGnpye5ublpvs/KyqJff/2VWrZs+ZAsk+NhlufGjRu0adMmio6OLjUdZmDz5s20f/9+ioqKohYtWtCuXbvo/fffp9zcXHruuedoyJAhNuVE+wAzX9Pt2rVLl5u6dOnyj84zAGj37t1qfxYVFUUODg4P26wiU9yxlkj5H9Z45mH0Z4/8Yvvff/9NPXr0oD179pC/vz9VqVKFiIguX75MZ8+epRYtWtDXX3+tu5tHdBAjo0dUxggdRsqI+tlIu4ywTVSHjJ6y5rNHPc5kZUpbh5ljxqw+M7uMWe0ys4xZ7TKzjFntMrOMWe0ys4zMZG5p23Xq1Clq27Yt3b17l5ycnOjcuXPUqVMnunr1Kh04cICee+45Wrt2rc077c04PrUiukBl1piRkTGrXbIyMouNxVkEKOpkjuhk5qlTpygqKoouXLhAzZo107SB3377jfz8/GjLli1Uu3ZtIiJ65ZVXaM6cOeTm5kZ3796l/v3704YNG4iISFEUioyMpO+++043YW2kz0Qnzc0cZ2Zrm8W51iiMy5cv09KlS2ny5MmGll80/otjV34KW2zdsGED9erVizw9PSkjI4M2btxIPXv2pCeeeILs7Oxox44dtHr1aurTp0+xym+kzLVr1+jw4cPUqFEjqlixIl29epX++9//UkZGBvXs2ZOCg4M159++fZtefvll2rRpE3l4eNDQoUPpnXfeITs7OyK6X0dVq1alnJych1J+kUUTmbxpVGyeO3eOnJ2d1V36fvnlF1qyZAmdPXuWAgIC6NVXX6Xw8HCdnMjYMSMjgywWi9pnJSUl0fLly1UdL730EtWoUcOmnwvC1gKlzNjxo48+svn7r7/+Oo0dO5Z8fHyIiGjkyJGa42ZsZ7J1KbLYdPHiReratSvFxcWRoijUp08fWrRokRq7BbVLWxTlhpPS9plseUryhuD4+Hhq0qRJgT4riRsBHrTYJlMekYXDpUuX0ogRI6hRo0aUmJhIH3/8Mb3yyivUu3dvsrOzo9WrV9PMmTPp3//+tyoj2gcYORcKgE6fPk3Vq1cne3t7yszMpI0bN1JGRgZ16tRJt+vp33//TZ07d6YDBw6QxWKh3NxcCgkJofPnz9OVK1fo9ddfpzlz5tj0s8wCvUjMiOaNTp060bp166h8+fJ0/fp16tSpE+3fv58qVapE165do6CgIPr555/J29u7WLZ9/fXX1LFjR3J1dS2wnLYQyWcyYy2Z8suOZ4jE+lrZ/iw/JXITtfDG82WMHj16IDw8HAkJCbpjCQkJiIiI0L2/IzExEQEBAahcuTKqV68ORVHwzDPPoFmzZrCzs0PPnj1179yQ0SMqY4QOI2VE/WyUXUbZZkSclTWfPepxJiNjhA4zx4xZfWZmGbPaZWYZs9plZhmz2mVmGbPaZWaZxMRE1KxZE87OzoiMjESvXr3Qq1cvREZGwtnZGbVr10ZiYqLhdnXs2BFDhw5Fbm4uAGDWrFno2LEjAODkyZMIDAzEO++8g/yYdXx6+fJlPPnkk1AUBQEBAQgLC0NYWBgCAgKgKAqefPJJXL582XA/GyVjVrtkZGTqUkZPx44dcfPmTQDAtWvX0KxZMyiKAm9vb1gsFtSrVw9///23RseSJUtgb2+P0NBQeHh44LPPPoO7uztefvllDB06FC4uLrr37LZt2xZdu3bFrVu3dDbfunULXbt2Rfv27dXvLBaLWr7x48fDz88Pu3btQlpaGmJjY1GrVi289dZbmt8xymcXLlxA06ZNYbFYYGdnh/79++POnTvq8UuXLune1WrWODNKxqhrjQdx6NChh1I3ovEva9fw4cPVWExPT0ePHj1gsVjUd6K2bt1aE6tNmjRR3+u+bt06eHp6Ytq0aerx999/H40bN9bZbNY4++2331C+fHkoioIKFSrgwIEDqFGjBurUqYNatWrBxcUFcXFxGh0jR45EUFAQ1q9fj2XLliEgIADPPPMMMjIyANxvz4qiGF6Wy5cvIywsDBaLBfb29rBYLAgNDYWPjw/s7Ozw5ptv6upFJm8aFZthYWHYtGkTAOCbb76BxWJBly5dMG7cOHTv3h0ODg7q8bx6RMaOkZGRWL9+PQAgNjYWTk5OaNiwIXr37o2QkBC4urpi7969unIWhq2cITN2VBQFfn5+mncJBwYGQlEUVKtWDYGBgahRo0ax/WyEjExdfv3117Czs4OXlxfc3Nywfft2eHp6om3btoiKioKdnR3WrFmjnh8dHY1mzZrh999/x/bt2xEaGoonnngC169fB2C7XQK2c6CiKAXmQKN8JlMeUR23bt0q9O+XX36x+R55mTHN/Pnzbf7Z2dlh/Pjx6ufilqdnz55wdnZG5cqVMWnSJGRnZxdqV/369fHJJ58AAHbt2gVnZ2d8/PHH6vEVK1YgODhYIyPaBxg1F5qQkICAgABYLBbUrl0bycnJCA0NRbly5eDq6opKlSrp3hHfu3dvdOvWDbdu3cK9e/cwYsQIREdHAwB27twJLy8v3fhcpq+RiRnRvKEoitqfDR8+HPXr10dycjIA4K+//kJoaCiGDRtWbNsURYGHhwcGDx6Mffv26X7PFqL5TGasJVN+mfEMIN7XyvRnMrm5KDzyi+1ubm44ePBggccPHDgANzc3zXcygxgZPaIyRugwUkbUz0bZZZRtRsRZWfPZox5nMjJG6DBzzMjoMcJnZpYxq11mljGrXWaWMatdZpYxq11mlpGZzDXCLldXV81ERUZGBhwcHHD16lUA9ycDAgMDdTabdXwqMwFk1piRkTGrXTIyspN5onpkJnNkJjNdXFxw5MgRnb1WDh8+DBcXF5t2NWjQAGvXrtWc/+233yIoKEjznVE+k5k0N2ucGSVj1LVGfHx8oX9ffvlliSycicqIxr+sXaKLreXKlUNKSgoAIDc3Fw4ODjh8+LB6PCkp6aFdO8vItG3bFi+//DJu376NuXPnws/PDy+//LJ6fNCgQejWrZtGh7+/P2JiYtTPV65cQVhYGNq3b4979+7ZXDQwoiwyiyYyedOo2CxXrpzatzRr1gyzZs3SHF+wYAFCQkI034mOHT08PNQxXWRkJEaPHq2RmThxIlq0aKH7HdEFSpmx49ChQ9G4cWMcP35c8729vT2OHTumKx9g3nYmU5eii01Vq1bFb7/9pn6+d+8eOnfujMaNG+PatWs22yUgd8OJET6TKY/MeM5isRT4Zz2eH5kxjcxim2h5ZBYOXVxccObMGfWzg4ODJr+lpKTA1dVVIyPaBxg1F9q1a1d06dIFhw8fxqhRoxAcHIyuXbsiMzNTjZ9+/fppdHh4eODo0aPq59TUVDg4OKg59LPPPkPdunU1MjJ9jUzMiOaNvP1Z3bp18e2332rO37Fjhy7GZGxTFAXTpk1DSEgIFEXBY489hnnz5qn53Bai+UxmrCVTfpnxDCDe18r0ZzK5uSg88ovtXl5e2L17d4HHY2Ji4OXlpflOZhAjo0dUxggdRsqI+tkou4yyzYg4K2s+e9TjTEbGCB1mjhkZPUb4zMwyZrXLzDJmtcvMMma1y8wyZrXLzDIyk7lG2FW1alXNU243btyAoii4ffs2ACA5ORlOTk46e806PpWZADJrzMjImNUuGRnZyTxRPTKTOTKTmb6+vron3fLy3XffwdfXV2OX9Yn6SpUqaSYPAeD06dO6nGGUz2Qmzc0aZ0bJGHWtYV1MsD4tk/evoMUGI8ovGv+ydokutvr4+ODAgQMAgOvXr0NRFM1E7f79++Hj46Oz16xxVqFCBXXyNzMzExaLRdNW4+LiUK1aNY0OFxcXdQHAyu3btxEeHo42bdogOTn5ocSMzKKJTN40KjbLly+P+Ph4AEDlypXV/62cOnVK12+Ijh3LlSuHEydOAACqVKmCQ4cO6XTYWtAQXaCUHTtu2LAB1atXx4IFC9TvClucMGs7k6lL0cWmcuXKaWwCgKysLHTr1g0NGzbE4cOHbS4aydxwYoTPZMojkzNmz56N3bt32/xbtmyZTZ/JjGlkFttEyyOzcOjn54eff/4ZAHD+/HkoioLvv/9ePb579274+flpZET7AKPmQr29vfHHH38AuJ//FUXBL7/8oh7fs2cP/P39NTq8vb01/k9PT4fFYsG1a9cA3G9n+XOTTF8jEzOieSNvf1a5cmWb/ZmtPCtqW96cceDAAQwfPhyenp5wcnJCz5498eOPP+p0iOYzmbGWTPllxjNWOdF5GtH+TCY3FwX9CyYeMXr37k0DBgygjRs30u3bt9Xvb9++TRs3bqRBgwbRCy+8oJHx9PSkO3fuqJ/T09MpOztbfVdAw4YN6eLFi8XWIypjhA4jZUT9bJRdRtlmRJyVNZ896nEmI2OEDjPHjFl9ZmYZs9plZhmz2mVmGbPaZWYZs9plZhlPT086ffo0FcTp06fJ09PTcLvatWtHr7/+OiUkJFBKSgoNGzaMGjduTO7u7kREdPbsWZvv3DPr+NTJyUnz+/m5c+cOOTk5FVuPWWXMapeMjExdytqmKAoR3X83ba1atTTHateuTRcuXNB85+XlRWfOnCEiogsXLlB2djadPXtWPX7mzBmqWLGiRubll1+m6OhomjdvHh0+fJguX75Mly9fpsOHD9O8efNo4MCBuve8T5o0iV5//XWyWCw6G65du0blypXTfGeUz27dukUVKlTQ6N2wYQMFBgZS69at6e+//y62jrImY9S1RsWKFWnZsmWUkpKi+0tOTqbNmzfrZIwov0z8y9hF9H/t+dKlS9SwYUPNsUaNGtFff/2lfm7bti29+uqrtGbNGhowYAC1b9+exo8fTwkJCfTnn3/Sm2++SU8++eRD8ZmMTGZmJrm4uBARkYODA7m6umreaWt912le/P396cSJE5rv3N3d6ccff6S7d+9S9+7dH0r5nZyc1LokIrJYLJSTk0PZ2dlERBQREWFzXCWaN42KzcjISFq3bh0REYWEhNDu3bs1x2NiYqhatWo6PSJjx2bNmtGmTZuIiKhWrVoUHx+vOf/QoUO6vsnd3Z1mzpxJu3btsvn3ySef6PTKjh27d+9Ov/76K23cuJE6duyovhe3IMzazmTq0t3dXW17N2/epOzsbE1bvHbtmubdyjVr1qTDhw9rfsPe3p7Wr19PNWvWpGeffVbnLysiOVCm/DIyMuUR1dGkSRMiul8/tv6aNm1KAHR6ZMY0S5YsocmTJ1NUVBQtXLhQd9wWouW5cuUKBQQEqJ8rVapEO3bsoDt37lCnTp0oPT1dp6Nr16700ksv0fTp06l79+4UHR1NY8aMoa1bt9K2bdvotddeo/bt22tkRPsAo+ZCU1NT1XxVrlw5KleuHPn6+qrHq1evTpcvX9boePLJJ2ny5MmUlpZGWVlZNGHCBKpZs6b6O1euXNHUNZFcXyMTMzJ5Y+DAgfTcc89RVlYWpaSkaI5dunRJN3cga5uV0NBQWrRoEV28eJGWLVtGV65coQ4dOlCNGjU054nmM9mxlmj5ZcYzRHLzNKL9GZF4bi4SwsvzZYx79+5h2LBhcHR0hMVigbOzM5ydnWGxWODo6Ijhw4fj3r17GpkBAwYgMjISJ06cQHJysvquHSu7d+9G9erVi61HVMYIHUbKiPrZKLuMss2IOCtrPnvU40xGxggdZo4Zs/rMzDJmtcvMMma1y8wyZrXLzDJmtcvMMpMmTUKFChXwn//8B/Hx8bh06RIuXbqE+Ph4/Oc//0HFihV1WzQaYdfly5fRvHlz9cnHgIAAzdNK69evx0cffYT8mHV8+sorryAgIAAbNmzQbAV369YtbNiwAYGBgRgxYoThfjZKxqx2ycjI1KWMHkVR0KlTJ3Tv3h0VKlTQPeG4b98+VKlSRfPdq6++ijp16uC9995DWFgYBgwYgHr16mHLli3YunUrHn/8cbz44os622bNmgVfX1/Nk4SKosDX1xezZ8/WnBsZGYlWrVqpf8uWLdMcf/fddxEZGan5ziifPf744/jqq690v2N9Ss3f31/35IhZ48womZLKmYqiFHqt0b59e7z77ru6760cOnRIt7WpUT4TiX9ZHYqiYOjQoRg9ejQqV66seyIrLi4OlSpVUj9funQJ7dq1g5ubG6KionDz5k2MGDFCtbFOnTo4depUidhmhEy9evWwc+dO9fPmzZuRnp6uft63b5/uqcbXXnvN5uslgPtPhDVr1uyhtOfu3bujR48eSE1NRWZmJkaNGoXatWtrypL/STiZvAkYE5vHjx+Hl5cXoqOj8e6778LNzQ39+vXD9OnTER0dDScnJ6xYsUIjIzp23Lt3L8qXL4933nkHCxYsQKVKlTBx4kSsWbMGkydPhqenp648rVq1sllGK7ZyhuzY0Upubi5mzJihvhO5oCcBzdrOZOqyX79+aNasGT7//HN07twZUVFRaN68OU6cOIGEhARERkZq2uHYsWN1r5eykpWVhS5duhT4ZLtIDjTKZzLlEdXxySef6N6TnpdLly5hypQpuu9lxjRWzp07hzZt2qBDhw64ePFioU+2ipanbt26mqfSrdy5cwfh4eFo1KiRzq7U1FQMHjwYDRo0wJAhQ5CRkYG5c+fC0dERiqKgVatW6tO1VkT7AKPmQmvVqqV5kn3RokXq7hnA/VjO3wckJSWhVq1asLe3h4ODAzw9PbF9+3b1+IoVK3Rbdcv0NTIxI5o3Bg4cqPn78ssvNb/35ptvIioqSmeDqG15tze3RWJiIiZMmKD5TjSfyYy1ZMovM54B5OZprBS1P5PJzUXhkV9st3Lr1i3s2rULa9euxdq1a7Fr1y6b7wUAtIMYi8WCgIAAzdZwhQ1iRPTIyhihwwgZWT8bURYjbDMyzsqKz2RkzGqXrG2iMkboMLL8RugxymdmlTGrXWaWMatdZpYxq11mljGrXWaXEZ3MNbLfPHnyJI4cOYLs7GzdMVuYdXwqMwFk5ph5lNum7GSeqB6ZyRyZycy8JCcnY+/evdi7d69uu8OikpSUhL/++kvznVE+K8qkeWGLM2aKM6NkipMzd+7cqebMnTt3FnqtsWHDBnz22WcFHr9+/TpWrlxpePnzUtT4l9Ehu9ian6SkJBw5cgRZWVklZpsRMlOmTMG6desKLNeECRPw3HPPab67fv26bnvWvNy+fVu3ZbARZbG1aJJ3ctrWoklBWN8RbCtv5qU0YxO4v03w888/D3d3d3Wh2sHBAREREdi4caNNXaJjx71792oWwq1/1apV0713GLi/QGnreysFLVAC4mPH/Bw4cAAffvih+i7h/Ji1nQHidSm62JSVlVVors/KysLp06d138vkQCN8JlOe4vQzebG2/4KQGdPk//2iLLaJlkd24dAWd+/e1SxU50WmDwBKfy506NChuvjNy8yZM9GpUyfd92lpadi2bRs2bdqEK1euFChvRaavkY0ZmT6gIFJTU3H37l3d96K25d3evKgUls8URSnwRsX8PGisVRi2yi8by4B4X5ufuLi4Qvuzkhqf5kcBbOzXwRSJxMREysjIoHr16pG9vf3DNqfMYmY/G2Gbmcsvg1nLY1a7iORsE5UxQocsZtVjlM/MKmNWu8wsY1a7zCxjVrvMLGNWu8wuk5KSom415uPjo9uazWi7Ll68SIsXL6bY2Fi6ePEiWSwWqlmzJnXr1o0GDhxIdnZ2JWqbKDI6bt++TXFxcRo/h4aGkoeHR4nqMauMWe2SkZGpS1nbbJGWlkZ2dnbk7Oz8wHPv3btHWVlZ6la6D4vbt2/TgQMH1O01S9pn2dnZlJ6eXuDvZWdn0/nz5zXbn4rqKKsyxY1LR0dHio+Pp+DgYGHZ0rDNiL6mJPuZ5ORkcnR0JD8/v2L9TnFse5jXwunp6WRnZ2fzdRIylHZZ0tPTac+ePZSRkUHNmzfXbIkvQmm1G9l6AUB///035ebmUqVKlcjBweGBMqJjxytXrlBycjLl5uaSr68vBQYGFtm+olCcsaMoZm5nMnWZl+TkZEpPT7epszR8XFgONOs8XWn3m8UZ0+QlLi6OYmNjKTo6WrdVeV6KWp4bN27QhQsX6LHHHrN5/M6dO3Tw4EGKjIws1C6zUVIxk5KSQs7Ozpqt5WUR7WuKGzPFzRsladuZM2fI399fs5W+LIXlM1uU5phWlqL2tSWdn6XHp8LL82WQ9PR0/PLLLzbvdLp79y5WrVol9Htnz57FoEGDSkSPqIwROoyUKQxbfjaDXSVpm6gOGT1lzWePepyVtExJ6TBzzMjoKcnzy5qMWe0ys4xZ7TKzjFntMrOMWe0ys8zDsuv3339H+fLlERoaiieffBJ2dnbo378/evfuDU9PT0RERBT4JISobUaNT/OSmpqK5cuXY8KECVi4cCGuXr0qpKOoev4pMma1qyCZ48ePY/ny5Thx4gQA4MSJExg2bBgGDRqk2Sq5NG0rqbLExcVpnpRcvXo1IiIi4OfnhxYtWth8GlW0zYwYMQI///yzkK1FoazHmRlk8p8/evRom38WiwXR0dHqZ1HKUl8j2gcsWLDAZh9ghms6o2RKcv6wJO0qikxR6lK23SxYsAD9+/dX8/Dq1asRHByMunXrYvz48cJP3Zml/osyBpLpN2Tbc1n1c1HOF/WzrI9LYxxQUj4r7fova/0mIDcOlvGzEXP7D6Ik+oCSsK2oegrDrG2mID0ycWaVSUhIeKCMTNuUuW4CjIlN2fws4rOi8sgvtv/5558ICAhQtyRo2bIlzp8/rx6/dOlSkbcAsXLo0CGdjIweURkjdBgp8yDy+9ksdpWUbaI6ZPSUNZ896nFWGjIlocPMMSOjp6TPL2syZrXLzDJmtcvMMma1y8wyZrXLzDIPy64WLVpotgf97LPP0KxZMwD3t2Jr3LgxRo4cKaTDlh6jxqfBwcG4du0agPsXxoGBgShfvjyaNm2KihUronLlysLbdps1ZmRkzGqXLZktW7bA0dERFStWhLOzM7Zs2QJvb2+0bdsWbdq0gZ2dndTExMPyWcOGDdX3Ri5btgwuLi4YOXIkFi9ejFGjRsHNzQ3//e9/1fNttZkLFy6ox221mbzb0c6aNQsXL14UslukPCV5Psvoz1cUBY0bN9ZsOdmqVSsoioKmTZuiVatWaN26tZBNMnaVlExp9DVF6QMCAgIK7QNk2pmsbWaQKeq8TnF9UBJlEa1LQK7dvPvuu3B3d0ePHj3g4+ODWbNmwcvLC++99x5mzJgBb29vTJ482fDyy8jIjIFk+g2Z9lyW/CxzvqifZXNmaYwDSsJnRtR/Wes3ZcbBMn4W7QOMmguVyWcy/ZlMXyNaFhmZ0mgztvTIxJmojEzbFL1uAowb08nk59K6rjXXXskPgXHjxlGDBg3owIEDdPPmTRo1ahQ9+eSTtHv3bvL397cp89133xX6m8nJySWiR1TGCB1Gyoj62Si7jLLNiDgraz571ONMRsYIHWaOGRk9RvjMzDJmtcvMMma1y8wyZrXLzDJmtcvMMma16+DBg7R69Wr1c58+fejFF1+ky5cvU5UqVWjOnDk0cOBAmj9/frH0GDU+TUhIoOzsbCIiGj9+PFWtWpUOHTpE5cuXp9TUVOrevTu9/fbbtHbt2mLpMauMWe2SkZk2bRq9+eab9N5779EXX3xBffr0oeHDh9P06dOJ6H79zpo1i9q0aVMsPUaVPzExkerUqUNERIsWLaL58+fT4MGD1eNNmzal6dOn04svvkhEtttMixYtHjjW/PHHH2nTpk30/vvv06RJk6hjx440ePBg6tSpE1ksFpsyZvVZWZIRPX/GjBn0ySef0AcffKCJcQcHB1q5ciXVr1/f5u+YtfwyfU1J9AHVqlWj+Pj4AvsA2XZmVj+X1LzOg3xgRFlE65JIrt2sXLmSVq5cSc899xzFx8dTaGgorVq1ivr27UtERPXq1aOxY8fS1KlTDS2/jIzMGIhIvN+Qac9lyc8yOojE/Cw7PhfVI1seURkj6r+s9Zsy42AZP4v2AUbNhcrkM5n+TKavMWubkdEjE2eiMjJtU/S6ici4MZ1Mfpa9rn0gwsvzZYzKlSvj8OHD6ufc3FwMGzYM/v7+SEpKKvTudEVRCvzLLyOjR1TGCB1Gyoj62Si7jLLNiDgraz571ONMRsYIHWaOGbP6zMwyZrXLzDJmtcvMMma1y8wyZrXLzDJmtSsgIACxsbHq5wsXLkBRFKSnpwMAUlJS4OzsjPyI6jFqfKooCi5fvgwAqFmzJn788UfN8T179qB69eqG+9koGbPaJSPj4eGBxMREAEBOTg7s7e1x8OBB9fiRI0dQpUoV5MesPvPy8sKBAwcA3G8Phw4d0hw/deoUXFxc1M+ybcYa/5mZmfjyyy8RFRUFOzs7VK1aFRMmTFB9+k/wWVmSkdGxf/9+BAUFYcyYMcjMzAQA2Nvb29wa0+zll+lrZO0S6QPMfO0sIyOjw8j+WbQsov05IN5uXFxccObMGfWzg4MDjh49qn4+ffo0XF1dDS+/UT7LK1PUfkOmPZc1PxcnNxXFz8UZn4vWpxE+M6r+y1K/KTMOlvGzEXP7MuWXyWfFHTub6dpRpi5l9MjEmYyMaNsUvW6ynmdEbMrkZ9nr2gdh+zbqR4i7d++Svf3/PeCvKAotXryYOnfuTJGRkXTy5EmdjK+vL23YsIFyc3Nt/h08eLBE9IjKGKHDSBlRPxtll1G2GRFnZc1nj3qcycgYocPMMWNWn5lZxqx2mVnGrHaZWcasdplZxqx2mVnGrHZ169aNhg0bRlu3bqWYmBjq27cvRUZGkouLCxER/fnnn1StWrVi6zFqfGr9bSKie/fuka+vr+ZYtWrV6MqVK4b72SgZs9olK2OtS4vFQs7OzlS+fHn1mLu7O926desf47OOHTvS4sWLiYgoMjKSvvrqK83x//3vf1S7dm31s+xY04qDgwP16tWLtm7dSsnJyTR48GBas2YN1a1b9x/js7IkI6OjadOmFBcXR1euXKEnnniCjh49qraJgjBr+WX6GiP6ADNfO8vIGDV/aHQfUNT+nEi83fj4+NDx48eJ6P6TdDk5OepnIqJjx45R5cqVH0r5jfKZlaL2GzLtuSz5WTY3ifhZdnwuqsconxlV/2Wp3yQSHwfL+NmIuf3ilr+o+UzWNjNeO8rUpaxtMtdbojKibVP0uonIuNiUzc8yfn4Qj/xie7169ejAgQO67xcuXEhdu3alLl266I6FhoZSXFxcgb+pKAoBKLYeURkjdBgpI+pno+wyyjYj4qys+exRjzMZGSN0mDlmZPQY4TMzy5jVLjPLmNUuM8uY1S4zy5jVLjPLmNWu9957j+rXr0+dO3emp59+mjIyMmj58uWa82fOnKn7HbOOT4mInn76aWrSpAndvn2b/vzzT82xM2fOkJeXV7H1mFXGrHbJyAQGBlJiYqL6+ddff9VsAXj27FndhJiMHqPKP3v2bNq5cydFRkZS9erV6YMPPqCnnnqKhgwZQpGRkTRlyhSaNWuWer7sWNMW/v7+NGXKFEpJSaGtW7fqjpvVZ2VJRjafubm50apVq2j8+PHUtm1bysnJKfA3ZPWYta8xog8w87WzjIxR84dGlV+0P7ci0m769u1L0dHRNHjwYIqKiqKxY8fSG2+8QUuWLKGlS5fSsGHDqHv37sUui9l9lp/C+g2Z9lyW/Cybm2xRkJ9lx+eiemTLIypjVP0TlZ1+U2YcLONnI+b2ZcpPJJ7PZG0z47WjTF3K6JGJM9lrNJG2KXrdRGRcbMrkZ1mfPZAiPf9ehpkxYwY6duxY4PHhw4dDURTNdz///DO2bNlSoExqaip2795dbD2iMkboMFJG1M9G2WWUbUbEWVnz2aMeZzIyRugwc8zI6DHCZ2aWMatdZpYxq11mljGrXWaWMatdZpYxq11W7t69izt37hQolx+zjk+nTJmi+du6davm+BtvvIHnn3++2HrMKmNWu2RkFi9ejM2bNxd4/vjx4/HSSy/pvjerzwDgxo0bGDduHOrXrw9nZ2c4OjoiICAAffr0we+//645V6bNBAYG4urVqwXKFIRZfVaWZGRjJi9//fUXvvnmG6SmphZ4jlnLb0WkrzGiDzDztbOMjFHzh0aURaY/t8WD2k1OTg6mT5+OZ599FjNmzEBubi7WrVuH6tWrw8vLCwMHDtTJmrX+ZXwm228AYu25LPlZRoesn0XH5zJ6jPCZUfWfn39yvykzDpbxsxFz+4Ax+UzGNrNeO8rUpYwemTiTvUbLS1Hapsh1E2DsmA4Qy88l4TNbKEARb+1iGIZhGIZhGIZhGIZhGIZhGIZhGIZhGIaIeBt5hmEYhmEYhmEYhmEYhmEYhmEYhmEYhhGGF9sZhmEYhmEYhmEYhmEYhmEYhmEYhmEYRhBebGcYhmEYhmEYhmEYhmEYhmEYhmEYhmEYQXixnWEYhmEYhmEYhmEYhmEYhmEYhmEYhmEE4cV2hmEYhmEYhmEYhjERK1euJE9Pz4dthgZFUeibb7552GYwDMMwDMMwDMMwjKngxXaGYRiGYRiGYRiGKSIDBw4kRVFIURRycHCgGjVq0NixY+nevXslpqN379508uTJIp1b0gvzU6ZMocaNG+u+v3jxInXs2LFEdGRkZNBjjz1GQ4YM0R0bO3Ys1ahRg+7cuVMiuhiGYRiGYRiGYRimNLF/2AYwDMMwDMMwDMMwzD+JDh060IoVKygrK4vi4uJowIABpCgKzZ49u0R+38XFhVxcXErkt6xkZmaSo6OjtLyPj0+J2eLk5ESrV6+m8PBw6tGjB0VFRRER0b59+2jevHm0Y8cOcnd3LzF9REQAKCcnh+zteRqEYRiGYRiGYRiGKTn4yXaGYRiGYRiGYRiGEcDJyYl8fHyoevXq1K1bN2rbti1t376diIhyc3Np5syZVKNGDXJxcaFGjRrRV199pZH/7rvvqE6dOuTs7EytW7emVatWkaIodPPmTSLSP60eHx9PrVu3Jnd3d/Lw8KDQ0FA6cOAA7d69mwYNGkS3bt1Sn7afMmUKEREFBgbSu+++S9HR0eTh4aE+RT5u3DgKCgoiV1dXqlmzJk2aNImysrJUvVOnTqX4+Hj191auXElE+m3kjxw5Qm3atCEXFxfy8vKiIUOGUGpqqnp84MCB1K1bN3r//ffJ19eXvLy86NVXX1V1hYaG0ttvv00vvfQS3bx5k+7du0eDBg2i1157jSIjIyk2NpaeeuopcnFxoerVq9PIkSMpLS1N/f3PPvuMnnjiCXJ3dycfHx/q06cP/f333+rx3bt3k6IotGXLFgoNDSUnJyeKjY2Vr3SGYRiGYRiGYRiGsQEvtjMMwzAMwzAMwzCMJEePHqW9e/eqT43PnDmTVq9eTUuWLKFjx47R6NGjqV+/fvTTTz8REVFKSgr961//om7dulF8fDwNHTqU3n777UJ19O3bl/z8/Oj333+nuLg4euutt8jBwYEiIiLoww8/JA8PD7p48SJdvHiR3njjDVXu/fffp0aNGtEff/xBkyZNIiIid3d3WrlyJR0/fpzmz59Py5Yto3nz5hHR/e3rx4wZQ4899pj6e71799bZk5aWRlFRUVShQgX6/fffaf369bRjxw4aMWKE5ryYmBhKSkqimJgYWrVqFa1cuVJdvCcievvtt8nHx4dGjhxJEydOJEVRaMaMGZSUlEQdOnSgHj160OHDh+nLL7+k2NhYze9nZWXRu+++S/Hx8fTNN9/Q6dOnaeDAgTpb33rrLZo1axadOHGCGjZsWKifGYZhGIZhGIZhGEYU3j+NYRiGYRiGYRiGYQTYvHkzubm5UXZ2NmVkZJDFYqGFCxdSRkYGzZgxg3bs2EHh4eFERFSzZk2KjY2lpUuXUmRkJC1dupTq1q1Lc+fOJSKiunXr0tGjR2n69OkF6jt79iy9+eabVK9ePSIiqlOnjnqsfPnypCiKzW3e27RpQ2PGjNF8N3HiRPX/wMBAeuONN+iLL76gsWPHkouLC7m5uZG9vX2h28avXbuW7t27R6tXr6Zy5coREdHChQupc+fONHv2bKpSpQoREVWoUIEWLlxIdnZ2VK9ePXrmmWdo586dNHjwYCIisre3p9WrV1NoaCjl5ubSnj17yNnZmWbOnEl9+/alUaNGqeX96KOPKDIykhYvXkzOzs704osvqvbUrFmTPvroI2ratCmlpqaSm5ubemzatGnUrl27AsvCMAzDMAzDMAzDMMWBF9sZhmEYhmEYhmEYRoDWrVvT4sWLKS0tjebNm0f29vbUo0cPOnbsGKWnp+sWdzMzMykkJISIiP78809q2rSp5nhYWFih+l5//XV6+eWX6bPPPqO2bdtSz549qVatWg+084knntB99+WXX9JHH31ESUlJlJqaStnZ2eTh4fHA38rLiRMnqFGjRupCOxFRixYtKDc3l/788091sf2xxx4jOzs79RxfX186cuSI5rfq169PPXr0oJs3b6r2xsfH0+HDh2nNmjXqeQAoNzeXUlJSKDg4mOLi4mjKlCkUHx9PN27coNzcXCK6f2NC/fr1C/UBwzAMwzAMwzAMw5QUvNjOMAzDMAzDMAzDMAKUK1eOateuTUREy5cvp0aNGtF///tfatCgARERff/991StWjWNjJOTk7S+KVOmUJ8+fej777+nLVu20DvvvENffPEFde/e/YF25uXXX3+lvn370tSpUykqKorKly9PX3zxBX3wwQfSthWGg4OD5rOiKOqieF7s7e3J3v7/pidSU1Np6NChNHLkSN25/v7+6jb2UVFRtGbNGvL29qazZ89SVFQUZWZmas7P7wOGYRiGYRiGYRiGKUl4sZ1hGIZhGIZhGIZhJLFYLDRhwgR6/fXX6eTJk+Tk5ERnz56lyMhIm+fXrVuXfvjhB813v//++wP1BAUFUVBQEI0ePZpeeOEFWrFiBXXv3p0cHR0pJyenSLbu3buXAgICNO+IP3PmjOacovxecHAwrVy5ktLS0tTF7D179pDFYqG6desWyZbCaNKkCR0/fly9oSE/R44coWvXrtGsWbOoevXqRER04MCBYutlGIZhGIZhGIZhGFEsD9sAhmEYhmEYhmEYhvkn07NnT7Kzs6OlS5fSG2+8QaNHj6ZVq1ZRUlISHTx4kBYsWECrVq0iIqKhQ4dSQkICjRs3jk6ePEn/+9//aOXKlUR0/8nv/Ny9e5dGjBhBu3fvpjNnztCePXvo999/p+DgYCK6/9711NRU2rlzJ129epXS09MLtLNOnTp09uxZ+uKLLygpKYk++ugj2rhxo+acwMBASklJoUOHDtHVq1cpIyND9zt9+/YlZ2dnGjBgAB09epRiYmLotddeo/79+6tbyBeHcePG0d69e2nEiBF06NAhSkxMpG+//ZZGjBhBRPefbnd0dKQFCxZQcnIyfffdd/Tuu+8WWy/DMAzDMAzDMAzDiMKL7QzDMAzDMAzDMAxTDOzt7WnEiBE0Z84cGj9+PE2aNIlmzpxJwcHB1KFDB/r++++pRo0aRERUo0YN+uqrr2jDhg3UsGFDWrx4sfqkua2t5u3s7OjatWsUHR1NQUFB1KtXL+rYsSNNnTqViIgiIiJo2LBh1Lt3b/L29qY5c+YUaGeXLl1o9OjRNGLECGrcuDHt3buXJk2apDmnR48e1KFDB2rdujV5e3vTunXrdL/j6upK27Zto+vXr1PTpk3pX//6Fz399NO0cOFCaR/mpWHDhvTTTz/RyZMn6amnnqKQkBCaPHkyVa1alYiIvL29aeXKlbR+/XqqX78+zZo1i95///0S0c0wDMMwDMMwDMMwIigA8LCNYBiGYRiGYRiGYZhHlenTp9OSJUvor7/+etimMAzDMAzDMAzDMAwjAL+znWEYhmEYhmEYhmEMZNGiRdS0aVPy8vKiPXv20Ny5c9Ut0hmGYRiGYRiGYRiG+efAi+0MwzAMwzAMwzAMYyCJiYn03nvv0fXr18nf35/GjBlD48ePf9hmMQzDMAzDMAzDMAwjCG8jzzAMwzAMwzAMwzAMwzAMwzAMwzAMwzCCWB62AQzDMAzDMAzDMAzDMAzDMAzDMAzDMAzzT4MX2xmGYRiGYRiGYRiGYRiGYRiGYRiGYRhGEF5sZxiGYRiGYRiGYRiGYRiGYRiGYRiGYRhBeLGdYRiGYRiGYRiGYRiGYRiGYRiGYRiGYQThxXaGYRiGYRiGYRiGYRiGYRiGYRiGYRiGEYQX2xmGYRiGYRiGYRiGYRiGYRiGYRiGYRhGEF5sZxiGYRiGYRiGYRiGYRiGYRiGYRiGYRhBeLGdYRiGYRiGYRiGYRiGYRiGYRiGYRiGYQT5fzhpEvylTJyEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2500x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(25, 9))\n",
    "df['RegistrationYear'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers\n",
    "df.loc[(df['RegistrationYear'] < 1910) | (df['RegistrationYear'] > 2019)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='RegistrationYear'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB9sAAAMWCAYAAABRJuY0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAACKNElEQVR4nOzdfZhVZb038N8MI4OgA6LCQCCgpojv+YKYqSUxKnXU1I5maYrx6AGvoxQK5xhqdRKxTFNMe0zJ6+jx5TymJYYRiqaiJIrvcqwDB31wUFOYxOT1fv7oYj9Ogs4Na5oF8/lc11w6s+79m999r7XX3pvvrL2rUkopAAAAAAAAAIAWq27rBgAAAAAAAABgUyNsBwAAAAAAAIBMwnYAAAAAAAAAyCRsBwAAAAAAAIBMwnYAAAAAAAAAyCRsBwAAAAAAAIBMwnYAAAAAAAAAyFTT1g20pTVr1sSiRYti6623jqqqqrZuBwAAAAAAAIA2lFKKP//5z9G7d++orv7oa9fbddi+aNGi6Nu3b1u3AQAAAAAAAECJvPrqq9GnT5+PHNOuw/att946Iv66UHV1dW3cDQAAAAAAAABtqampKfr27VvJkj9Kuw7b1751fF1dnbAdAAAAAAAAgIiIFn0M+Ue/yTwAAAAAAAAA8CHCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIlBW2X3rppXHAAQfE1ltvHT169Ihjjz025s2b12zM4YcfHlVVVc2+zjrrrGZjFi5cGMOHD4/OnTtHjx49YuzYsbFq1apmY2bOnBmf+tSnora2NnbeeeeYMmXKh/qZPHly9O/fPzp16hSDBw+O2bNn50wHAAAAAAAAADZIVtj+0EMPxahRo+Lxxx+P6dOnx8qVK2PYsGGxbNmyZuO+8Y1vxOuvv175mjRpUmXb6tWrY/jw4bFixYp47LHH4uc//3lMmTIlJkyYUBkzf/78GD58eHz2s5+NuXPnxrnnnhtnnnlm3H///ZUxt99+e4wZMyYuuuiieOqpp2LvvfeOhoaGeOONNzZ0LQAAAAAAAACgRapSSmlDb/zmm29Gjx494qGHHopDDz00Iv56Zfs+++wTV1555Tpv8+tf/zq+8IUvxKJFi6Jnz54REXHdddfFBRdcEG+++WZ07NgxLrjggpg6dWo8//zzlduddNJJsWTJkpg2bVpERAwePDgOOOCAuOaaayIiYs2aNdG3b98455xzYty4cS3qv6mpKbp27RpLly6Nurq6DV0GAAAAAAAAADYDORnyRn1m+9KlSyMionv37s1+fsstt8R2220Xe+yxR4wfPz7ee++9yrZZs2bFnnvuWQnaIyIaGhqiqakpXnjhhcqYoUOHNqvZ0NAQs2bNioiIFStWxJw5c5qNqa6ujqFDh1bGrMvy5cujqamp2RcAAAAAAAAA5KrZ0BuuWbMmzj333Pj0pz8de+yxR+XnX/nKV6Jfv37Ru3fvePbZZ+OCCy6IefPmxV133RUREY2Njc2C9oiofN/Y2PiRY5qamuIvf/lLvPPOO7F69ep1jnn55ZfX2/Oll14al1xyyYZOGQAAAAAAAAAiYiPC9lGjRsXzzz8fjzzySLOfjxw5svL/e+65Z/Tq1SuOOOKI+OMf/xg77bTThndagPHjx8eYMWMq3zc1NUXfvn3bsCMAAAAAAAAANkUbFLaPHj067r333nj44YejT58+Hzl28ODBERHxhz/8IXbaaaeor6+P2bNnNxuzePHiiIior6+v/Hftzz44pq6uLrbccsvo0KFDdOjQYZ1j1tZYl9ra2qitrW3ZJAEAAAAAAABgPbI+sz2lFKNHj45f/OIX8cADD8SAAQM+9jZz586NiIhevXpFRMSQIUPiueeeizfeeKMyZvr06VFXVxeDBg2qjJkxY0azOtOnT48hQ4ZERETHjh1jv/32azZmzZo1MWPGjMoYAAAAAAAAAGgtWVe2jxo1Km699da45557Yuutt658xnrXrl1jyy23jD/+8Y9x6623xtFHHx3bbrttPPvss3HeeefFoYceGnvttVdERAwbNiwGDRoUX/va12LSpEnR2NgYF154YYwaNapy1flZZ50V11xzTZx//vlxxhlnxAMPPBB33HFHTJ06tdLLmDFj4rTTTov9998/DjzwwLjyyitj2bJlcfrppxe1NgAAAAAAAACwTlUppdTiwVVV6/z5TTfdFF//+tfj1Vdfja9+9avx/PPPx7Jly6Jv375x3HHHxYUXXhh1dXWV8f/zP/8TZ599dsycOTO6dOkSp512WkycODFqav5/9j9z5sw477zz4sUXX4w+ffrEt7/97fj617/e7Pdec801cfnll0djY2Pss88+8eMf/7jytvUt0dTUFF27do2lS5c26w8AAAAAAACA9icnQ84K2zc3wnYAAAAAAAAA1srJkLM+sx0AAAAAAAAAELYDAAAAAAAAQDZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkqmnrBsqo/7ipHztmwcThf4dOAAAAAAAAACgjV7YDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQKaatm5gc9d/3NQWjVswcXgrdwIAAAAAAABAUVzZDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZatq6AfL0Hze1ReMWTBzeyp0AAAAAAAAAtF+ubAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMiUFbZfeumlccABB8TWW28dPXr0iGOPPTbmzZvXbMz7778fo0aNim233Ta22mqrOP7442Px4sXNxixcuDCGDx8enTt3jh49esTYsWNj1apVzcbMnDkzPvWpT0VtbW3svPPOMWXKlA/1M3ny5Ojfv3906tQpBg8eHLNnz86ZDgAAAAAAAABskJqcwQ899FCMGjUqDjjggFi1alX8y7/8SwwbNixefPHF6NKlS0REnHfeeTF16tS48847o2vXrjF69Oj40pe+FI8++mhERKxevTqGDx8e9fX18dhjj8Xrr78ep556amyxxRbx/e9/PyIi5s+fH8OHD4+zzjorbrnllpgxY0aceeaZ0atXr2hoaIiIiNtvvz3GjBkT1113XQwePDiuvPLKaGhoiHnz5kWPHj2KXKPNWv9xU1s0bsHE4a3cCQAAAAAAAMCmoyqllDb0xm+++Wb06NEjHnrooTj00ENj6dKlsf3228ett94aJ5xwQkREvPzyy7HbbrvFrFmz4qCDDopf//rX8YUvfCEWLVoUPXv2jIiI6667Li644IJ48803o2PHjnHBBRfE1KlT4/nnn6/8rpNOOimWLFkS06ZNi4iIwYMHxwEHHBDXXHNNRESsWbMm+vbtG+ecc06MGzeuRf03NTVF165dY+nSpVFXV1f5eUsC6JaGz0WH2WWvBwAAAAAAALCpWl+GvC4b9ZntS5cujYiI7t27R0TEnDlzYuXKlTF06NDKmIEDB8YOO+wQs2bNioiIWbNmxZ577lkJ2iMiGhoaoqmpKV544YXKmA/WWDtmbY0VK1bEnDlzmo2prq6OoUOHVsasy/Lly6OpqanZFwAAAAAAAADk2uCwfc2aNXHuuefGpz/96dhjjz0iIqKxsTE6duwY3bp1aza2Z8+e0djYWBnzwaB97fa12z5qTFNTU/zlL3+Jt956K1avXr3OMWtrrMull14aXbt2rXz17ds3f+IAAAAAAAAAtHsbHLaPGjUqnn/++bjtttuK7KdVjR8/PpYuXVr5evXVV9u6JQAAAAAAAAA2QTUbcqPRo0fHvffeGw8//HD06dOn8vP6+vpYsWJFLFmypNnV7YsXL476+vrKmNmzZzert3jx4sq2tf9d+7MPjqmrq4stt9wyOnToEB06dFjnmLU11qW2tjZqa2vzJwwAAAAAAAAAH5AVtqeU4pxzzolf/OIXMXPmzBgwYECz7fvtt19sscUWMWPGjDj++OMjImLevHmxcOHCGDJkSEREDBkyJP7t3/4t3njjjejRo0dEREyfPj3q6upi0KBBlTH33Xdfs9rTp0+v1OjYsWPst99+MWPGjDj22GMj4q9vaz9jxowYPXp05hJQpP7jprZo3IKJw1u5EwAAAAAAAIDWkxW2jxo1Km699da45557Yuutt658PnrXrl1jyy23jK5du8aIESNizJgx0b1796irq4tzzjknhgwZEgcddFBERAwbNiwGDRoUX/va12LSpEnR2NgYF154YYwaNapy1flZZ50V11xzTZx//vlxxhlnxAMPPBB33HFHTJ36/4PcMWPGxGmnnRb7779/HHjggXHllVfGsmXL4vTTTy9qbQAAAAAAAABgnbLC9p/85CcREXH44Yc3+/lNN90UX//61yMi4kc/+lFUV1fH8ccfH8uXL4+Ghoa49tprK2M7dOgQ9957b5x99tkxZMiQ6NKlS5x22mnxne98pzJmwIABMXXq1DjvvPPiqquuij59+sQNN9wQDQ0NlTH/+I//GG+++WZMmDAhGhsbY5999olp06ZFz549c9cAAAAAAAAAALJkv438x+nUqVNMnjw5Jk+evN4x/fr1+9DbxP+tww8/PJ5++umPHDN69GhvGw8AAAAAAADA311W2A5/Tz7/HQAAAAAAACir6rZuAAAAAAAAAAA2NcJ2AAAAAAAAAMgkbAcAAAAAAACATD6znXbDZ8ADAAAAAAAARXFlOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkqmnrBmBT1X/c1BaNWzBxeCt3AgAAAAAAAPy9ubIdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADIJ2wEAAAAAAAAgk7AdAAAAAAAAADLVtHUDwF/1Hze1ReMWTBzeyp0AAAAAAAAAH8eV7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJlq2roBoHj9x01t0bgFE4e3cicAAAAAAACweXJlOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkqmnrBoDy6z9uaovGLZg4vJU7AQAAAAAAgHJwZTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAEAmYTsAAAAAAAAAZBK2AwAAAAAAAECm7LD94Ycfji9+8YvRu3fvqKqqirvvvrvZ9q9//etRVVXV7OvII49sNubtt9+OU045Jerq6qJbt24xYsSIePfdd5uNefbZZ+Mzn/lMdOrUKfr27RuTJk36UC933nlnDBw4MDp16hR77rln3HfffbnTAQAAAAAAAIBs2WH7smXLYu+9947Jkyevd8yRRx4Zr7/+euXrP/7jP5ptP+WUU+KFF16I6dOnx7333hsPP/xwjBw5srK9qakphg0bFv369Ys5c+bE5ZdfHhdffHH89Kc/rYx57LHH4uSTT44RI0bE008/Hccee2wce+yx8fzzz+dOCQAAAAAAAACy1OTe4KijjoqjjjrqI8fU1tZGfX39Ore99NJLMW3atPj9738f+++/f0REXH311XH00UfHD37wg+jdu3fccsstsWLFirjxxhujY8eOsfvuu8fcuXPjiiuuqITyV111VRx55JExduzYiIj47ne/G9OnT49rrrkmrrvuutxpAQAAAAAAAECLtcpnts+cOTN69OgRu+66a5x99tnxpz/9qbJt1qxZ0a1bt0rQHhExdOjQqK6ujieeeKIy5tBDD42OHTtWxjQ0NMS8efPinXfeqYwZOnRos9/b0NAQs2bNWm9fy5cvj6ampmZfAAAAAAAAAJCr8LD9yCOPjJtvvjlmzJgRl112WTz00ENx1FFHxerVqyMiorGxMXr06NHsNjU1NdG9e/dobGysjOnZs2ezMWu//7gxa7evy6WXXhpdu3atfPXt23fjJgsAAAAAAABAu5T9NvIf56STTqr8/5577hl77bVX7LTTTjFz5sw44ogjiv51WcaPHx9jxoypfN/U1CRwBwAAAAAAACBbq7yN/AftuOOOsd1228Uf/vCHiIior6+PN954o9mYVatWxdtvv135nPf6+vpYvHhxszFrv/+4Mev7rPiIv36WfF1dXbMvAAAAAAAAAMjV6mH7a6+9Fn/605+iV69eERExZMiQWLJkScyZM6cy5oEHHog1a9bE4MGDK2MefvjhWLlyZWXM9OnTY9ddd41tttmmMmbGjBnNftf06dNjyJAhrT0lAAAAAAAAANq57LD93Xffjblz58bcuXMjImL+/Pkxd+7cWLhwYbz77rsxduzYePzxx2PBggUxY8aMOOaYY2LnnXeOhoaGiIjYbbfd4sgjj4xvfOMbMXv27Hj00Udj9OjRcdJJJ0Xv3r0jIuIrX/lKdOzYMUaMGBEvvPBC3H777XHVVVc1ewv4f/7nf45p06bFD3/4w3j55Zfj4osvjieffDJGjx5dwLIAAAAAAAAAwPplh+1PPvlk7LvvvrHvvvtGRMSYMWNi3333jQkTJkSHDh3i2WefjX/4h3+IXXbZJUaMGBH77bdf/O53v4va2tpKjVtuuSUGDhwYRxxxRBx99NFxyCGHxE9/+tPK9q5du8ZvfvObmD9/fuy3337xzW9+MyZMmBAjR46sjDn44IPj1ltvjZ/+9Kex9957x3/+53/G3XffHXvsscfGrAcAAAAAAAAAfKya3BscfvjhkVJa7/b777//Y2t07949br311o8cs9dee8Xvfve7jxxz4oknxoknnvixvw8AAAAAAAAAitTqn9kOAAAAAAAAAJsbYTsAAAAAAAAAZMp+G3mAjdV/3NQWjVswcXgrdwIAAAAAAAAbxpXtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJDJZ7YDmzyfAQ8AAAAAAMDfmyvbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMtW0dQMAZdN/3NSPHbNg4vC/QycAAAAAAACUlSvbAQAAAAAAACCTK9sBWlFLrpKPcKU8AAAAAADApsaV7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJmE7QAAAAAAAACQSdgOAAAAAAAAAJlq2roBAFqu/7ipLRq3YOLwVu4EAAAAAACgfXNlOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkyg7bH3744fjiF78YvXv3jqqqqrj77rubbU8pxYQJE6JXr16x5ZZbxtChQ+OVV15pNubtt9+OU045Jerq6qJbt24xYsSIePfdd5uNefbZZ+Mzn/lMdOrUKfr27RuTJk36UC933nlnDBw4MDp16hR77rln3HfffbnTAQAAAAAAAIBs2WH7smXLYu+9947Jkyevc/ukSZPixz/+cVx33XXxxBNPRJcuXaKhoSHef//9yphTTjklXnjhhZg+fXrce++98fDDD8fIkSMr25uammLYsGHRr1+/mDNnTlx++eVx8cUXx09/+tPKmMceeyxOPvnkGDFiRDz99NNx7LHHxrHHHhvPP/987pQAAAAAAAAAIEtN7g2OOuqoOOqoo9a5LaUUV155ZVx44YVxzDHHRETEzTffHD179oy77747TjrppHjppZdi2rRp8fvf/z7233//iIi4+uqr4+ijj44f/OAH0bt377jllltixYoVceONN0bHjh1j9913j7lz58YVV1xRCeWvuuqqOPLII2Ps2LEREfHd7343pk+fHtdcc01cd9116+xv+fLlsXz58sr3TU1NudMHAAAAAAAAgGI/s33+/PnR2NgYQ4cOrfysa9euMXjw4Jg1a1ZERMyaNSu6detWCdojIoYOHRrV1dXxxBNPVMYceuih0bFjx8qYhoaGmDdvXrzzzjuVMR/8PWvHrP0963LppZdG165dK199+/bd+EkDAAAAAAAA0O4UGrY3NjZGRETPnj2b/bxnz56VbY2NjdGjR49m22tqaqJ79+7Nxqyrxgd/x/rGrN2+LuPHj4+lS5dWvl599dXcKQIAAAAAAABA/tvIb8pqa2ujtra2rdsAAAAAAAAAYBNXaNheX18fERGLFy+OXr16VX6+ePHi2GeffSpj3njjjWa3W7VqVbz99tuV29fX18fixYubjVn7/ceNWbsdgI/Xf9zUFo1bMHF4K3cCAAAAAACwaSn0beQHDBgQ9fX1MWPGjMrPmpqa4oknnoghQ4ZERMSQIUNiyZIlMWfOnMqYBx54INasWRODBw+ujHn44Ydj5cqVlTHTp0+PXXfdNbbZZpvKmA/+nrVj1v4eAAAAAAAAAGgt2WH7u+++G3Pnzo25c+dGRMT8+fNj7ty5sXDhwqiqqopzzz03vve978Uvf/nLeO655+LUU0+N3r17x7HHHhsREbvttlsceeSR8Y1vfCNmz54djz76aIwePTpOOumk6N27d0REfOUrX4mOHTvGiBEj4oUXXojbb789rrrqqhgzZkylj3/+53+OadOmxQ9/+MN4+eWX4+KLL44nn3wyRo8evfGrAgAAAAAAAAAfIftt5J988sn47Gc/W/l+bQB+2mmnxZQpU+L888+PZcuWxciRI2PJkiVxyCGHxLRp06JTp06V29xyyy0xevToOOKII6K6ujqOP/74+PGPf1zZ3rVr1/jNb34To0aNiv322y+22267mDBhQowcObIy5uCDD45bb701LrzwwviXf/mX+OQnPxl333137LHHHhu0EAAAAAAAAADQUtlh++GHHx4ppfVur6qqiu985zvxne98Z71junfvHrfeeutH/p699torfve7333kmBNPPDFOPPHEj24YAAAAAAAAAApW6Ge2AwAAAAAAAEB7IGwHAAAAAAAAgEzCdgAAAAAAAADIlP2Z7QCwPv3HTW3RuAUTh/9dawEAAAAAABTNle0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkEnYDgAAAAAAAACZhO0AAAAAAAAAkKmmrRsAgL+H/uOmtmjcgonDW7kTAAAAAABgc+DKdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEyFh+0XX3xxVFVVNfsaOHBgZfv7778fo0aNim233Ta22mqrOP7442Px4sXNaixcuDCGDx8enTt3jh49esTYsWNj1apVzcbMnDkzPvWpT0VtbW3svPPOMWXKlKKnAgAAAAAAAADr1CpXtu++++7x+uuvV74eeeSRyrbzzjsvfvWrX8Wdd94ZDz30UCxatCi+9KUvVbavXr06hg8fHitWrIjHHnssfv7zn8eUKVNiwoQJlTHz58+P4cOHx2c/+9mYO3dunHvuuXHmmWfG/fff3xrTAQAAAAAAAIBmalqlaE1N1NfXf+jnS5cujZ/97Gdx6623xuc+97mIiLjppptit912i8cffzwOOuig+M1vfhMvvvhi/Pa3v42ePXvGPvvsE9/97nfjggsuiIsvvjg6duwY1113XQwYMCB++MMfRkTEbrvtFo888kj86Ec/ioaGhvX2tXz58li+fHnl+6ampoJnDgAAAAAAAEB70CpXtr/yyivRu3fv2HHHHeOUU06JhQsXRkTEnDlzYuXKlTF06NDK2IEDB8YOO+wQs2bNioiIWbNmxZ577hk9e/asjGloaIimpqZ44YUXKmM+WGPtmLU11ufSSy+Nrl27Vr769u1byHwBAAAAAAAAaF8KD9sHDx4cU6ZMiWnTpsVPfvKTmD9/fnzmM5+JP//5z9HY2BgdO3aMbt26NbtNz549o7GxMSIiGhsbmwXta7ev3fZRY5qamuIvf/nLensbP358LF26tPL16quvbux0AQAAAAAAAGiHCn8b+aOOOqry/3vttVcMHjw4+vXrF3fccUdsueWWRf+6LLW1tVFbW9umPQAAAAAAAACw6WuVt5H/oG7dusUuu+wSf/jDH6K+vj5WrFgRS5YsaTZm8eLFlc94r6+vj8WLF39o+9ptHzWmrq6uzQN9AAAAAAAAADZ/rR62v/vuu/HHP/4xevXqFfvtt19sscUWMWPGjMr2efPmxcKFC2PIkCERETFkyJB47rnn4o033qiMmT59etTV1cWgQYMqYz5YY+2YtTUAAAAAAAAAoDUV/jby3/rWt+KLX/xi9OvXLxYtWhQXXXRRdOjQIU4++eTo2rVrjBgxIsaMGRPdu3ePurq6OOecc2LIkCFx0EEHRUTEsGHDYtCgQfG1r30tJk2aFI2NjXHhhRfGqFGjKm8Bf9ZZZ8U111wT559/fpxxxhnxwAMPxB133BFTp04tejoAsE79x7XsMWfBxOGt3AkAAAAAANAWCg/bX3vttTj55JPjT3/6U2y//fZxyCGHxOOPPx7bb799RET86Ec/iurq6jj++ONj+fLl0dDQENdee23l9h06dIh77703zj777BgyZEh06dIlTjvttPjOd75TGTNgwICYOnVqnHfeeXHVVVdFnz594oYbboiGhoaipwMAAAAAAAAAH1J42H7bbbd95PZOnTrF5MmTY/Lkyesd069fv7jvvvs+ss7hhx8eTz/99Ab1CAAAAAAAAAAbo9U/sx0AAAAAAAAANjfCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIVNPWDQAAEf3HTW3RuAUTh7dyJwAAAAAAQEu4sh0AAAAAAAAAMgnbAQAAAAAAACCTsB0AAAAAAAAAMvnMdgDYzPj8dwAAAAAAaH2ubAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATMJ2AAAAAAAAAMgkbAcAAAAAAACATDVt3QAAUG79x01t0bgFE4e3cicAAAAAAFAermwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEw1bd0AANC+9B83tUXjFkwc3sqdAAAAAADAhnNlOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkErYDAAAAAAAAQCZhOwAAAAAAAABkqmnrBgAANkb/cVNbNG7BxOGt3AkAAAAAAO2JsB0A4ANaEt4L7gEAAAAA8DbyAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJCppq0bAADYXPUfN7VF4xZMHN7KnQAAAAAAUDRXtgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGQStgMAAAAAAABAJmE7AAAAAAAAAGSqaesGAABomf7jprZo3IKJw9ukHgAAAABAe+LKdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIVNPWDQAAsHnoP25qi8YtmDi8lTsBAAAAAGh9rmwHAAAAAAAAgEzCdgAAAAAAAADIJGwHAAAAAAAAgEzCdgAAAAAAAADIVNPWDQAAwN/qP25qi8YtmDi8lTsBAAAAAFg3V7YDAAAAAAAAQCZXtgMAsNlzpTwAAAAAUDRXtgMAAAAAAABAJle2AwBAJlfKAwAAAACubAcAAAAAAACATK5sBwCANuZKeQAAAADY9LiyHQAAAAAAAAAyCdsBAAAAAAAAIJO3kQcAgM1MS96WvqVvSe8t7gEAAABg3VzZDgAAAAAAAACZXNkOAAD83bhSHgAAAIDNhbAdAADYZBUd3nsLfgAAAABaStgOAACwCRDeAwAAAJSLsB0AAKAdEt4DAAAAbBxhOwAAAButyPC+LT4eoC3rAQAAAJumTT5snzx5clx++eXR2NgYe++9d1x99dVx4IEHtnVbAAAA0CJt8ccA/rAAAAAANt4mHbbffvvtMWbMmLjuuuti8ODBceWVV0ZDQ0PMmzcvevTo0dbtAQAAAB9Q9ncZKPMfKvjDBwAAgPLZpMP2K664Ir7xjW/E6aefHhER1113XUydOjVuvPHGGDduXBt3BwAAAFBO/hgAAABg422yYfuKFStizpw5MX78+MrPqqurY+jQoTFr1qx13mb58uWxfPnyyvdLly6NiIimpqZm49Ysf+9jf//f3mZ9WlJLvdav1d7qlbm3zaVemXvbXOqVubey1ytzb5tLvTL3trnUK3Nvm0u9MvdW9npl7m1zqVfm3jaXemXurez1ytzb5lJvj4vub1Gt5y9paNE49Vq3VlvWAwCA1rD2dUtK6WPHVqWWjCqhRYsWxSc+8Yl47LHHYsiQIZWfn3/++fHQQw/FE0888aHbXHzxxXHJJZf8PdsEAAAAAAAAYBPz6quvRp8+fT5yzCZ7ZfuGGD9+fIwZM6by/Zo1a+Ltt9+ObbfdNqqqqtZ5m6ampujbt2+8+uqrUVdXt9E9qFeOWu2tXpl7K3u9MvfW3uqVubey1ytzb+2tXpl7K3u9MvdW9npl7q291Stzb2WvV+beyl6vzL21t3pl7q3s9crcW9nrlbm39lavzL2VvV6Zeyt7vTL31t7qlbm3stcrc2/trV6Zeyt7vTL3VvZ6bdVbSin+/Oc/R+/evT+25iYbtm+33XbRoUOHWLx4cbOfL168OOrr69d5m9ra2qitrW32s27durXo99XV1RWyE9UrV632Vq/MvZW9Xpl7a2/1ytxb2euVubf2Vq/MvZW9Xpl7K3u9MvfW3uqVubey1ytzb2WvV+be2lu9MvdW9npl7q3s9crcW3urV+beyl6vzL2VvV6Ze2tv9crcW9nrlbm39lavzL2VvV6Zeyt7vbborWvXri2qVV1EQ22hY8eOsd9++8WMGTMqP1uzZk3MmDGj2dvKAwAAAAAAAEDRNtkr2yMixowZE6eddlrsv//+ceCBB8aVV14Zy5Yti9NPP72tWwMAAAAAAABgM7ZJh+3/+I//GG+++WZMmDAhGhsbY5999olp06ZFz549C/sdtbW1cdFFF33o7efV+/vXK3NvZa9X5t7KXq/MvbW3emXurez1ytxbe6tX5t7KXq/MvZW9Xpl7a2/1ytxb2euVubey1ytzb+2tXpl7K3u9MvdW9npl7q291Stzb2WvV+beyl6vzL21t3pl7q3s9crcW3urV+beyl6vzL2VvV6Ze1urKqWUCqsGAAAAAAAAAO3AJvuZ7QAAAAAAAADQVoTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJBJ2A4AAAAAAAAAmYTtAAAAAAAAAJCppq0bKKulS5dGY2NjRETU19dH165d27ij5orsb9WqVfHCCy80qzdo0KDYYostSlGv6H1R9npFKvNxUrQy91f2+1hjY2M88cQTzeoNHjw46uvrN6hekYrurczHSUSx/ZV5v7Y37em4AwBg8+O1BQAArJ+w/W/ccMMNccUVV8S8efOa/XzXXXeNb37zmzFixIjsmitWrIi77747Zs2a1eyFycEHHxzHHHNMdOzYsU36W7NmTUyYMCEmT54cS5cubbata9euMXr06Ljkkkuiurplb4BQdL2i90XZ67WX4+SDZs+e/aH5DhkyJA488MA27a+ofVH2+9iyZcvif/2v/xW33XZbVFVVRffu3SMi4u23346UUpx88slx/fXXR+fOnVtUb60i9mvRvbXWcVzEXIvur7X2a0Rx822NekXWKus5YFPor8jHsrUcd5vn2kUU+w/3ZZ9rmY+VogOUMq9dRLHzLXrtyr4viuyvyOO46PtEezsfl7m3MtdrL8fJpvLaouzn4zLXK/NjxaZQr8zn47KvXXt6Tlbm467MvbVGf2Web9nn2p7Wrsyv34vuz3124+bbTKJi0qRJqXPnzmncuHHpwQcfTC+++GJ68cUX04MPPpjGjx+funTpki6//PKsmq+88kracccdU6dOndJhhx2WvvzlL6cvf/nL6bDDDkudOnVKO++8c3rllVfapL+xY8em7bffPl133XVp/vz56b333kvvvfdemj9/frr++utTjx490vnnn98m9Yqea9nrtafjJKWUFi9enA455JBUVVWV+vXrlw488MB04IEHpn79+qWqqqp0yCGHpMWLF7dJf0XuizLfx1JKacSIEemTn/xkmjZtWlq1alXl56tWrUr3339/2mWXXdKZZ57Z4npF7teieyt67Yqca9H9Fb12rTHfIusV3VuZzwFl76/I3lJy3G2ua5dSSu+++2465ZRTUocOHVJNTU3q0aNH6tGjR6qpqUkdOnRIX/3qV9OyZcs2i7mW+Vgpcj+0xlzLfNwVvXZl3xdF91fkcVz0+bM9nY/L3FvZ67Wn46Tsry3Kfj4uc72yP1aUvV6Zz8dlX7v29JyszMddmXtrjf7KPN+yz7U9rV2ZX78X3Z/77MbNd12E7R+www47pNtvv32922+77bbUt2/frJpDhw5NxxxzTFq6dOmHti1dujQdc8wxadiwYW3SX8+ePdO0adPWu33atGmpR48ebVKv6LmWvV57Ok5SSun4449PQ4YMSS+//PKHtr388svp4IMPTieccEKb9FfkvijzfSyllLp165YeffTR9W5/5JFHUrdu3Vpcr8j9WnRvRa9dkXMtur+i1y6l4udbZL2ieyvzOaDs/RXZW0qOu7U2t7VLqdh/uC/7XMt8rBQdoJR97Yqcb9FrV/Z9UXR/RR7HRZ8/29P5uMy9lb1eezpOyv7aouzn4zLXK/tjRdnrlfl8XPa1a0/Pycp83JW5t9bor8zzLftc29Palfn1e9H9uc/+fxsy33URtn9Ap06d0osvvrje7S+88ELacssts2puueWW6bnnnlvv9meffbbFNYvur3PnzunZZ59d7/ZnnnkmdenSpU3qFT3XstdrT8dJSilttdVW6amnnlrv9ieffDJttdVWbdJfkfuizPexlFKqq6tLv//979e7ffbs2amurq7F9Yrcr0X3VvTaFTnXovsreu1SKn6+RdYrurcynwPK3l+RvaXkuPugzWntUir2H+7LPtcyHytFByhlX7si51v02pV9XxTdX5HHcdHnz/Z0Pi5zb2Wv156Ok7K/tij7+bjM9cr+WFH2emU+H5d97drTc7IyH3dl7q01+ivzfMs+1/a0dmV+/V50f+6zzeXOd13yP2R5M3bAAQfExIkTY9WqVR/atnr16rjsssvigAMOyKrZrVu3WLBgwXq3L1iwILp169Ym/R1++OHxrW99K956660PbXvrrbfiggsuiMMPP7xN6hU917LXa0/HSUREbW1tNDU1rXf7n//856itrW2T/orcF2W+j0VEfOELX4iRI0fG008//aFtTz/9dJx99tnxxS9+scX1ityvRfdW9NoVOdei+yt67SKKn2+R9YrurczngLL3V2RvEY67D9qc1i4iYs2aNR/5uWQdO3aMNWvWtElvZa9X5LFS5H6IKP/aFTnfoteu7Pui6P6KPI6LPn+2p/NxmXsre732dJyU/bVF2c/HZa5X9seKstcr8/m47GvXnp6Tlfm4K3NvrdFfmedb9rm2p7Ur8+v3ovtzn20ud77rtFFR/WbmmWeeSfX19WnbbbdNxx13XDrrrLPSWWedlY477ri07bbbpl69en3kX6Ksy7e//e20zTbbpCuuuCI988wzqbGxMTU2NqZnnnkmXXHFFal79+7poosuapP+Fi5cmPbYY49UU1OT9t1333TkkUemI488Mu27776ppqYm7bXXXmnhwoVtUq/ouZa9Xns6TlJK6Z/+6Z9Sv3790l133dXsbVSWLl2a7rrrrtS/f/80evToNumvyH1R5vtYSim9/fbb6cgjj0xVVVWpe/fuaeDAgWngwIGpe/fuqbq6Oh111FHpnXfeaXG9Ivdr0b0VvXZFzrXo/opeu9aYb5H1iu6tzOeAsvdXZG8pOe4217VLKaWvfOUrad99913nXxU/9dRTab/99kunnHJKm/RW9npFHitF7oeUyr92Rc636LUr+74our8ij+Oiz5/t6Xxc5t7KXq89HSdlf21R9vNxmeuV/bGi7PXKfD4u+9q1p+dkZT7uytxba/RX5vmWfa7tae3K/Pq96P7cZ/9qQ+e7LsL2v9HU1JSuvfbadOqpp6Zhw4alYcOGpVNPPTX95Cc/WednK7TExIkTU69evVJVVVWqrq5O1dXVqaqqKvXq1Stddtllbdrf6tWr03333ZcmTJiQRo4cmUaOHJkmTJiQfv3rX6fVq1e3ab2i51r2eu3pOHn//ffTWWedlTp27Jiqq6tTp06dUqdOnVJ1dXXq2LFjOvvss9P777/fZv0VuS/KfB9b66WXXko33nhj+v73v5++//3vpxtvvDG99NJL2XWK3q9F9pZSsWvXGnMtet8WuXZFz7fIeq2xL8p8Dih7f0X25rjbPNcupWL/4b7scy3zsVJ0gFL2tStyvkWvXdn3RWuEbUWe84qsVXS9Mt8vytzbplCvvRwna5X1tUXZz8dlrlf2x4qy1yvz+bjsa9eenpOV+bgrc2+t0V+Z51v2ubantSvz6/ei+3Of3fh9+7eqUkpp466Np6Xmz58fjY2NERFRX18fAwYMaOOOKKP2dJw0NTXFnDlzms13v/32i7q6ujbu7K/a074oUtn3a5Ha01wjip9vkfVaY1+U/RxQ5v6K7M1xt3muXUTEyy+/HLNmzWpWb8iQITFw4MA2763s9SKKO1aK3A8R5V+7Iudb9NqVfV8U3V9Esee8oh8X28v5uMy9bQr12stx0hqK7K/s5+My1yv7Y0XZ65X5fFz2tWtPz8nKfNyVubfW6K/M8y37XNvT2pX59XvR/bnPFvf8WNgOwDq9/vrrsXLlythhhx3aupUPKXNvZWftAACAInhtAQAAEdVt3cCmZOjQobHjjjsWWvOee+6Jm2++uZBaRfe32267RYcOHUpZr+i5lr1eezpOIiKefPLJePjhhwupVXR/Re6LMt/HIiI+97nPFXqlbJH7tejeil67IucaUWx/Ra9dRPHzLbJe0b2V+RwQUe7+iuwtwnG3Mcq8dhF//Yf7hQsXFlKr7HMt87FS5H6IKP/aFTnfoteu7Pui6P6KPI6LPn+2p/NxmXsre732dJyU/bVF2c/HZa5X9seKstcr8/m47GvXnp6Tlfm4K3NvEfbF5lyvzGtX5tfvER57NkYh892oN6FvZ6655pp08cUXF1pz1113TdXV1YXUKrq/X/ziF2nKlCmlrFf0XMterz0dJymlNHDgwMLmW3R/Re6LMt/HUkpp9uzZaebMmYXVK3K/Ft1b0WtX5FxTKra/otcupeLnW2S9onsr8zkgpXL3V2RvKTnuNkaZ167oemXurTXqFXmslH2uZa5X5t42hXpFHsdFnz/b0/m4zL2VvV57Ok7K/tqizGtX9npl7q291Stzb2WvV+beyl6vzL21t3pl7q3s9crcW0rlfs5Y9rUrYz1vIw+UzqJFi2LlypXRr1+/tm6FArWn/dqe5hpR/HyLrNfe9kV74rjbcGVeu4iI3//+9/Hee+/FYYcdttG1yj7XMh8rRe6HiPKvXZHzLXrtyr4viu6vPSnz/aLMvW0K9YrUnuYaUWx/ZT8fl7le2R8ryl6vzOfjsq9de3pOVubjrsy9RdgXm3O9Mq9de3oO5T6bT9j+EZYvXx4REbW1tW3cyaZr+fLl8dprr0WfPn1KuY4zZ86MwYMHx5ZbbtnWrWxyFi9eHCmlqK+vb+tWKlavXh1vvfVWVFdXx/bbb9/W7TSzdOnSaGxsjIiI+vr66Nq1axt3tOl55ZVXYuHChdGvX7/Yeeed27qdv4uUUqxZs2aD3sp79erVzW43e/bsWLNmTey7776lPB+3J1OmTInjjjvOeQAAgE3GwoUL4/XXX4/q6urYcccdY9ttt23rlgAAoBR8ZvvfmD59ehx99NGxzTbbROfOnaNz586xzTbbxNFHHx2//e1vN7juf//3f8fNN98cl112WVx++eXxf/7P/4mmpqasGm+88Uaz7+fOnRunnXZafPrTn44TTjghZs6cucH9rctLL72U9dneU6ZMiVmzZkVExPvvvx8jRoyILl26xC677BJbbbVVnHXWWZU/YGiJG264IU477bS46aabIiLi9ttvj9122y123HHHuOiii/Imsx7Dhg2LBQsWbNBtn3nmmfje974X1157bbz11lvNtjU1NcUZZ5yRVe+NN96IBx54IJYuXRoRfw2zJ02aFBMnToznnnuuxXX23HPP+O53vxuvvvpq1u9fn7fffjtOOOGE2GGHHeLss8+O1atXx5lnnhm9evWKT3ziE3HwwQfH66+/nlVzzpw5hfS21tSpU+PQQw+NLl26RO/evaO+vj66desWX/va1zb6szuWLFkS//t//+/49re/HTfccENl/7TUDTfcEIMGDYru3bvHoEGDmv3/z372s6xaW2+9dYwYMSIee+yxrNttqGeeeWajP6955cqV8corr2Sv26WXXhozZsyIiIh33nknhg4dGrvuumt8/vOfj1133TWOOuqoWLJkSVbN++67L84888w4//zz4+WXX2627Z133onPfe5zWfXW5fTTT49FixZl327VqlVx4YUXxmGHHVY5v11++eWx1VZbRefOneO0006LFStWtKjW//zP/8T+++8ftbW1cdRRR0VTU1N8/vOfj4MOOigOPvjgGDRoUPzXf/1Xdo8Rfw3wP+iJJ56Ihx9+OFauXLlB9YrU2NgY99xzT1x//fVx/fXXxz333FP5A5ciLVu2bKM/w2fkyJEbdJx8UBHPK2bPnt1sn957771x2GGHxSc+8YnYf//9N+hzo4p6LFufDT2nRBT/2BPResfdxj72rNXa99kNPeettWrVqnjmmWfi/vvvj/vvvz+eeeaZUpxPVq9eHf/93/8da9asiYi//vHoHXfcEbfddlssXrx4g2quPXfcfvvtceedd8acOXOiNf7muYhz1MZqrfvFqlWrYvr06fGzn/0sfvvb337o+P44f/t8vTUsXry40M+Nu+SSS1ql71WrVhXS58beX4t4LPtbRZw/W+OxrKj7RWs8ln3QxjzOrs/GPlaU+fnn+mzoubi1His29vx57bXXRr9+/WLAgAFx8MEHx0EHHRQ9evSIQw45ZIOPyaIfa1v7OUXRx9uGPl78PR7L1irqsWJDH8v+nnPdUEUfx639fLHo5ylF3y/KfF7f2OdkrXWO2tjze9Fa43XU3yrT+XNTeywr+rVF0eeUtYp4/NnYdSv6fNwar3vWZWPXrojXAkUdx639PKBVzp8b9Sb0m5kpU6akmpqadNJJJ6Wbbrop3Xfffem+++5LN910Uzr55JPTFltskW6++easmu+++2464YQTUlVVVaqqqkrV1dWpvr4+dejQIW211VbpmmuuaXGt6urqtHjx4pRSSo8++mjaYost0mGHHZbGjh2bPv/5z6eampr00EMPZfX3UebOnZv1OQUDBgxIjz/+eEoppW9961upf//+6a677kovvfRSuvvuu9Muu+ySxo4d26JaP/rRj1KXLl3Sl770pdSrV6/0ve99L2277bbpe9/7XrrkkktSXV1duv7661vc27777rvOr6qqqrTbbrtVvm+p+++/P3Xs2DHtvvvuaYcddkjbbrtteuCBByrbGxsbs9buwQcfTF26dElVVVWpvr4+zZ07N/Xp0yd98pOfTLvuumuqra1N999/f4tqVVVVpW233TZ16NAhNTQ0pP/8z/9MK1eubHEvf+uMM85Ie+yxR7r66qvTYYcdlo455pi01157pUceeSQ99thj6YADDkinnnpqVs2qqqq00047pX/7t39L//f//t8N7i2llG6++ea09dZbp29+85vpX//1X1N9fX0aN25c+slPfpIOO+ywtN1226X/+q//anG94447Lt15550ppZSef/75tN1226Xtt98+DR48OPXs2TPV19enF198sUW1Jk2alDp37pzGjRuXHnzwwfTiiy+mF198MT344INp/PjxqUuXLunyyy9vcW9VVVVp9913T1VVVWngwIHpBz/4QXrjjTdafPtcc+fOTVVVVS0ef9lll6X33nsvpZTSqlWr0je/+c3UsWPHVF1dnWpqatLpp5+eVqxY0aJaffr0SU899VRKKaUzzzwz7bvvvumpp55Kf/nLX9LcuXPTQQcdlEaMGNHi3m655ZbUoUOHNHz48HTIIYekTp06pX//93+vbM+9zz7zzDPr/Npiiy3SL37xi8r3LXXhhRemnj17pjFjxqRBgwals846K/Xt2zf9+7//e/r5z3+ePvGJT6TLLrusRbWOP/74dNhhh6Vf/epX6ctf/nL69Kc/nQ4//PD02muvpUWLFqWGhoZ07LHHtri3lFJatGhR+vSnP506dOiQDj300PT222+n4cOHVx7bdtlll7Ro0aIW11uxYkUaO3Zs2mmnndIBBxyQfvaznzXbnrM/3n333XTKKaekDh06pJqamtSjR4/Uo0ePVFNTkzp06JC++tWvpmXLlmXN96PkPDZus8026/yqqqpKXbt2rXyfo7WeV/zyl79M1dXV6dRTT02TJ09OZ555ZqqpqUl33XVXi+sV+ViWUrHnlJSKfewp+rgr8rEnpeLvs0Wf81avXp3+9V//NXXr1q3S09qvbt26pQsvvDCtXr26RbWKPJ+snWuvXr1SdXV12mOPPdLChQvTHnvskbp06ZK22mqrtM0226TZs2dnzXXs2LGpc+fOqbq6OlVXV1fm2q9fv/TLX/6yxbVaIvf5++TJk9MRRxyRTjzxxPTb3/622bY333wzDRgwoMW1ir5fjB49Ov3qV79KKaX06quvpoEDB6YOHTqknj17pg4dOqQ999wzvfbaay2uV11dnT73uc+lW265Jb3//vstvt26NDU1pVNOOSXtsMMO6dRTT03Lly9P//RP/1Q5Lx966KFp6dKlLa63dOnSD30tWbIkbbHFFumJJ56o/KwoucfJ7bffnpYvX175/uqrr0477LBDqq6u/n/tnXl4jdfaxp+9szPtRIRIJEQSUyShiNSQOAQtoo7pC9WihjpF21Q5WrQ1t5TSr4oaqsfUmkqrg6I1JE5De0oQpImEhBwtraHGkCD394cr75ctg/3G2vLEfn7Xta/L3ivufa/pWevda73rhZeXF6ZOnarr+1WOZarjp+qxTHW/UDmWqR5nVY8VnOef90NvH1M9VqiMn7Nnz0aNGjUwf/58LF26FKGhoZg2bRq2bt2K5557DmazGfv27dPlT+VYq3JOAaiPd6rHC5Vj2f3Q245Vj2W2yKvKeY/qdqwyBqhud6r7hWo9lfWquh2rjlGq58dc+wTAP35yHstUt2PVdXE/9Iw/quOJ6nis8rrHGvSUneprAdXtWHWfVR0/i0MW2wtRv379Uhv4Rx99hHr16unSHDZsGFq3bo0jR44gIyMDvXv3xtixY3H9+nX861//gtlsxurVq63SMhgM2o/iHTt2xPPPP2+R/uqrr6JDhw5Wexs9enSprwEDBuiaWDs7O+PUqVMAgODgYGzdutUifffu3QgICLBKKyQkRCuXAwcOwGQy4ZNPPtHSP/nkE0RERFjtzWQyISYmBlOmTNFekydPhtFoxEsvvaR9Zi2RkZF48803AQD5+fmYNWsW3N3dtTzrvVD/29/+hpdffhlXr17F7NmzUbNmTbz88sta+muvvYaoqCirtAwGA3777Tds2rQJ3bp1g8lkgre3N8aMGaPrh6YC/Pz8sGfPHgB382UwGPDDDz9o6YmJiahZs6YuTYPBgBdeeEH7oalr167YtGkTbt++rdtfSEgI1q1bp73ft28f/P39kZ+fDwDo27cvevXqZbVelSpVkJqaCgDo0qUL+vXrpw3aeXl5GDp0KDp16mSVVkBAANavX19i+rp161CrVi2rvRXEgEOHDiEuLg5Vq1aFk5MT/ud//gdbtmzR8mwtvXr1KvXVoUMHXe248MLd7NmzUaVKFSxbtgwpKSn47LPP4OPjY/WCsbOzM06ePAkACAoKKrKRaP/+/fDz87PaW9OmTfHhhx9q79evXw83NzctrujtswUTpHsnD4U/16NXp04dbcDPyMiA0Wi0aNfr169Ho0aNrNLy9vbGwYMHAQCXLl2CwWDAjz/+qKUnJSWhevXqVnsDgOeeew5RUVH45ptv0LdvX0RFRaFNmzY4ffo0Tp06hdatW1vErPsxefJkVK9eHbNnz8Zbb72FypUrY9iwYVp6QayxhqFDh6J+/frYtm2bRQy5ffs2vv/+ewQHB+Mf//iH9Zm9D3omru7u7ujatStWrFihvZYvXw4HBwdMnz5d+0wPtppX/O1vf8P48eMt0qdPn45WrVpZ7U3lWAaojSmA2rFHdbtTOfYA6vus6pj3+uuvw9vbG4sXL0ZWVhZycnKQk5ODrKwsLFmyBD4+Phg7dqxVWirjCQB07twZvXv3xpEjR/Dqq68iNDQUffr0QV5eHm7duoUBAwbgySeftFpv3LhxCA0Nxbfffovt27ejbdu2mDVrFlJTUzFx4kTdC3f3Q0+M+vDDD2E2m/Hyyy9jwIABcHJywowZM7R0vWOj6n5RvXp1HDlyBADw9NNP48knn8S5c+cAABcuXMDf//539O7d22o9g8GAmJgYODk5oUqVKoiLi9PGS73ExcUhJCQE8+bNQ7t27dCjRw80atQIiYmJ2L17N8LCwrRrBWso+CHn3ldZ+9j90LuAUjgeL1u2DC4uLpg0aRK+++47vPPOO3Bzc8PSpUut1lM5lqmOn6rHMtX9QuVYZotxVuVYwXn+eT/09jHVY4XK+BkUFIQtW7Zo748dOwYvLy9tQ//IkSPRsWNHq70BasdalXMKQH28Uz1eqBzL7kdZxgqVY5nqvKqe96hsx6pjgC3mKSr7hUo91fWquh2rjlEq4zvnPgHwj5/cxzKV7Vh1XdwPPeOP6vikOh6rvO6xhrKWnYprAdXtWHWfVf37QnHIYnshnJ2dkZaWVmJ6WloaXFxcdGlWq1YN+/fv195fvHgRLi4u2u71BQsWoGnTplZpFf5R3M/PDz/99JNFesFOfmsxGo1o1qwZ2rVrV+zr8ccf1xV4AwMDtbu7a9asWWR386+//go3NzertFxdXbWFe+Bu3Rw9elR7n5GRAU9PT6u9JSYmom7dupg0aZLFDhqTyYSUlBSrdQrw8PDA8ePHLT5bvXo13Nzc8O233+qekBTWu3XrFkwmk0XwSE9PR+XKla3SKtxOgLt3BMyYMQP169eH0WhEZGRkkV38pWE2m7VFTwBwdHTUAhMAZGZmWl2v93q8desWNm7ciKeeekrbSTR27FgcO3bMai1XV1dkZWVZfGYymbQ7Pf7zn//oaiuurq5aXfj5+Wl3Vxdw7Ngxq+vCxcWl1A0OKSkpcHV1tdrbvXV78+ZNrFmzBk888QSMRiP8/f0xceJEq/VMJhO6dOmCwYMHF/vq3r277ovhAn/h4eFFTp/47LPP0LBhQ6u0goODsXnzZgB3T80o2PBRwMGDB+Hh4WG1Nzc3N2RmZlp8tmvXLri7u2PRokW6+2yTJk3QtWtXpKam4uTJkzh58iSysrJgMpmwfft27TNrcXFxQXZ2tsX7gh+Ogbv9rFKlSlZpVapUScvrnTt3YDKZcOjQIS09IyPDaq0CCo85Fy5cgMFgsNj1vHPnTtSpU8dqvXr16mmbCwo81atXD4MHD0Z+fr6u+vD09CzSPgqTmJioKwaUdDd6wcvDw8NqbxkZGdrpH1evXtU+L+vYA9huXuHj42OhC9yd9+gpO5Vj2b3+HjSmFNZTMfaobncqx54CDZV9VnXMq169OrZt21Zi+rZt2+Dj42OVlsp4AtyNAQVjd05ODhwcHPCf//xHSz969Ci8vLys1vPz88O///1v7f3p06fh7u6u7cyeNm0aIiMjdflTFaPCwsIsLuz37NkDb29vbS6ht+xU9wsXFxdtPPP397eoBwA4cuSIrmufghhw7tw5zJkzB2FhYdr10MKFC3XdjVGrVi3tuue3336DwWCwaIebN29GgwYNrNarWbMmunbtil27diEhIQEJCQmIj4+Hg4MDli9frn1mLSWd6lXwCgkJKfMcr0WLFnjvvfcs0hcuXKjrlDCVY5nq+Kl6LFPdL1SOZarHWdVjBef5p8pYDKgfK1TGT7PZbHG9nZ+fD5PJpJ0qcOjQIbi7u1vtDVA71qqcUwDq453q8ULlWKZ6rFA9lqnMK6B+3qOyHauOAbZqd4CafqFST3W9qm7HqmOUyvjOuU8AvOMnwHssU92OVdeFyvFHdXxSHY9VXvcAtis7FdcCtpqTqeqzqn9fKA5ZbC9Es2bNSj3mfOzYsWjWrJkuTU9PT4sjrPPy8mAymbSjn9PT061ewDcYDDh+/DguX76M2rVrF/kh4fjx4zCbzVZ7Cw4Oxqefflpi+sGDB3UNqm+++SYiIyPx119/Yfz48ejWrZu2uHD9+nU8/fTTVt9V4OXlZbFI6e/vb3FhnpGRofuC7tKlS3jmmWfQsmVL7QeUsi54eHt7F1mUAIC1a9fCbDZj0aJFusquWrVq2maC69evw2g0WmymSE5OtrqzF96VdC/x8fEYMGCArsXxJk2aaCc+bNmyBZUqVcL777+vpS9atMjqO24LuHfRGLg7eE2bNg116tSB0WhEmzZtrNIKDQ3Vjo4E7t616+TkpN3dkZGRoSu/LVu2xMcffwzg7kCzadMmi/QffvgBvr6+Vmm1adMGAwcOLPYY/9u3b2PgwIFo27at1d5Kq9usrCxMmDBB153yjz32mMWJEfeiNwYYDAYttnl5eVlsygDuLhhbG6Nmz56N0NBQZGRk4P3330dkZKTWbzMzM9GuXTtdu82K26AEAAkJCXB3d8dbb72lK6+5ubl49dVXERYWZhGLyxpTqlevjsOHD2vvo6KiLI6uSU1NtXpzQatWrTBhwgQAd3d1Vq9e3eKO5WnTpuk6GQQouhnAzc0NGRkZ2vtTp07p2jhS3CaZ06dPIzg4GP3798dvv/1mdX14eHiUenTlL7/8omtjhtlsxpgxYyzuRi/8mjp1qq62cuvWLYwdOxZ169ZFYmIigAdbbFc9r4iPj0dycjICAwOLHHGWlpama6xVOZYV+FMVUwr0VI09qtudyrEHUN9nVcc8s9lsEfPuJTk5WdcGTVXxBLDsY3l5eXBwcEBSUpKWnpqaquvxD5UqVcKJEye09wWboM6cOQPg7sY7Pe1YZYwqruyOHDmijRtl2Tyqsl80btxYO+UlNDQU27dvt0jfu3cvqlatarVecTFg7969eP7551GpUiWYzWY899xzVmk5Oztb9DGz2WyxyHny5Eld9XrhwgX07NkT7du3txj/y9rHnJ2dMWjQIItTvQq/hg8fXuY5XrVq1Sw28QF3r0P1bORTOZapjp+qxzLV/ULlWKZ6nFU9VnCef6qeL6oeK1TGz6ZNm2p9DLi7ycFsNmunqqWlpeneyKtyrFU5pwDUxzvV44XqsUzlWKF6LFOZV0D9vEdlO1YdA2zR7lT2C5V6qutVdTtWHaNUxnfOfQLgHT8B3mOZLa4tVNaFyvFHdXxSHY9VXvcAtis7FdcCtpiTqeyzqn9fKA5ZbC9EwfPZHnvsMYwePRozZ87EzJkzMXr0aDRu3Bju7u66n4nesWNHi6PNZs+ebXH08YEDB6y+WC844qPguI/CFzwA8PXXX+s65r5fv34YNWpUiel6n9ecm5uL7t27o0qVKujYsSNcXFxgNptRv359uLm5ISAgwOqd9q1bt7Y4Qvlevv32W90LvAUsW7YMvr6+WLJkCRwdHcs0yHTs2LHEZ22vWbMGjo6OuiYkPXr0wN///nckJiZi2LBhePzxx9G1a1dcu3YN169fR+/evRETE2OVVnGB6F707Pz57LPP4ODggHr16sHZ2RkbNmxAjRo18PTTT+OZZ56Bk5OT7ueLlLZoDAA7duxAv379rNJasGABKleujLFjx2LSpEmoUaOGxbO8P/vsM1072DZv3oyqVati+fLlWL58OYKCgvDJJ59gz549WLZsGWrVqlXqppzCJCcnw9fXF15eXujVqxdGjBiBESNGoFevXvDy8oKfn1+Rgaw0rKlbPUfJDx48GC+99FKJ6b/++iuCgoJ0+Zs+fTo+/PBD+Pn5FYmXycnJuibWr7zyChwdHRESEgIXFxcYjUbt2TGPP/64NtGxhh49emDSpEnFphXE/rIcz7plyxb4+/tjxowZ2gSsLDGlffv2pR4n/vnnn1u9QL5t2za4uLjAyckJLi4u2L17N4KDg9GiRQu0atUKDg4OpT7eoDgCAgIsdvyNGzcOFy5c0N4fOnRI1w/PtWvXLvI8MODuDtng4GB07NjR6vro168fwsPDi2xAA+6OsREREejfv7/V3qKiojB37twS0/Uep1jAzp07ERAQgDfeeKPMYw9gm3lFwRGvH3zwgUX62rVrERYWZrU3lWNZgT+VMUXl2KO63akcewD1fbYAVTHvqaeeQqdOnbQjuwpz7tw5xMTEoGvXrlZpqYwnAPDEE09g6NChOH36NKZOnYp69ephyJAhWvpLL71k9UIWcDemvPPOO9r7tWvXWtzFeuTIEV3tWGWMqlWrlsWu/QJSUlJQvXp1DBw4UFfZqe4Xy5cvh7+/P+Lj47Fq1SqEhoZix44d+O2337Br1y489thjuo7fLi0GXLt2DZ988onVx4PXqFHD4se0Z5991kL76NGjuuq1gIULF6JGjRpYs2YNgLL/IBYREYGFCxeWmF6WDZWrVq3C119/DX9/f+zdu9ci/ejRo7oWjFWOZarjp+qxTHW/UDmWqR5nC1A1VnCef6qeL6oeK1TGz/Xr18PR0RFPP/00Bg4cCHd3d4uNvIsXL9Z1hxegdqxVOacA1Mc71eOFyrFM9VhRgKqxTGVeAfXzHpXtWHUMUN3uVPcLlXqq67UAVe1YdYxSGd859wmAd/wEeI9lBahqx6rrQuX4ozo+qY7HKq97APVlp/JaQHU7Vt1nVf++UByy2H4PWVlZGDt2LNq2bYvg4GAEBwejbdu2GDduXJHdXtaQlJSEqlWrwtfXFwEBAXBycsLatWu19AULFmDgwIFWaRUc8VHwunfheu7cuUWOyiiNM2fO6DrGzVq2bt2Kl156CTExMejUqRMGDRqEjz/+GNeuXbNaIzExsdRnMHz00UeYP39+mT2mp6ejefPmMBgMZRpkvvzyy1I3KqxevRrt2rXT5ad+/fowGAwIDQ3F6dOn0b17d5hMJu2Z64UHtdIYPHgwrly5YvV3W0NiYiLmzJmjHYGYkpKC5557DrGxsbqfNwxYt2ish4ULFyIqKgoRERF48803cePGDS0tPT3d4jhua9i4cSP8/f2LPHPQxcUFo0aN0vVMxCtXrmDhwoUYOHAgOnXqhE6dOmHgwIFYtGiR7uNOpkyZoh0zo4KbN28q1QsMDERQUJD2unfhbu7cubqe/wzcXfB/7733MGLECAwbNgyTJ0/GDz/8oPv59AkJCRbPnrqXXbt2YfDgwbo0Czh79iy6dOmCNm3alHnieuzYsSLH3Bdm9erVuhbIs7KysHHjRm3cOnv2LCZOnIgxY8ZoRz/poXv37qX+oLhgwQJ06NDBar2hQ4fi+eefLzbt9OnTqFevntWTw4sXLyImJgYGgwFVq1ZFSEgIQkJCULVqVRiNRnTp0gV//fWX1d6mT5+OKVOmlJienZ1d5rZy/vx59OrVC56enqU+tqY0VM4rCo5zLXidP3/eIn3lypVYuXKl1d5UjmWA+piicuxR3e4AtWOP6j5bGBUxLzs7G40aNYLJZEJ4eDhiYmIQExOD8PBwmEwmNG7c2GLnfGmojCfA3btMvby8YDQa4e3tjaNHj6Jly5bw9fVFjRo14OrqWuxiTUns2LEDzs7OaNGiBdq2bQuTyWTRlmfPnq2rLlTGqGeffbbE+ezRo0fh7e2tq+xs0S/ef/99mM1muLq6ahvuCl49e/a0eETH/VAZA2JiYrB48eIS05cvX67rwr8wKSkpaNKkCZ599tky97GRI0fi1VdfLTH9+PHjuq5V7n3+duEfoADgk08+0bW5VeVYBqiNn6rHMtX9QmU7tsXcvQAVYwXn+afq+aLqsQJQGz+3bNmCfv36ITY2tshNH+fPny8yh7wfKsdalXMKQH28Uz1eqIwBqseKwqgYy1T/bqR63qOyHauOAbZodyr7hUo91fVaGBXtWHWMAtTFd859AuAdPwHeY1lhVLRj1XWhcvxRHZ9Ux2PV1z0qy071tYAt5mQq+yygdn5cHAYAIMGmnDlzhjZv3ky5ubnUoUMHCgsLK29LAhHl5+fT1atXycPDgwwGQ3nbISKiCxcukJeXl/Z+586ddOPGDYqMjLT4vKKze/duat26NZlMpvK2UiJ37tyhpKQkysrKovz8fPLz86OIiAiqVKlSeVursPz888/k7OxM4eHh5W3FJsybN4/i4+Np/vz55O/vX952Hiq//PILmc1matSokVV/f+rUKUpLS6POnTsXm/7777/T9u3badCgQVZ7SE1NpZ9//pnOnj1LRES+vr4UGRlJISEhVmtUFLjPKx7WWKY3pthi7FHd7u7cuUMHDhygzMxMm449evtscTxozMvPz6fvv/++2PLr1KkTGY1Gq3RsEU+uX79OaWlp1KBBA3J3d6ebN2/S6tWr6caNG9SxY0dq0KCB1VpERMnJyfT5559Tbm4ude7cmTp27Kjr/9uKw4cPU1JSEg0ZMqTY9KNHj9IXX3xBkydP1qWblpZGP/30k7J+cenSJdq+fbtFv2jdujXVr19fl87KlSvpmWeeIWdn5zL5KMzFixfJaDSSp6dnselbt24lV1dXateuXZn08/LyaPz48RQfH09ffvkl1a5du+xmHwKbN28mR0fHEvthcagey1THT9Vjmap+8TCvo1TM3W05P+Yw/1SJLcYKVfHTFqgca1XNKaxBb7xTPV6oHMtszYOOZarzaot5j8p2rDIG2Hqeci9lmQeo0rPVfLYAFXMyW8QoFfGde5+oCPGzooxlD9qOH3ZMUUlZ4pPqORn33/BKoizXAirbsa3mPLacH8tiezHcvn2bUlJStAbh5+dHoaGh5OjoWM7O7nKvP19fXwoLCyuzP856nL3ZQk8l9pRXIv7+iuPWrVt05swZCggIUKJ3+/Zt+v3339nqqYR72alGpT/uebUnuNcFd3+CIAiCIAhC+SLzRUEQBEEQBEEg4ntbaTmQn59PkyZNoo8++oguX75skVa5cmWKi4ujqVOnlmkn0a5duygxMZHOnDlDRqOR6tSpQ927d9e1Y0K1P856nL3ZQq8Ae2gnheGYX5Xe7sevv/5KzZo1ozt37ijRS0lJYaFnT2V3b17r1q1L3bp1U363iMq6fRAt1XWrUo+zt5Ioa10kJydTUlIStW/fnmrXrk0pKSn00UcfUX5+PvXq1UvZHQVl8VfgrV27dlSnTp0H9sZZj7M37npffPEFdenShcxmsy4PFdGbaj0idflV7S03N5eMRqO20fHEiRO0bNkyys7OpsDAQBo6dKiuuyg41wVnbxVBT2VbUd3uVOvdj7/++ou+/fZbGjhw4EP3x73sOOs9jHaSlZVFx48fJz8/vwc6icZWegWouk5R6Y972XHW4+YNAJ08eZJq1apFJpOJ8vLyaNOmTZSbm0tPPfUUVatW7ZHyl5+fX+xvV/n5+XT69OkH3tTSoUMHWr58OQUGBj6Qjgo9W9ctJ2+c9WxdD0S86kK1PyLb9ltOfZao5LwCoP/+97/lGqM4eysNvdcCRA+nX5TVG5H6PlGhxsYHOoT+EeP111+Ht7c3Fi9ejKysLOTk5CAnJwdZWVlYsmQJfHx8MHbsWF2af/zxB1q0aAGj0QiTyQSj0YiIiAj4+vrCwcEBr7/+ern546zH2Zst9OypnXDPr0pv9+PQoUNlfnYURz17KruHmdey+FOtpTq/KvU4e7sfZamLL774Ag4ODvDy8oK7uzu2b98OT09PPPnkk+jcuTMcHBywevXqcvGn2htnPc7eKoKewWCAh4cHXnjhBfz8889W/7+K5s0Weirzq9pbdHQ0NmzYAABITEyEs7MzGjdujL59+yI8PBxmsxl79+61Wo9zXXD2VhH0VLYV1e1Otd790DvW2lPZcdZT7e3FF1/UnhGZk5OD2NhYGI1GGAwGGI1GtG/fXtczJIvTK3iWaVn0SqMs81mV+VWd14dRF1z0uLe7tLQ0BAYGwmg0ol69esjMzERERATc3NxgNptRrVo1pKenP5C/B8mvSn+XL19Gnz594OLiAh8fH0ycOBG3b9/W0s+ePaurn3399dfFvhwcHLBgwQLtfXnpqSw7zt6466WlpSEgIECZN+51odqfyn7Lvc9yjlGcvVmD3nmU6n6h0pvquqiIdSuL7YWoXr06tm3bVmL6tm3b4OPjo0uzb9++6NmzJy5fvoybN28iLi4OAwcOBADs3LkTXl5emDt3brn446zH2Zst9OypnQC886vSW3h4eKmvkJAQXYMCdz17KjuVeVXtT3VebZFflXqcvdmiLpo1a4Z33nkHALB27Vp4enpi2rRpWvqcOXPQtGnTcvGn0ht3Pc7eKoKewWDAtGnTEB4eDoPBgIYNG+KDDz7A+fPnrdaoCN5soacyv6q9eXh4aBf20dHRGD16tEX6hAkT0Lp1a6v1ONcFZ28VQU9lW1Hd7lTrXb58udTXjz/+qGustaey46yn2pvRaMQff/wBAHjjjTfg7++PXbt24fr160hMTETdunUxfvz4ctGzxXxWpT/OZcddj7M3AOjRowe6d++Ow4cPY9SoUQgNDUWPHj2Ql5eHmzdvolu3bhgwYMAj4W/kyJEIDg7Ghg0bsHTpUgQGBqJr167Izc0FcHdBwWAwWO2tYANBwWaH4l56+q1qPZVlx9kbdz3V3rjXhWp/Kvst9z7LOUZx9gaovxZQ2S9Ue1NdF9zrtjhksb0QZrMZhw8fLjE9OTkZbm5uujQ9PDxw9OhR7f21a9fg6OiIy5cvAwA+/fRTNGjQoFz8cdbj7M0WevbUTgDe+VXpzdnZGYMGDcKUKVOKfQ0fPlxXEOeuZ09lpzKvqv2pzqst8qtSj7M3W9SFm5sbsrKyAAD5+flwdHS0iIEnTpyAu7t7ufhT6Y27HmdvFUHPYDBoP3bu378fL774Ijw9PeHs7Iw+ffrghx9+eCS82UJPZX5t4S01NRXA3c2Qhw4dskg/fvz4I1MXnL1VBD2VbUV1u7NFOzYajSW+9P6oY09lx1nPlvGuUaNGWLNmjUX6119/jeDg4HLRs8V8VqU/zmXHXY+zNwDw9vbGwYMHAdy9JjMYDPjxxx+19D179iAgIOCR8BcQEID4+Hjt/blz59CiRQt06tQJN2/e1H33XkxMDLp27arltwCTyYSUlBSrdWylp7LsOHvjrqfaG/e6UO1PZb/l3mc5xyjO3gD11wIq+4Vqb6rrgnvdFocsthfiqaeeQqdOnXDu3LkiaefOndMqRA/e3t4WlZWTkwOj0YgLFy4AuPuDmLOzc7n446zH2Zst9OypnQC886vSW0REBBYuXFhi+sGDB3UNCtz17KnsVOZVtT/VeQXU51elHmdvtqgLX19f7N+/HwBw8eJFGAwGi8nnL7/8Al9f33Lxp9Ibdz3O3iqCXuEfOwu4ceMGVq1ahXbt2sFoNCIoKKjCe7OFnsr8qvbWoUMHvPfeewCAqKgorFy50iJ948aNZf5RXIU/lXqcvVUEPZVtRXW7U63n4eGBWbNmISEhodjX0qVLdY219lR2nPVsEe/+/PNPAEC1atUsNn4CwMmTJ+Hq6loueraYz6r0x7nsuOtx9gYArq6uOHXqlPbe3d0dx48f195nZ2frus7j7M/V1RWZmZkWn125cgWRkZHo0KEDMjMzdfez//3f/0WtWrXw7bffap89yIKCSj3VdcvZG2c91d4A3nVhC38q+y33Pss1RnH2Bqi/FlDZL2zhTWVdcK/b4pDF9kJkZ2ejUaNGMJlMCA8PR0xMDGJiYhAeHg6TyYTGjRsjOztbl2avXr0QGxuLa9euIS8vD6NGjUK9evW09J9//tnqH8RU++Osx9mbLfTsqZ1wz69KbyNHjsSrr75aYvrx48fRrl07q71x17OnslOZV9X+VOcVUJ9flXqcvdmiLgYMGICWLVvis88+Q7du3dC5c2e0atUKqampSEtLQ3R0NHr37l0u/lR6467H2VtF0Ct8jGdxZGRk4M0336zw3myhpzK/qr3t3bsXlStXxuTJkzF//nxUq1YNEyZMwOrVqzFp0iR4enpi1qxZVutxrgvO3iqCnsq2orrdqdZr165dqX9/6NAhXccf2lPZcdZT7c1gMGD48OEYPXo0fHx8ipwWkZSUhGrVqpWLni3msyr9cS477nqcvQFA3bp1Le7WW7hwIa5cuWKhp3dDJVd/DRo0wHfffVfk86tXryIyMhJNmjQp09G2Bw8eRFhYGIYNG4br168/8IKCKj3VdcvZG2c9W9QDwLcuVPuzRb/l2mc5xyjO3gD11wIq+4Vqb6rrgnvdFocstt/DnTt3sGXLFkyaNAnDhg3DsGHDMGnSJGzduhV37tzRrXfixAnUrVsXJpMJjo6O8PT0xPbt27X05cuX63omkGp/nPU4e1OtZ2/thHN+VXuzJ+yp7Owpr4D6/KrU4+zNFpw9exYdO3aEu7s7OnfujEuXLiEuLk473ql+/foWu1orsjfOepy9VQS94u6SLSucvdlCT2V+VXsD7i5AtWrVqsizz2rWrIm5c+fq0uJcF5y9VQQ9QG1bUamlWu/jjz/Ghx9+WGL62bNnMWXKlHLzx7nsuOup1IqOjka7du2019KlSy3S3377bURHR5ebnmpU+uNedpz1OHsDgOHDhxfRKMy7776Lp5566pHw98orr5S4WfLKlSto2bJlmZ8jm5OTg+HDh6N+/fpwcHB44AUFFXqq65azN856tqoHgGddqPZnq37Lsc9yjlGcvQHqrwVU9gvV3lTXBfe6LQ4DAJBgU3JycigxMZHy8vKoVatWVK1atfK2JDDE3toJ5/xy9sYdeyo7e8orkfr8qtTj7O1hkZmZSTk5ORQSEkImk6m87Vig2htnPc7eOOmdOnWKAgICyGAwPLAH7t4eRl6JypZfW3o7d+4cZWZmUn5+Pvn5+VFQUJBuDc51wdlbRdArjIq2YgstW+ipxp7KjrPew2gnmZmZ5OTkRP7+/iz1VKPSH/ey46zH2RsRUVZWFrm4uJCfn58SvfL099dff9Hvv/9ODRs2LDb96tWrdODAAYqOji6zn2+++Ybi4+PpjTfeIB8fnzLr2EqvMA9at5y9VSQ9FVrc6+JB/Nm633Lqs5xjFGdv5YHqmKIH1XVREetWFtuL4ZdffqGffvqJzp49S0REvr6+FBUVRc2bNy9nZ3dR7Y+zHmdvttBTiT3llYi3v+K8RUZGUosWLR5JPZXYU16J1Prjnld7gntdcPcnCIIgCIIglC8yXxQEQRAEQRCEkpHF9kL8+eefFBsbS3v27KGAgACqXr06ERH98ccflJ2dTa1bt6YvvvhC9y6HGzdu0Nq1aykxMZHOnDlDRqOR6tSpQz179qQnnnii3Pxx1uPszRZ6RPbTTrjnl7M37npE9lN2KvOq2h/3PmsLPa7euNcF937BXY+zN3vT4+yNux5nb9z1OHuzNz3O3rjrcfbGXY+zN5V63OezqrXsTY+zN3vT4+yNux5nb9z1OHuzNz3O3rjrcfbGXY+zt4qgVwQlh9E/IsTGxiIyMhJpaWlF0tLS0hAVFVXicwJKIiMjA4GBgfDx8UGtWrVgMBjQtWtXtGzZEg4ODujTpw9u3bpVLv4463H2Zgs9e2onAO/8cvbGXc+eyk5lXlX7495nVetx9sa9Lrj3C856nL3Zmx5nb9z1OHvjrsfZm73pcfbGXY+zN+56nL2p1uM+n+Vcdtz1OHuzNz3O3rjrcfbGXY+zN3vT4+yNux5nb9z1OHurCHrFIYvthXB3d8eBAwdKTN+/fz/c3d11aXbp0gXDhw9Hfn4+AGDmzJno0qULACA9PR1BQUGYPHlyufjjrMfZmy307KmdALzzy9kbdz17KjuVeVXtj3ufVa3H2Rv3uuDeLzjrcfZmb3qcvXHX4+yNux5nb/amx9kbdz3O3rjrcfamWo/7fJZz2XHX4+zN3vQ4e+Oux9kbdz3O3uxNj7M37nqcvXHX4+ytIugVhyy2F8LLywsJCQklpsfHx8PLy0uXptlsRnp6uvY+NzcXjo6OOH/+PADgq6++QlBQULn446zH2Zst9OypnQC888vZG3c9eyo7lXlV7Y97n1Wtx9kb97rg3i8463H2Zm96nL1x1+PsjbseZ2/2psfZG3c9zt6463H2plqP+3yWc9lx1+Pszd70OHvjrsfZG3c9zt7sTY+zN+56nL1x1+PsrSLoFYfxwQ+if3To27cvDRo0iDZt2kRXrlzRPr9y5Qpt2rSJhgwZQs8++6wuTU9PT7p69ar2Picnh27fvk1OTk5ERNS4cWM6c+ZMufjjrMfZmy307KmdEPHOL2dv3PXsqexU5lW1P+59VrUeZ2/c64J7v+Csx9mbvelx9sZdj7M37nqcvdmbHmdv3PU4e+Oux9mbaj3u81nOZcddj7M3e9Pj7I27Hmdv3PU4e7M3Pc7euOtx9sZdj7O3iqBXLA+0VP+IcfPmTYwYMQJOTk4wGo1wcXGBi4sLjEYjnJyc8OKLL+LmzZu6NAcNGoTo6GikpqYiMzMTffv2RXh4uJaekJCAWrVqlYs/znqcvdlCz57aCff8cvbGXc+eyk5lXlX7495nVetx9sa9Lrj3C856nL3Zmx5nb9z1OHvjrsfZm73pcfbGXY+zN+56nL2p1uM+n+Vcdtz1OHuzNz3O3rjrcfbGXY+zN3vT4+yNux5nb9z1OHurCHrFIYvtxXD58mXs2rULa9aswZo1a7Br1y5cvny5TFp//PEHWrVqBYPBAKPRiMDAQIvnXW3YsAHz5s0rN3/c9Th7U6lnb+2Ec345e+OuZ09lZ4u8qvSnWkt1flXqcfZWAOe6UOmPe13YU7uzJz3O3rjrcfbGXY+zN3vT4+yNux5nb9z1OHuzhR7Adz7Lvew463H2Zm96nL1x1+PsjbseZ2/2psfZG3c9zt6463H2VhH0isMAAA92b7xgDRkZGZSbm0shISFkMpnK247AFHtrJ5zzy9kbd+yp7Owpr0Tq86tSj7M3W8DZH/e6sKd2Z096nL1x1+PsjbseZ2/2psfZG3c9zt6463H2Zgs91Uhd8NDj7M3e9Dh7467H2Rt3Pc7e7E2Pszfuepy9cdfj7K0i6FnwQEv1jyA5OTn48ccfkZKSUiTtxo0bWLlypdLvy87OxpAhQ6z+e9X+OOtx9mYLvdJ41NrJ/Sjv/HL2xl2vNB61sisNvXkFeMfP+1GW/D4svfL2xr0uuPeLiqrH2Zu96XH2xl2Pszfuepy92ZseZ2/c9Th7467H2VtZ9LjPZx+Wlr3pcfZmb3qcvXHX4+yNux5nb/amx9kbdz3O3rjrcfbGVU8W2wtx7NgxBAYGakcJtG3bFr/99puWfvbsWRiNRqXfeejQIas1VfvjrMfZmy307sej1E6soTzzy9kbd7378SiV3f3Qk1fV/rj32YetV57euNcF935RkfU4e7M3Pc7euOtx9sZdj7M3e9Pj7I27Hmdv3PU4e9Orx30++zC17E2Pszd70+PsjbseZ2/c9Th7szc9zt6463H2xl2PszeuevzOkSpHxo0bR40aNaL9+/fTpUuXaNSoUfS3v/2NEhISKCAgoEya33zzTanpmZmZ5eaPsx5nb7bQs6d2QsQ7v5y9cdezp7JTmVfV/rj3WdV6nL1xrwvu/YKzHmdv9qbH2Rt3Pc7euOtx9mZvepy9cdfj7I27HmdvqvW4z2c5lx13Pc7e7E2Pszfuepy9cdfj7M3e9Dh7467H2Rt3Pc7eKoJesSha+H8k8PHxweHDh7X3+fn5GDFiBAICAnDixIky7dgt2P1rMBhKfFmrqdofZz3O3myhZ0/tBOCdX87euOvZU9mpzKtqf9z7rGo9zt641wX3fsFZj7M3e9Pj7I27Hmdv3PU4e7M3Pc7euOtx9sZdj7M31Xrc57Ocy467Hmdv9qbH2Rt3Pc7euOtx9mZvepy9cdfj7I27HmdvFUGvOIwPvlz/6HDjxg0ymf7/Zn+DwUCLFi2ibt26UXR0NKWnp+vW9PPzoy+//JLy8/OLfR04cKDc/HHW4+zNFnr21E6IeOeXszfuevZUdirzqtof9z6rWo+zN+51wb1fcNbj7M3e9Dh7467H2Rt3Pc7e7E2Pszfuepy9cdfj7E21Hvf5LOey467H2Zu96XH2xl2Pszfuepy92ZseZ2/c9Th7467H2VtF0CsOWWwvREhICO3fv7/I5wsWLKAePXpQ9+7ddWtGRERQUlJSiekGg4EAlIs/znqcvdlCz57aCRHv/HL2xl3PnspOZV5V++PeZ1XrcfbGvS649wvOepy92ZseZ2/c9Th7467H2Zu96XH2xl2Pszfuepy9qdbjPp/lXHbc9Th7szc9zt6463H2xl2Pszd70+PsjbseZ2/c9Th7qwh6xVKW2+EfVWbMmIEuXbqUmP7iiy/CYDDo0vz3v/+NrVu3lph+7do1JCQklIs/znqcvdlCz57aCcA7v5y9cdezp7JTmVfV/rj3WdV6nL1xrwvu/YKzHmdv9qbH2Rt3Pc7euOtx9mZvepy9cdfj7I27HmdvqvW4z2c5lx13Pc7e7E2Pszfuepy9cdfj7M3e9Dh7467H2Rt3Pc7eKoJecRiAB12uFwRBEARBEARBEARBEARBEARBEARBEAT7Qo6RFwRBEARBEARBEARBEARBEARBEARBEASdyGK7IAiCIAiCIAiCIAiCIAiCIAiCIAiCIOhEFtsFQRAEQRAEQRAEQRAEQRAEQRAEQRAEQSey2C4IgiAIgiAIgiAIgiAIgiAIgiAIgiAIOpHFdkEQBEEQBEEQBEFgxIoVK8jT07O8bVhgMBjoq6++Km8bgiAIgiAIgiAIgsAKWWwXBEEQBEEQBEEQBCsZPHgwGQwGMhgM5OjoSLVr16axY8fSzZs3lX1H3759KT093aq/Vb0wP2XKFGratGmRz8+cOUNdunRR8h25ubnUsGFDGjZsWJG0sWPHUu3atenq1atKvksQBEEQBEEQBEEQbImpvA0IgiAIgiAIgiAIQkUiJiaGli9fTrdu3aKkpCQaNGgQGQwGmjVrlhJ9V1dXcnV1VaJVQF5eHjk5OZX5//v6+irz4uzsTKtWraLIyEiKjY2lzp07ExHRzz//TB988AHt2LGDKlWqpOz7iIgA0J07d8hkkp9BBEEQBEEQBEEQBHXIne2CIAiCIAiCIAiCoANnZ2fy9fWlWrVqUc+ePenJJ5+k7du3ExFRfn4+vfvuu1S7dm1ydXWlJk2a0MaNGy3+/zfffEP169cnFxcXat++Pa1cuZIMBgNdunSJiIrerZ6cnEzt27enSpUqkYeHB0VERND+/fspISGBhgwZQpcvX9butp8yZQoREQUFBdHbb79NAwcOJA8PD+0u8nHjxlFwcDCZzWaqU6cOTZw4kW7duqV979SpUyk5OVnTW7FiBREVPUb+yJEj1KFDB3J1dSUvLy8aNmwYXbt2TUsfPHgw9ezZk+bMmUN+fn7k5eVFL7/8svZdERER9NZbb9HQoUPp0qVLdPPmTRoyZAi98sorFB0dTYmJidSmTRtydXWlWrVq0ciRI+n69eua/qeffkqPP/44VapUiXx9falfv370559/aukJCQlkMBho69atFBERQc7OzpSYmFj2ShcEQRAEQRAEQRCEYpDFdkEQBEEQBEEQBEEoI0ePHqW9e/dqd42/++67tGrVKlq8eDGlpKTQ6NGjacCAAbR7924iIsrKyqLevXtTz549KTk5mYYPH05vvfVWqd/Rv39/8vf3p3379lFSUhKNHz+eHB0dKSoqiubOnUseHh505swZOnPmDL322mva/5szZw41adKEDh48SBMnTiQiokqVKtGKFSvo119/pQ8//JCWLl1KH3zwARHdPb5+zJgx1LBhQ02vb9++Rfxcv36dOnfuTFWqVKF9+/bRhg0baMeOHRQXF2fxd/Hx8XTixAmKj4+nlStX0ooVK7TFeyKit956i3x9fWnkyJE0YcIEMhgMNGPGDDpx4gTFxMRQbGwsHT58mNavX0+JiYkW+rdu3aK3336bkpOT6auvvqKTJ0/S4MGDi3gdP348zZw5k1JTU6lx48allrMgCIIgCIIgCIIg6EXOTxMEQRAEQRAEQRAEHWzevJnc3d3p9u3blJubS0ajkRYsWEC5ubk0Y8YM2rFjB0VGRhIRUZ06dSgxMZGWLFlC0dHRtGTJEmrQoAHNnj2biIgaNGhAR48epenTp5f4fdnZ2fT6669TSEgIERHVr19fS6tcuTIZDIZij3nv0KEDjRkzxuKzCRMmaP8OCgqi1157jdatW0djx44lV1dXcnd3J5PJVOqx8WvWrKGbN2/SqlWryM3NjYiIFixYQN26daNZs2ZR9erViYioSpUqtGDBAnJwcKCQkBDq2rUr7dy5k1544QUiIjKZTLRq1SqKiIig/Px82rNnD7m4uNC7775L/fv3p1GjRmn5nTdvHkVHR9OiRYvIxcWFnn/+ec1PnTp1aN68edS8eXO6du0aubu7a2nTpk2jjh07lpgXQRAEQRAEQRAEQXgQZLFdEARBEARBEARBEHTQvn17WrRoEV2/fp0++OADMplMFBsbSykpKZSTk1NkcTcvL4/Cw8OJiOjYsWPUvHlzi/QWLVqU+n3//Oc/6R//+Ad9+umn9OSTT1KfPn2obt269/X5+OOPF/ls/fr1NG/ePDpx4gRdu3aNbt++TR4eHvfVKkxqaio1adJEW2gnImrdujXl5+fTsWPHtMX2hg0bkoODg/Y3fn5+dOTIEQutsLAwio2NpUuXLml+k5OT6fDhw7R69Wrt7wBQfn4+ZWVlUWhoKCUlJdGUKVMoOTmZ/vrrL8rPzyeiuxsTwsLCSi0DQRAEQRAEQRAEQVCFLLYLgiAIgiAIgiAIgg7c3NyoXr16RES0bNkyatKkCf3rX/+iRo0aERHRd999RzVr1rT4P87OzmX+vilTplC/fv3ou+++o61bt9LkyZNp3bp11KtXr/v6LMxPP/1E/fv3p6lTp1Lnzp2pcuXKtG7dOnr//ffL7K00HB0dLd4bDAZtUbwwJpOJTKb//3ni2rVrNHz4cBo5cmSRvw0ICNCOse/cuTOtXr2avL29KTs7mzp37kx5eXkWf39vGQiCIAiCIAiCIAiCSmSxXRAEQRAEQRAEQRDKiNFopDfffJP++c9/Unp6Ojk7O1N2djZFR0cX+/cNGjSgLVu2WHy2b9+++35PcHAwBQcH0+jRo+nZZ5+l5cuXU69evcjJyYnu3Lljlde9e/dSYGCgxTPiT506ZfE31uiFhobSihUr6Pr169pi9p49e8hoNFKDBg2s8lIazZo1o19//VXb0HAvR44coQsXLtDMmTOpVq1aRES0f//+B/5eQRAEQRAEQRAEQdCLsbwNCIIgCIIgCIIgCEJFpk+fPuTg4EBLliyh1157jUaPHk0rV66kEydO0IEDB2j+/Pm0cuVKIiIaPnw4paWl0bhx4yg9PZ0+//xzWrFiBRHdvfP7Xm7cuEFxcXGUkJBAp06doj179tC+ffsoNDSUiO4+d/3atWu0c+dOOn/+POXk5JTos379+pSdnU3r1q2jEydO0Lx582jTpk0WfxMUFERZWVl06NAhOn/+POXm5hbR6d+/P7m4uNCgQYPo6NGjFB8fT6+88go999xz2hHyD8K4ceNo7969FBcXR4cOHaKMjAz6+uuvKS4ujoju3t3u5ORE8+fPp8zMTPrmm2/o7bfffuDvFQRBEARBEARBEAS9yGK7IAiCIAiCIAiCIDwAJpOJ4uLi6L333qM33niDJk6cSO+++y6FhoZSTEwMfffdd1S7dm0iIqpduzZt3LiRvvzyS2rcuDEtWrRIu9O8uKPmHRwc6MKFCzRw4EAKDg6mp59+mrp06UJTp04lIqKoqCgaMWIE9e3bl7y9vem9994r0Wf37t1p9OjRFBcXR02bNqW9e/fSxIkTLf4mNjaWYmJiqH379uTt7U1r164tomM2m+n777+nixcvUvPmzal37970xBNP0IIFC8pchoVp3Lgx7d69m9LT06lNmzYUHh5OkyZNoho1ahARkbe3N61YsYI2bNhAYWFhNHPmTJozZ46S7xYEQRAEQRAEQRAEPRgAoLxNCIIgCIIgCIIgCIK9Mn36dFq8eDH997//LW8rgiAIgiAIgiAIgiDoQJ7ZLgiCIAiCIAiCIAgPkYULF1Lz5s3Jy8uL9uzZQ7Nnz9aOSBcEQRAEQRAEQRAEoeIgi+2CIAiCIAiCIAiC8BDJyMigd955hy5evEgBAQE0ZswYeuONN8rbliAIgiAIgiAIgiAIOpFj5AVBEARBEARBEARBEARBEARBEARBEARBJ8byNiAIgiAIgiAIgiAIgiAIgiAIgiAIgiAIFQ1ZbBcEQRAEQRAEQRAEQRAEQRAEQRAEQRAEnchiuyAIgiAIgiAIgiAIgiAIgiAIgiAIgiDoRBbbBUEQBEEQBEEQBEEQBEEQBEEQBEEQBEEnstguCIIgCIIgCIIgCIIgCIIgCIIgCIIgCDqRxXZBEARBEARBEARBEARBEARBEARBEARB0IkstguCIAiCIAiCIAiCIAiCIAiCIAiCIAiCTmSxXRAEQRAEQRAEQRAEQRAEQRAEQRAEQRB08n8roCzVPGOwDwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2500x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(25, 9))\n",
    "df['RegistrationYear'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cleaning our data, we drop duplicate values, as well as data that won't be useful for prediction, and handle outliers for RegistrationYear by converting them to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price                  171\n",
       "VehicleType          37484\n",
       "RegistrationYear       171\n",
       "Gearbox              19863\n",
       "Power                  171\n",
       "Model                19797\n",
       "Mileage                171\n",
       "RegistrationMonth      171\n",
       "FuelType             32932\n",
       "Brand                  171\n",
       "NotRepaired          71169\n",
       "NumberOfPictures       171\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VehicleType: 8\n",
      "Gearbox: 2\n",
      "Model: 250\n",
      "FuelType: 7\n",
      "Brand: 40\n",
      "NotRepaired: 2\n"
     ]
    }
   ],
   "source": [
    "cat_features = [\n",
    "    'VehicleType',\n",
    "    'Gearbox',\n",
    "    'Model',\n",
    "    'FuelType',\n",
    "    'Brand',\n",
    "    'NotRepaired'\n",
    "]\n",
    "\n",
    "for feature in cat_features:\n",
    "    print(f'{feature}: {df[feature].value_counts().count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values\n",
    "df[cat_features] = df[cat_features].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price                0\n",
       "VehicleType          0\n",
       "RegistrationYear     0\n",
       "Gearbox              0\n",
       "Power                0\n",
       "Model                0\n",
       "Mileage              0\n",
       "RegistrationMonth    0\n",
       "FuelType             0\n",
       "Brand                0\n",
       "NotRepaired          0\n",
       "NumberOfPictures     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a plot to see if we want to drop any categories that have\n",
    "# an excessive amount of data that might slow things down\n",
    "plt.figure(figsize=(15, 8))\n",
    "df.nunique().sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find missing values, establish our categorical features for encoding, fill in missing values, drop NaN values, and assess the dataframe for any categories to drop that might have an excessive amount of data and slow down model training, etc. Price has the most data compared to the other categories, but since it will be used as our target for machine learning, we leave this alone for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353936, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Power</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <th>VehicleType_convertible</th>\n",
       "      <th>VehicleType_coupe</th>\n",
       "      <th>VehicleType_other</th>\n",
       "      <th>VehicleType_sedan</th>\n",
       "      <th>...</th>\n",
       "      <th>Gearbox_unknown</th>\n",
       "      <th>FuelType_electric</th>\n",
       "      <th>FuelType_gasoline</th>\n",
       "      <th>FuelType_hybrid</th>\n",
       "      <th>FuelType_lpg</th>\n",
       "      <th>FuelType_other</th>\n",
       "      <th>FuelType_petrol</th>\n",
       "      <th>FuelType_unknown</th>\n",
       "      <th>NotRepaired_unknown</th>\n",
       "      <th>NotRepaired_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18300.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9800.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3600.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>650.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2200.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14500.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>999.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  RegistrationYear  Power   Mileage  RegistrationMonth  \\\n",
       "0    480.0            1993.0    0.0  150000.0                0.0   \n",
       "1  18300.0            2011.0  190.0  125000.0                5.0   \n",
       "2   9800.0            2004.0  163.0  125000.0                8.0   \n",
       "3   1500.0            2001.0   75.0  150000.0                6.0   \n",
       "4   3600.0            2008.0   69.0   90000.0                7.0   \n",
       "5    650.0            1995.0  102.0  150000.0               10.0   \n",
       "6   2200.0            2004.0  109.0  150000.0                8.0   \n",
       "7      0.0            1980.0   50.0   40000.0                7.0   \n",
       "8  14500.0            2014.0  125.0   30000.0                8.0   \n",
       "9    999.0            1998.0  101.0  150000.0                0.0   \n",
       "\n",
       "   NumberOfPictures  VehicleType_convertible  VehicleType_coupe  \\\n",
       "0               0.0                        0                  0   \n",
       "1               0.0                        0                  1   \n",
       "2               0.0                        0                  0   \n",
       "3               0.0                        0                  0   \n",
       "4               0.0                        0                  0   \n",
       "5               0.0                        0                  0   \n",
       "6               0.0                        1                  0   \n",
       "7               0.0                        0                  0   \n",
       "8               0.0                        0                  0   \n",
       "9               0.0                        0                  0   \n",
       "\n",
       "   VehicleType_other  VehicleType_sedan  ...  Gearbox_unknown  \\\n",
       "0                  0                  0  ...                0   \n",
       "1                  0                  0  ...                0   \n",
       "2                  0                  0  ...                0   \n",
       "3                  0                  0  ...                0   \n",
       "4                  0                  0  ...                0   \n",
       "5                  0                  1  ...                0   \n",
       "6                  0                  0  ...                0   \n",
       "7                  0                  1  ...                0   \n",
       "8                  0                  0  ...                0   \n",
       "9                  0                  0  ...                0   \n",
       "\n",
       "   FuelType_electric  FuelType_gasoline  FuelType_hybrid  FuelType_lpg  \\\n",
       "0                  0                  0                0             0   \n",
       "1                  0                  1                0             0   \n",
       "2                  0                  1                0             0   \n",
       "3                  0                  0                0             0   \n",
       "4                  0                  1                0             0   \n",
       "5                  0                  0                0             0   \n",
       "6                  0                  0                0             0   \n",
       "7                  0                  0                0             0   \n",
       "8                  0                  0                0             0   \n",
       "9                  0                  0                0             0   \n",
       "\n",
       "   FuelType_other  FuelType_petrol  FuelType_unknown  NotRepaired_unknown  \\\n",
       "0               0                1                 0                    1   \n",
       "1               0                0                 0                    0   \n",
       "2               0                0                 0                    1   \n",
       "3               0                1                 0                    0   \n",
       "4               0                0                 0                    0   \n",
       "5               0                1                 0                    0   \n",
       "6               0                1                 0                    0   \n",
       "7               0                1                 0                    0   \n",
       "8               0                1                 0                    1   \n",
       "9               0                0                 1                    1   \n",
       "\n",
       "   NotRepaired_yes  \n",
       "0                0  \n",
       "1                1  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "5                1  \n",
       "6                0  \n",
       "7                0  \n",
       "8                0  \n",
       "9                0  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OHE encoding\n",
    "df_ohe = df.drop(['Model', 'Brand'], axis=1) # drop categories with a lot of data\n",
    "df_ohe = pd.get_dummies(df_ohe, columns=['VehicleType', 'Gearbox', 'FuelType', 'NotRepaired'], drop_first=True, dtype='int')\n",
    "print(df_ohe.shape)\n",
    "df_ohe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353936, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Gearbox</th>\n",
       "      <th>Power</th>\n",
       "      <th>Model</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <th>FuelType</th>\n",
       "      <th>Brand</th>\n",
       "      <th>NotRepaired</th>\n",
       "      <th>NumberOfPictures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9800.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3600.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>650.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>999.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  VehicleType  RegistrationYear  Gearbox  Power  Model   Mileage  \\\n",
       "0    480.0          7.0            1993.0      1.0    0.0  116.0  150000.0   \n",
       "1  18300.0          2.0            2011.0      1.0  190.0  228.0  125000.0   \n",
       "2   9800.0          6.0            2004.0      0.0  163.0  117.0  125000.0   \n",
       "3   1500.0          5.0            2001.0      1.0   75.0  116.0  150000.0   \n",
       "4   3600.0          5.0            2008.0      1.0   69.0  101.0   90000.0   \n",
       "5    650.0          4.0            1995.0      1.0  102.0   11.0  150000.0   \n",
       "6   2200.0          1.0            2004.0      1.0  109.0    8.0  150000.0   \n",
       "7      0.0          4.0            1980.0      1.0   50.0  166.0   40000.0   \n",
       "8  14500.0          0.0            2014.0      1.0  125.0   60.0   30000.0   \n",
       "9    999.0          5.0            1998.0      1.0  101.0  116.0  150000.0   \n",
       "\n",
       "   RegistrationMonth  FuelType  Brand  NotRepaired  NumberOfPictures  \n",
       "0                0.0       6.0   38.0          1.0               0.0  \n",
       "1                5.0       2.0    1.0          2.0               0.0  \n",
       "2                8.0       2.0   14.0          1.0               0.0  \n",
       "3                6.0       6.0   38.0          0.0               0.0  \n",
       "4                7.0       2.0   31.0          0.0               0.0  \n",
       "5               10.0       6.0    2.0          2.0               0.0  \n",
       "6                8.0       6.0   25.0          0.0               0.0  \n",
       "7                7.0       6.0   38.0          0.0               0.0  \n",
       "8                8.0       6.0   10.0          1.0               0.0  \n",
       "9                0.0       7.0   38.0          1.0               0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ordinal encoding\n",
    "df_ord = df.copy() # use the full dataframe this time\n",
    "df_ord[cat_features] = OrdinalEncoder().fit_transform(df[cat_features])\n",
    "print(df_ord.shape)\n",
    "df_ord.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a previous look at our our categorical features counts, we drop two categories with the most data (Model, Brand) before running OHE encoding on the rest of the categorical features. We also run ordinal encoding for the entire categorical features set so that we have the appropriate encoded data to work with in anticipation of training different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "features = df.drop(['Price'], axis=1)\n",
    "target = df['Price']\n",
    "\n",
    "features_train, features_remaining, target_train, target_remaining = train_test_split(features, target, test_size=0.4, random_state=12345)\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_remaining, target_remaining, test_size=0.5, random_state=12345)\n",
    "\n",
    "# OHE encoding\n",
    "features_ohe = df_ohe.drop(['Price'], axis=1)\n",
    "target_ohe = df_ohe['Price']\n",
    "\n",
    "features_train_ohe, features_remaining_ohe, target_train_ohe, target_remaining_ohe = train_test_split(features_ohe, target_ohe, test_size=0.4, random_state=12345)\n",
    "features_valid_ohe, features_test_ohe, target_valid_ohe, target_test_ohe = train_test_split(features_remaining_ohe, target_remaining_ohe, test_size=0.5, random_state=12345)\n",
    "\n",
    "# Ordinal encoding\n",
    "features_ord = df_ord.drop(['Price'], axis=1)\n",
    "target_ord = df_ord['Price']\n",
    "\n",
    "features_train_ord, features_remaining_ord, target_train_ord, target_remaining_ord = train_test_split(features_ord, target_ord, test_size=0.4, random_state=12345)\n",
    "features_valid_ord, features_test_ord, target_valid_ord, target_test_ord = train_test_split(features_remaining_ord, target_remaining_ord, test_size=0.5, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection, tuning, and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, a):\n",
    "    return mean_squared_error(y, a) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use scoring='neg_root_mean_squared_error' instead\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(model, param_grid, features, target, cat_features=None):\n",
    "    gs = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=rmse_scorer,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    if cat_features is None:\n",
    "        gs.fit(features, target)\n",
    "    else:\n",
    "        gs.fit(\n",
    "            features,\n",
    "            target,\n",
    "            categorical_feature=cat_features\n",
    "        )\n",
    "    \n",
    "    print(f'Best RMSE score: {-gs.best_score_:.2f}')\n",
    "    print(f'Best params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(regressor, X_train, y_train, X_test, y_test, params=None, cat_features=None, eval_set=False):\n",
    "    start_time = time.time()\n",
    "\n",
    "    if params is None:\n",
    "        model = regressor()\n",
    "    else:\n",
    "        model = regressor(**params)\n",
    "\n",
    "    if cat_features is not None:\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            categorical_feature=cat_features\n",
    "        )\n",
    "    elif eval_set:\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_test, y_test)\n",
    "        )\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    start_time_predict = time.time()\n",
    "\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    end_time_predict = time.time()\n",
    "    prediction_time = end_time_predict - start_time_predict\n",
    "\n",
    "    print(f'RMSE score: {rmse(y_test, pred_test):.2f}')\n",
    "    print(f'Training time: {training_time:.2f}s')\n",
    "    print(f'Prediction time: {prediction_time:.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We establish a few functions, including one for RMSE calculation (plus creating a RMSE scorer for GridSearchCV), one for hyperparamter tuning, and one for model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE score: 4551.19\n"
     ]
    }
   ],
   "source": [
    "# TODO: the mean should be calculated on training set instead\n",
    "pred_mean = np.ones(target_test.shape) * target_test.mean()\n",
    "print(f\"Baseline RMSE score: {rmse(target_test, pred_mean):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "RMSE score: 3256.43\n",
      "Training time: 0.08s\n",
      "Prediction time: 0.02s\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "print('Linear Regression:')\n",
    "\n",
    "evaluate_model(\n",
    "    LinearRegression,\n",
    "    features_train_ohe,\n",
    "    target_train_ohe,\n",
    "    features_valid_ohe,\n",
    "    target_valid_ohe\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END ........................................max_depth=1; total time=   0.1s\n",
      "[CV] END ........................................max_depth=1; total time=   0.1s\n",
      "[CV] END ........................................max_depth=1; total time=   0.1s\n",
      "[CV] END ........................................max_depth=1; total time=   0.0s\n",
      "[CV] END ........................................max_depth=1; total time=   0.0s\n",
      "[CV] END ........................................max_depth=2; total time=   0.1s\n",
      "[CV] END ........................................max_depth=2; total time=   0.1s\n",
      "[CV] END ........................................max_depth=2; total time=   0.1s\n",
      "[CV] END ........................................max_depth=2; total time=   0.1s\n",
      "[CV] END ........................................max_depth=2; total time=   0.1s\n",
      "[CV] END ........................................max_depth=3; total time=   0.1s\n",
      "[CV] END ........................................max_depth=3; total time=   0.1s\n",
      "[CV] END ........................................max_depth=3; total time=   0.1s\n",
      "[CV] END ........................................max_depth=3; total time=   0.1s\n",
      "[CV] END ........................................max_depth=3; total time=   0.1s\n",
      "[CV] END ........................................max_depth=4; total time=   0.1s\n",
      "[CV] END ........................................max_depth=4; total time=   0.1s\n",
      "[CV] END ........................................max_depth=4; total time=   0.1s\n",
      "[CV] END ........................................max_depth=4; total time=   0.1s\n",
      "[CV] END ........................................max_depth=4; total time=   0.1s\n",
      "[CV] END ........................................max_depth=5; total time=   0.1s\n",
      "[CV] END ........................................max_depth=5; total time=   0.1s\n",
      "[CV] END ........................................max_depth=5; total time=   0.1s\n",
      "[CV] END ........................................max_depth=5; total time=   0.1s\n",
      "[CV] END ........................................max_depth=5; total time=   0.1s\n",
      "[CV] END ........................................max_depth=6; total time=   0.2s\n",
      "[CV] END ........................................max_depth=6; total time=   0.2s\n",
      "[CV] END ........................................max_depth=6; total time=   0.2s\n",
      "[CV] END ........................................max_depth=6; total time=   0.2s\n",
      "[CV] END ........................................max_depth=6; total time=   0.2s\n",
      "[CV] END ........................................max_depth=7; total time=   0.2s\n",
      "[CV] END ........................................max_depth=7; total time=   0.2s\n",
      "[CV] END ........................................max_depth=7; total time=   0.2s\n",
      "[CV] END ........................................max_depth=7; total time=   0.2s\n",
      "[CV] END ........................................max_depth=7; total time=   0.2s\n",
      "[CV] END ........................................max_depth=8; total time=   0.2s\n",
      "[CV] END ........................................max_depth=8; total time=   0.2s\n",
      "[CV] END ........................................max_depth=8; total time=   0.2s\n",
      "[CV] END ........................................max_depth=8; total time=   0.2s\n",
      "[CV] END ........................................max_depth=8; total time=   0.2s\n",
      "[CV] END ........................................max_depth=9; total time=   0.2s\n",
      "[CV] END ........................................max_depth=9; total time=   0.2s\n",
      "[CV] END ........................................max_depth=9; total time=   0.2s\n",
      "[CV] END ........................................max_depth=9; total time=   0.2s\n",
      "[CV] END ........................................max_depth=9; total time=   0.2s\n",
      "[CV] END .......................................max_depth=10; total time=   0.2s\n",
      "[CV] END .......................................max_depth=10; total time=   0.2s\n",
      "[CV] END .......................................max_depth=10; total time=   0.2s\n",
      "[CV] END .......................................max_depth=10; total time=   0.2s\n",
      "[CV] END .......................................max_depth=10; total time=   0.2s\n",
      "Best RMSE score: 2134.46\n",
      "Best params: {'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid_dt = {\n",
    "    'max_depth': range(1, 11)\n",
    "}\n",
    "\n",
    "model_dt = DecisionTreeRegressor(random_state=12345)\n",
    "\n",
    "print('Decision Tree:')\n",
    "\n",
    "tune_hyperparameters(model_dt, param_grid_dt, features_train_ord, target_train_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "RMSE score: 2139.64\n",
      "Training time: 0.29s\n",
      "Prediction time: 0.01s\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "params_dt = {\n",
    "    'max_depth': 10\n",
    "}\n",
    "\n",
    "print('Decision Tree:')\n",
    "\n",
    "evaluate_model(\n",
    "    DecisionTreeRegressor,\n",
    "    features_train_ord,\n",
    "    target_train_ord,\n",
    "    features_valid_ord,\n",
    "    target_valid_ord,\n",
    "    params_dt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] END .......................max_depth=1, n_estimators=10; total time=   0.2s\n",
      "[CV] END .......................max_depth=1, n_estimators=10; total time=   0.2s\n",
      "[CV] END .......................max_depth=1, n_estimators=10; total time=   0.2s\n",
      "[CV] END .......................max_depth=1, n_estimators=10; total time=   0.2s\n",
      "[CV] END .......................max_depth=1, n_estimators=10; total time=   0.2s\n",
      "[CV] END .......................max_depth=1, n_estimators=25; total time=   0.5s\n",
      "[CV] END .......................max_depth=1, n_estimators=25; total time=   0.5s\n",
      "[CV] END .......................max_depth=1, n_estimators=25; total time=   0.5s\n",
      "[CV] END .......................max_depth=1, n_estimators=25; total time=   0.5s\n",
      "[CV] END .......................max_depth=1, n_estimators=25; total time=   0.5s\n",
      "[CV] END .......................max_depth=1, n_estimators=50; total time=   1.0s\n",
      "[CV] END .......................max_depth=1, n_estimators=50; total time=   1.0s\n",
      "[CV] END .......................max_depth=1, n_estimators=50; total time=   1.0s\n",
      "[CV] END .......................max_depth=1, n_estimators=50; total time=   1.1s\n",
      "[CV] END .......................max_depth=1, n_estimators=50; total time=   1.0s\n",
      "[CV] END .......................max_depth=5, n_estimators=10; total time=   0.8s\n",
      "[CV] END .......................max_depth=5, n_estimators=10; total time=   0.8s\n",
      "[CV] END .......................max_depth=5, n_estimators=10; total time=   0.8s\n",
      "[CV] END .......................max_depth=5, n_estimators=10; total time=   0.8s\n",
      "[CV] END .......................max_depth=5, n_estimators=10; total time=   0.8s\n",
      "[CV] END .......................max_depth=5, n_estimators=25; total time=   2.4s\n",
      "[CV] END .......................max_depth=5, n_estimators=25; total time=   2.1s\n",
      "[CV] END .......................max_depth=5, n_estimators=25; total time=   2.1s\n",
      "[CV] END .......................max_depth=5, n_estimators=25; total time=   2.1s\n",
      "[CV] END .......................max_depth=5, n_estimators=25; total time=   2.1s\n",
      "[CV] END .......................max_depth=5, n_estimators=50; total time=   4.1s\n",
      "[CV] END .......................max_depth=5, n_estimators=50; total time=   4.1s\n",
      "[CV] END .......................max_depth=5, n_estimators=50; total time=   4.1s\n",
      "[CV] END .......................max_depth=5, n_estimators=50; total time=   4.2s\n",
      "[CV] END .......................max_depth=5, n_estimators=50; total time=   4.1s\n",
      "[CV] END ......................max_depth=10, n_estimators=10; total time=   1.5s\n",
      "[CV] END ......................max_depth=10, n_estimators=10; total time=   1.6s\n",
      "[CV] END ......................max_depth=10, n_estimators=10; total time=   1.5s\n",
      "[CV] END ......................max_depth=10, n_estimators=10; total time=   1.5s\n",
      "[CV] END ......................max_depth=10, n_estimators=10; total time=   1.6s\n",
      "[CV] END ......................max_depth=10, n_estimators=25; total time=   3.9s\n",
      "[CV] END ......................max_depth=10, n_estimators=25; total time=   3.8s\n",
      "[CV] END ......................max_depth=10, n_estimators=25; total time=   3.9s\n",
      "[CV] END ......................max_depth=10, n_estimators=25; total time=   3.9s\n",
      "[CV] END ......................max_depth=10, n_estimators=25; total time=   4.3s\n",
      "[CV] END ......................max_depth=10, n_estimators=50; total time=   8.5s\n",
      "[CV] END ......................max_depth=10, n_estimators=50; total time=   8.2s\n",
      "[CV] END ......................max_depth=10, n_estimators=50; total time=   7.7s\n",
      "[CV] END ......................max_depth=10, n_estimators=50; total time=   7.8s\n",
      "[CV] END ......................max_depth=10, n_estimators=50; total time=   7.6s\n",
      "Best RMSE score: 2013.28\n",
      "Best params: {'max_depth': 10, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 25, 50],\n",
    "    'max_depth': [1, 5, 10]\n",
    "}\n",
    "\n",
    "model_rf = RandomForestRegressor(random_state=12345)\n",
    "\n",
    "print('Random Forest:')\n",
    "\n",
    "tune_hyperparameters(model_rf, param_grid_rf, features_train_ord, target_train_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "RMSE score: 2037.44\n",
      "Training time: 9.34s\n",
      "Prediction time: 0.17s\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "params_rf = {\n",
    "    'max_depth': 10,\n",
    "    'n_estimators': 50,\n",
    "    'random_state': 12345\n",
    "}\n",
    "\n",
    "print('Random Forest:')\n",
    "\n",
    "evaluate_model(\n",
    "    RandomForestRegressor,\n",
    "    features_train_ord,\n",
    "    target_train_ord,\n",
    "    features_valid_ord,\n",
    "    target_valid_ord,\n",
    "    params_rf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our regression models, we see from a baseline as well as a sanity check that the RMSE score tends to improve. The best RMSE score comes from our random forest model, although it has the longest training time among all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build gradient boosting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM:\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=31; total time=   0.7s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=31; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=31; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=31; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002020 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END .....learning_rate=0.01, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002080 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END ....learning_rate=0.01, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002005 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001945 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END ....learning_rate=0.01, max_depth=15, num_leaves=50; total time=   0.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=31; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=31; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=31; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=31; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=31; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END .....learning_rate=0.05, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=50; total time=   0.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END ....learning_rate=0.05, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=50; total time=   0.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END ....learning_rate=0.05, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=31; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=31; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=31; total time=   0.5s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=31; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=31; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=50; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=50; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=50; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END ......learning_rate=0.1, max_depth=5, num_leaves=50; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=31; total time=   0.8s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END .....learning_rate=0.1, max_depth=10, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=20; total time=   0.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=20; total time=   0.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002427 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=31; total time=   0.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 169888, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4407.083608\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 677\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4415.425525\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4403.055436\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4413.590921\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 169889, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4405.984661\n",
      "[CV] END .....learning_rate=0.1, max_depth=15, num_leaves=50; total time=   0.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 212361, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4409.028032\n",
      "Best RMSE score: 1753.20\n",
      "Best params: {'learning_rate': 0.1, 'max_depth': 15, 'num_leaves': 50}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid_lgbm = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [20, 31, 50],\n",
    "    'max_depth': [5, 10, 15]\n",
    "}\n",
    "\n",
    "model_lgbm = lgb.LGBMRegressor()\n",
    "\n",
    "print('LightGBM:')\n",
    "\n",
    "tune_hyperparameters(\n",
    "    model_lgbm,\n",
    "    param_grid_lgbm,\n",
    "    features_train_ord,\n",
    "    target_train_ord,\n",
    "    cat_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM:\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 212361, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4409.028032\n",
      "RMSE score: 1755.27\n",
      "Training time: 0.90s\n",
      "Prediction time: 0.09s\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "params_lgbm = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 15,\n",
    "    'num_leaves': 50\n",
    "}  \n",
    "\n",
    "print('LightGBM:')\n",
    "\n",
    "evaluate_model(\n",
    "    lgb.LGBMRegressor,\n",
    "    features_train_ord,\n",
    "    target_train_ord,\n",
    "    features_valid_ord,\n",
    "    target_valid_ord,\n",
    "    params_lgbm,\n",
    "    cat_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost:\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 3327.5751724\ttotal: 110ms\tremaining: 988ms\n",
      "1:\tlearn: 2729.7689098\ttotal: 137ms\tremaining: 548ms\n",
      "2:\tlearn: 2446.5264211\ttotal: 163ms\tremaining: 381ms\n",
      "3:\tlearn: 2290.3729517\ttotal: 182ms\tremaining: 274ms\n",
      "4:\tlearn: 2201.5435688\ttotal: 197ms\tremaining: 197ms\n",
      "5:\tlearn: 2166.8367953\ttotal: 212ms\tremaining: 141ms\n",
      "6:\tlearn: 2124.3081587\ttotal: 238ms\tremaining: 102ms\n",
      "7:\tlearn: 2102.7946785\ttotal: 263ms\tremaining: 65.8ms\n",
      "8:\tlearn: 2084.2466712\ttotal: 282ms\tremaining: 31.4ms\n",
      "9:\tlearn: 2061.4271625\ttotal: 297ms\tremaining: 0us\n",
      "[CV] END ......................................iterations=10; total time=   0.5s\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 3260.8121783\ttotal: 14.4ms\tremaining: 130ms\n",
      "1:\tlearn: 2713.1607914\ttotal: 25.7ms\tremaining: 103ms\n",
      "2:\tlearn: 2430.7698196\ttotal: 39.3ms\tremaining: 91.7ms\n",
      "3:\tlearn: 2280.0861882\ttotal: 50.6ms\tremaining: 75.8ms\n",
      "4:\tlearn: 2209.2984413\ttotal: 60.8ms\tremaining: 60.8ms\n",
      "5:\tlearn: 2148.4123817\ttotal: 70.3ms\tremaining: 46.9ms\n",
      "6:\tlearn: 2110.7657872\ttotal: 81.9ms\tremaining: 35.1ms\n",
      "7:\tlearn: 2084.9114672\ttotal: 94.8ms\tremaining: 23.7ms\n",
      "8:\tlearn: 2067.0036904\ttotal: 114ms\tremaining: 12.7ms\n",
      "9:\tlearn: 2052.5722060\ttotal: 128ms\tremaining: 0us\n",
      "[CV] END ......................................iterations=10; total time=   0.3s\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 3259.5672165\ttotal: 12.8ms\tremaining: 115ms\n",
      "1:\tlearn: 2708.5662130\ttotal: 23.9ms\tremaining: 95.8ms\n",
      "2:\tlearn: 2446.2100604\ttotal: 36.4ms\tremaining: 85ms\n",
      "3:\tlearn: 2285.3119221\ttotal: 47ms\tremaining: 70.5ms\n",
      "4:\tlearn: 2210.2655515\ttotal: 59.6ms\tremaining: 59.6ms\n",
      "5:\tlearn: 2155.3516634\ttotal: 69.5ms\tremaining: 46.3ms\n",
      "6:\tlearn: 2113.7708837\ttotal: 80.6ms\tremaining: 34.5ms\n",
      "7:\tlearn: 2088.2447521\ttotal: 93.2ms\tremaining: 23.3ms\n",
      "8:\tlearn: 2068.5082718\ttotal: 106ms\tremaining: 11.7ms\n",
      "9:\tlearn: 2050.1621098\ttotal: 117ms\tremaining: 0us\n",
      "[CV] END ......................................iterations=10; total time=   0.2s\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 3285.9474978\ttotal: 15.7ms\tremaining: 141ms\n",
      "1:\tlearn: 2720.6734319\ttotal: 28.5ms\tremaining: 114ms\n",
      "2:\tlearn: 2452.7743510\ttotal: 41.1ms\tremaining: 96ms\n",
      "3:\tlearn: 2295.3864926\ttotal: 54.1ms\tremaining: 81.1ms\n",
      "4:\tlearn: 2213.2239386\ttotal: 65.6ms\tremaining: 65.6ms\n",
      "5:\tlearn: 2153.9265370\ttotal: 75.1ms\tremaining: 50.1ms\n",
      "6:\tlearn: 2110.0304517\ttotal: 86.4ms\tremaining: 37ms\n",
      "7:\tlearn: 2087.2648676\ttotal: 97.5ms\tremaining: 24.4ms\n",
      "8:\tlearn: 2069.6429386\ttotal: 108ms\tremaining: 12ms\n",
      "9:\tlearn: 2046.3217730\ttotal: 119ms\tremaining: 0us\n",
      "[CV] END ......................................iterations=10; total time=   0.2s\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 3262.9636629\ttotal: 13.4ms\tremaining: 121ms\n",
      "1:\tlearn: 2701.4451413\ttotal: 29.6ms\tremaining: 118ms\n",
      "2:\tlearn: 2439.3291401\ttotal: 41.9ms\tremaining: 97.7ms\n",
      "3:\tlearn: 2289.7836550\ttotal: 53.7ms\tremaining: 80.5ms\n",
      "4:\tlearn: 2218.8427608\ttotal: 65.1ms\tremaining: 65.1ms\n",
      "5:\tlearn: 2154.2040746\ttotal: 74.8ms\tremaining: 49.9ms\n",
      "6:\tlearn: 2110.6631168\ttotal: 86ms\tremaining: 36.8ms\n",
      "7:\tlearn: 2089.1160317\ttotal: 99ms\tremaining: 24.8ms\n",
      "8:\tlearn: 2065.9422204\ttotal: 110ms\tremaining: 12.2ms\n",
      "9:\tlearn: 2052.4527891\ttotal: 121ms\tremaining: 0us\n",
      "[CV] END ......................................iterations=10; total time=   0.2s\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 3327.5751724\ttotal: 13.4ms\tremaining: 1.32s\n",
      "1:\tlearn: 2729.7689098\ttotal: 27.9ms\tremaining: 1.36s\n",
      "2:\tlearn: 2446.5264211\ttotal: 41.6ms\tremaining: 1.34s\n",
      "3:\tlearn: 2290.3729517\ttotal: 54ms\tremaining: 1.29s\n",
      "4:\tlearn: 2201.5435688\ttotal: 67.5ms\tremaining: 1.28s\n",
      "5:\tlearn: 2166.8367953\ttotal: 78.4ms\tremaining: 1.23s\n",
      "6:\tlearn: 2124.3081587\ttotal: 89.3ms\tremaining: 1.19s\n",
      "7:\tlearn: 2102.7946785\ttotal: 102ms\tremaining: 1.17s\n",
      "8:\tlearn: 2084.2466712\ttotal: 112ms\tremaining: 1.13s\n",
      "9:\tlearn: 2061.4271625\ttotal: 122ms\tremaining: 1.1s\n",
      "10:\tlearn: 2045.0688248\ttotal: 132ms\tremaining: 1.06s\n",
      "11:\tlearn: 2029.7741773\ttotal: 143ms\tremaining: 1.05s\n",
      "12:\tlearn: 2014.1558514\ttotal: 155ms\tremaining: 1.04s\n",
      "13:\tlearn: 2006.0431029\ttotal: 166ms\tremaining: 1.02s\n",
      "14:\tlearn: 2000.5804003\ttotal: 176ms\tremaining: 996ms\n",
      "15:\tlearn: 1995.5663266\ttotal: 187ms\tremaining: 982ms\n",
      "16:\tlearn: 1985.8014154\ttotal: 198ms\tremaining: 966ms\n",
      "17:\tlearn: 1976.5768213\ttotal: 209ms\tremaining: 950ms\n",
      "18:\tlearn: 1963.4419661\ttotal: 219ms\tremaining: 935ms\n",
      "19:\tlearn: 1955.5405651\ttotal: 229ms\tremaining: 917ms\n",
      "20:\tlearn: 1952.1426047\ttotal: 242ms\tremaining: 909ms\n",
      "21:\tlearn: 1945.0178992\ttotal: 253ms\tremaining: 897ms\n",
      "22:\tlearn: 1940.8377660\ttotal: 264ms\tremaining: 885ms\n",
      "23:\tlearn: 1935.2168591\ttotal: 274ms\tremaining: 869ms\n",
      "24:\tlearn: 1929.1691568\ttotal: 286ms\tremaining: 859ms\n",
      "25:\tlearn: 1923.8808428\ttotal: 296ms\tremaining: 843ms\n",
      "26:\tlearn: 1918.5944569\ttotal: 308ms\tremaining: 832ms\n",
      "27:\tlearn: 1913.4237067\ttotal: 319ms\tremaining: 821ms\n",
      "28:\tlearn: 1908.6351204\ttotal: 330ms\tremaining: 809ms\n",
      "29:\tlearn: 1902.7431084\ttotal: 342ms\tremaining: 797ms\n",
      "30:\tlearn: 1899.2110017\ttotal: 353ms\tremaining: 785ms\n",
      "31:\tlearn: 1894.2001486\ttotal: 365ms\tremaining: 777ms\n",
      "32:\tlearn: 1890.0227189\ttotal: 377ms\tremaining: 766ms\n",
      "33:\tlearn: 1887.1189235\ttotal: 389ms\tremaining: 756ms\n",
      "34:\tlearn: 1883.8008194\ttotal: 401ms\tremaining: 745ms\n",
      "35:\tlearn: 1881.8964158\ttotal: 416ms\tremaining: 740ms\n",
      "36:\tlearn: 1878.9747089\ttotal: 429ms\tremaining: 731ms\n",
      "37:\tlearn: 1875.6506338\ttotal: 443ms\tremaining: 722ms\n",
      "38:\tlearn: 1872.5469022\ttotal: 454ms\tremaining: 710ms\n",
      "39:\tlearn: 1868.9458680\ttotal: 465ms\tremaining: 697ms\n",
      "40:\tlearn: 1865.7964690\ttotal: 476ms\tremaining: 684ms\n",
      "41:\tlearn: 1861.0252743\ttotal: 486ms\tremaining: 671ms\n",
      "42:\tlearn: 1859.1781808\ttotal: 498ms\tremaining: 660ms\n",
      "43:\tlearn: 1856.5070252\ttotal: 508ms\tremaining: 646ms\n",
      "44:\tlearn: 1852.8362206\ttotal: 518ms\tremaining: 633ms\n",
      "45:\tlearn: 1851.4919462\ttotal: 529ms\tremaining: 621ms\n",
      "46:\tlearn: 1848.9796060\ttotal: 552ms\tremaining: 623ms\n",
      "47:\tlearn: 1846.8570024\ttotal: 566ms\tremaining: 613ms\n",
      "48:\tlearn: 1843.5163438\ttotal: 577ms\tremaining: 601ms\n",
      "49:\tlearn: 1841.6649111\ttotal: 589ms\tremaining: 589ms\n",
      "50:\tlearn: 1839.5994981\ttotal: 602ms\tremaining: 578ms\n",
      "51:\tlearn: 1837.1259232\ttotal: 613ms\tremaining: 566ms\n",
      "52:\tlearn: 1834.7251975\ttotal: 625ms\tremaining: 555ms\n",
      "53:\tlearn: 1831.2549698\ttotal: 636ms\tremaining: 542ms\n",
      "54:\tlearn: 1828.7867822\ttotal: 647ms\tremaining: 530ms\n",
      "55:\tlearn: 1826.9699526\ttotal: 659ms\tremaining: 518ms\n",
      "56:\tlearn: 1825.9071790\ttotal: 669ms\tremaining: 505ms\n",
      "57:\tlearn: 1823.8450486\ttotal: 679ms\tremaining: 492ms\n",
      "58:\tlearn: 1821.3654699\ttotal: 691ms\tremaining: 480ms\n",
      "59:\tlearn: 1818.6074252\ttotal: 701ms\tremaining: 467ms\n",
      "60:\tlearn: 1817.1348564\ttotal: 712ms\tremaining: 455ms\n",
      "61:\tlearn: 1815.4814408\ttotal: 722ms\tremaining: 443ms\n",
      "62:\tlearn: 1813.6851181\ttotal: 733ms\tremaining: 430ms\n",
      "63:\tlearn: 1812.3853077\ttotal: 745ms\tremaining: 419ms\n",
      "64:\tlearn: 1811.2040139\ttotal: 758ms\tremaining: 408ms\n",
      "65:\tlearn: 1809.2794977\ttotal: 769ms\tremaining: 396ms\n",
      "66:\tlearn: 1808.2256288\ttotal: 781ms\tremaining: 385ms\n",
      "67:\tlearn: 1807.3197790\ttotal: 793ms\tremaining: 373ms\n",
      "68:\tlearn: 1805.1956709\ttotal: 804ms\tremaining: 361ms\n",
      "69:\tlearn: 1803.9759439\ttotal: 816ms\tremaining: 350ms\n",
      "70:\tlearn: 1803.0289324\ttotal: 829ms\tremaining: 339ms\n",
      "71:\tlearn: 1801.0796967\ttotal: 840ms\tremaining: 327ms\n",
      "72:\tlearn: 1799.5758119\ttotal: 851ms\tremaining: 315ms\n",
      "73:\tlearn: 1798.5486837\ttotal: 864ms\tremaining: 303ms\n",
      "74:\tlearn: 1795.9343087\ttotal: 877ms\tremaining: 292ms\n",
      "75:\tlearn: 1794.4996446\ttotal: 889ms\tremaining: 281ms\n",
      "76:\tlearn: 1793.0182264\ttotal: 903ms\tremaining: 270ms\n",
      "77:\tlearn: 1790.8549108\ttotal: 914ms\tremaining: 258ms\n",
      "78:\tlearn: 1788.6524973\ttotal: 925ms\tremaining: 246ms\n",
      "79:\tlearn: 1787.4288999\ttotal: 936ms\tremaining: 234ms\n",
      "80:\tlearn: 1786.7033839\ttotal: 947ms\tremaining: 222ms\n",
      "81:\tlearn: 1785.7524294\ttotal: 958ms\tremaining: 210ms\n",
      "82:\tlearn: 1784.5067914\ttotal: 970ms\tremaining: 199ms\n",
      "83:\tlearn: 1783.9219198\ttotal: 982ms\tremaining: 187ms\n",
      "84:\tlearn: 1782.9907949\ttotal: 993ms\tremaining: 175ms\n",
      "85:\tlearn: 1781.2296360\ttotal: 1s\tremaining: 163ms\n",
      "86:\tlearn: 1780.6318386\ttotal: 1.02s\tremaining: 152ms\n",
      "87:\tlearn: 1779.8755237\ttotal: 1.03s\tremaining: 140ms\n",
      "88:\tlearn: 1778.1192113\ttotal: 1.04s\tremaining: 128ms\n",
      "89:\tlearn: 1776.6128078\ttotal: 1.05s\tremaining: 117ms\n",
      "90:\tlearn: 1775.1795940\ttotal: 1.06s\tremaining: 105ms\n",
      "91:\tlearn: 1774.2932738\ttotal: 1.07s\tremaining: 93.1ms\n",
      "92:\tlearn: 1773.7154391\ttotal: 1.08s\tremaining: 81.5ms\n",
      "93:\tlearn: 1772.0802441\ttotal: 1.09s\tremaining: 69.7ms\n",
      "94:\tlearn: 1771.3988562\ttotal: 1.1s\tremaining: 58.1ms\n",
      "95:\tlearn: 1770.5147810\ttotal: 1.11s\tremaining: 46.5ms\n",
      "96:\tlearn: 1769.6123553\ttotal: 1.13s\tremaining: 34.8ms\n",
      "97:\tlearn: 1768.4151493\ttotal: 1.14s\tremaining: 23.2ms\n",
      "98:\tlearn: 1766.8949305\ttotal: 1.15s\tremaining: 11.6ms\n",
      "99:\tlearn: 1766.2006579\ttotal: 1.16s\tremaining: 0us\n",
      "[CV] END .....................................iterations=100; total time=   1.3s\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 3260.8121783\ttotal: 12ms\tremaining: 1.19s\n",
      "1:\tlearn: 2713.1607914\ttotal: 23ms\tremaining: 1.13s\n",
      "2:\tlearn: 2430.7698196\ttotal: 35.7ms\tremaining: 1.15s\n",
      "3:\tlearn: 2280.0861882\ttotal: 45.7ms\tremaining: 1.1s\n",
      "4:\tlearn: 2209.2984413\ttotal: 55.9ms\tremaining: 1.06s\n",
      "5:\tlearn: 2148.4123817\ttotal: 65.6ms\tremaining: 1.03s\n",
      "6:\tlearn: 2110.7657872\ttotal: 80.4ms\tremaining: 1.07s\n",
      "7:\tlearn: 2084.9114672\ttotal: 94.6ms\tremaining: 1.09s\n",
      "8:\tlearn: 2067.0036904\ttotal: 107ms\tremaining: 1.08s\n",
      "9:\tlearn: 2052.5722060\ttotal: 120ms\tremaining: 1.08s\n",
      "10:\tlearn: 2032.7865201\ttotal: 130ms\tremaining: 1.05s\n",
      "11:\tlearn: 2019.8402119\ttotal: 140ms\tremaining: 1.03s\n",
      "12:\tlearn: 2011.2993997\ttotal: 150ms\tremaining: 1.01s\n",
      "13:\tlearn: 2003.0955447\ttotal: 161ms\tremaining: 988ms\n",
      "14:\tlearn: 1990.1948942\ttotal: 172ms\tremaining: 977ms\n",
      "15:\tlearn: 1982.4415458\ttotal: 184ms\tremaining: 965ms\n",
      "16:\tlearn: 1976.3414758\ttotal: 196ms\tremaining: 957ms\n",
      "17:\tlearn: 1970.1076500\ttotal: 207ms\tremaining: 942ms\n",
      "18:\tlearn: 1961.3142897\ttotal: 217ms\tremaining: 924ms\n",
      "19:\tlearn: 1950.7855322\ttotal: 228ms\tremaining: 913ms\n",
      "20:\tlearn: 1946.3196283\ttotal: 239ms\tremaining: 900ms\n",
      "21:\tlearn: 1943.8699343\ttotal: 251ms\tremaining: 891ms\n",
      "22:\tlearn: 1939.3130968\ttotal: 263ms\tremaining: 881ms\n",
      "23:\tlearn: 1929.2082740\ttotal: 273ms\tremaining: 864ms\n",
      "24:\tlearn: 1925.1659615\ttotal: 284ms\tremaining: 852ms\n",
      "25:\tlearn: 1920.1769746\ttotal: 296ms\tremaining: 842ms\n",
      "26:\tlearn: 1916.1479772\ttotal: 306ms\tremaining: 828ms\n",
      "27:\tlearn: 1912.2118128\ttotal: 319ms\tremaining: 819ms\n",
      "28:\tlearn: 1904.8539861\ttotal: 329ms\tremaining: 804ms\n",
      "29:\tlearn: 1899.4182502\ttotal: 340ms\tremaining: 794ms\n",
      "30:\tlearn: 1894.0278724\ttotal: 352ms\tremaining: 784ms\n",
      "31:\tlearn: 1890.9986301\ttotal: 364ms\tremaining: 773ms\n",
      "32:\tlearn: 1887.2630812\ttotal: 375ms\tremaining: 762ms\n",
      "33:\tlearn: 1883.2905026\ttotal: 385ms\tremaining: 748ms\n",
      "34:\tlearn: 1879.8911185\ttotal: 396ms\tremaining: 735ms\n",
      "35:\tlearn: 1876.6329518\ttotal: 408ms\tremaining: 725ms\n",
      "36:\tlearn: 1873.1881217\ttotal: 442ms\tremaining: 752ms\n",
      "37:\tlearn: 1870.7321932\ttotal: 470ms\tremaining: 766ms\n",
      "38:\tlearn: 1867.3872048\ttotal: 486ms\tremaining: 761ms\n",
      "39:\tlearn: 1863.9421331\ttotal: 498ms\tremaining: 747ms\n",
      "40:\tlearn: 1859.2659766\ttotal: 508ms\tremaining: 731ms\n",
      "41:\tlearn: 1857.3460448\ttotal: 519ms\tremaining: 717ms\n",
      "42:\tlearn: 1855.4045956\ttotal: 529ms\tremaining: 701ms\n",
      "43:\tlearn: 1851.0595135\ttotal: 559ms\tremaining: 711ms\n",
      "44:\tlearn: 1848.8289077\ttotal: 573ms\tremaining: 700ms\n",
      "45:\tlearn: 1846.2064825\ttotal: 588ms\tremaining: 690ms\n",
      "46:\tlearn: 1844.3282211\ttotal: 606ms\tremaining: 683ms\n",
      "47:\tlearn: 1842.7158292\ttotal: 620ms\tremaining: 672ms\n",
      "48:\tlearn: 1840.8180019\ttotal: 635ms\tremaining: 661ms\n",
      "49:\tlearn: 1838.2501942\ttotal: 646ms\tremaining: 646ms\n",
      "50:\tlearn: 1836.0154711\ttotal: 658ms\tremaining: 632ms\n",
      "51:\tlearn: 1832.7206006\ttotal: 669ms\tremaining: 618ms\n",
      "52:\tlearn: 1831.1999739\ttotal: 681ms\tremaining: 604ms\n",
      "53:\tlearn: 1828.9031457\ttotal: 692ms\tremaining: 590ms\n",
      "54:\tlearn: 1825.0270914\ttotal: 702ms\tremaining: 574ms\n",
      "55:\tlearn: 1823.3559884\ttotal: 713ms\tremaining: 561ms\n",
      "56:\tlearn: 1821.7350302\ttotal: 726ms\tremaining: 548ms\n",
      "57:\tlearn: 1819.9149555\ttotal: 737ms\tremaining: 533ms\n",
      "58:\tlearn: 1818.1852622\ttotal: 748ms\tremaining: 520ms\n",
      "59:\tlearn: 1815.3808593\ttotal: 758ms\tremaining: 505ms\n",
      "60:\tlearn: 1814.3752171\ttotal: 769ms\tremaining: 492ms\n",
      "61:\tlearn: 1812.7519223\ttotal: 780ms\tremaining: 478ms\n",
      "62:\tlearn: 1812.2724186\ttotal: 791ms\tremaining: 465ms\n",
      "63:\tlearn: 1810.1184737\ttotal: 802ms\tremaining: 451ms\n",
      "64:\tlearn: 1806.6642576\ttotal: 811ms\tremaining: 437ms\n",
      "65:\tlearn: 1804.4641523\ttotal: 821ms\tremaining: 423ms\n",
      "66:\tlearn: 1803.7747403\ttotal: 832ms\tremaining: 410ms\n",
      "67:\tlearn: 1802.9235422\ttotal: 845ms\tremaining: 398ms\n",
      "68:\tlearn: 1800.8137277\ttotal: 855ms\tremaining: 384ms\n",
      "69:\tlearn: 1799.2197136\ttotal: 866ms\tremaining: 371ms\n",
      "70:\tlearn: 1798.7939990\ttotal: 878ms\tremaining: 359ms\n",
      "71:\tlearn: 1796.2444382\ttotal: 888ms\tremaining: 345ms\n",
      "72:\tlearn: 1794.3116153\ttotal: 898ms\tremaining: 332ms\n",
      "73:\tlearn: 1793.4300151\ttotal: 910ms\tremaining: 320ms\n",
      "74:\tlearn: 1791.9655317\ttotal: 921ms\tremaining: 307ms\n",
      "75:\tlearn: 1790.9551126\ttotal: 932ms\tremaining: 294ms\n",
      "76:\tlearn: 1789.6719071\ttotal: 943ms\tremaining: 282ms\n",
      "77:\tlearn: 1788.0773634\ttotal: 954ms\tremaining: 269ms\n",
      "78:\tlearn: 1786.4511962\ttotal: 964ms\tremaining: 256ms\n",
      "79:\tlearn: 1785.5043031\ttotal: 975ms\tremaining: 244ms\n",
      "80:\tlearn: 1783.7397583\ttotal: 985ms\tremaining: 231ms\n",
      "81:\tlearn: 1782.9410527\ttotal: 997ms\tremaining: 219ms\n",
      "82:\tlearn: 1781.9448525\ttotal: 1.01s\tremaining: 207ms\n",
      "83:\tlearn: 1780.9902128\ttotal: 1.02s\tremaining: 195ms\n",
      "84:\tlearn: 1780.2558446\ttotal: 1.04s\tremaining: 183ms\n",
      "85:\tlearn: 1779.4291986\ttotal: 1.05s\tremaining: 171ms\n",
      "86:\tlearn: 1778.5974167\ttotal: 1.06s\tremaining: 158ms\n",
      "87:\tlearn: 1777.7165667\ttotal: 1.07s\tremaining: 146ms\n",
      "88:\tlearn: 1776.9937563\ttotal: 1.08s\tremaining: 133ms\n",
      "89:\tlearn: 1776.1948067\ttotal: 1.09s\tremaining: 121ms\n",
      "90:\tlearn: 1774.9748816\ttotal: 1.1s\tremaining: 109ms\n",
      "91:\tlearn: 1773.9580840\ttotal: 1.11s\tremaining: 96.9ms\n",
      "92:\tlearn: 1773.4389002\ttotal: 1.13s\tremaining: 84.7ms\n",
      "93:\tlearn: 1772.0720393\ttotal: 1.15s\tremaining: 73.2ms\n",
      "94:\tlearn: 1770.3567436\ttotal: 1.17s\tremaining: 61.5ms\n",
      "95:\tlearn: 1769.2392189\ttotal: 1.21s\tremaining: 50.5ms\n",
      "96:\tlearn: 1768.9055761\ttotal: 1.28s\tremaining: 39.6ms\n",
      "97:\tlearn: 1767.6388274\ttotal: 1.33s\tremaining: 27.1ms\n",
      "98:\tlearn: 1767.0905497\ttotal: 1.37s\tremaining: 13.8ms\n",
      "99:\tlearn: 1766.1322302\ttotal: 1.38s\tremaining: 0us\n",
      "[CV] END .....................................iterations=100; total time=   1.5s\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 3259.5672165\ttotal: 12.4ms\tremaining: 1.23s\n",
      "1:\tlearn: 2708.5662130\ttotal: 23.5ms\tremaining: 1.15s\n",
      "2:\tlearn: 2446.2100604\ttotal: 35.6ms\tremaining: 1.15s\n",
      "3:\tlearn: 2285.3119221\ttotal: 45.8ms\tremaining: 1.1s\n",
      "4:\tlearn: 2210.2655515\ttotal: 57.7ms\tremaining: 1.1s\n",
      "5:\tlearn: 2155.3516634\ttotal: 67.5ms\tremaining: 1.06s\n",
      "6:\tlearn: 2113.7708837\ttotal: 78.2ms\tremaining: 1.04s\n",
      "7:\tlearn: 2088.2447521\ttotal: 91ms\tremaining: 1.05s\n",
      "8:\tlearn: 2068.5082718\ttotal: 103ms\tremaining: 1.04s\n",
      "9:\tlearn: 2050.1621098\ttotal: 115ms\tremaining: 1.03s\n",
      "10:\tlearn: 2032.7286142\ttotal: 126ms\tremaining: 1.02s\n",
      "11:\tlearn: 2019.7445319\ttotal: 137ms\tremaining: 1.01s\n",
      "12:\tlearn: 2011.1287680\ttotal: 148ms\tremaining: 993ms\n",
      "13:\tlearn: 2002.3705958\ttotal: 160ms\tremaining: 982ms\n",
      "14:\tlearn: 1990.4100442\ttotal: 170ms\tremaining: 963ms\n",
      "15:\tlearn: 1973.3618096\ttotal: 180ms\tremaining: 943ms\n",
      "16:\tlearn: 1966.9635620\ttotal: 190ms\tremaining: 925ms\n",
      "17:\tlearn: 1956.1038398\ttotal: 200ms\tremaining: 913ms\n",
      "18:\tlearn: 1949.1744895\ttotal: 212ms\tremaining: 905ms\n",
      "19:\tlearn: 1944.5460725\ttotal: 223ms\tremaining: 892ms\n",
      "20:\tlearn: 1937.9884244\ttotal: 233ms\tremaining: 875ms\n",
      "21:\tlearn: 1932.8620480\ttotal: 243ms\tremaining: 860ms\n",
      "22:\tlearn: 1928.8849211\ttotal: 254ms\tremaining: 851ms\n",
      "23:\tlearn: 1924.2543692\ttotal: 266ms\tremaining: 844ms\n",
      "24:\tlearn: 1922.0414766\ttotal: 280ms\tremaining: 839ms\n",
      "25:\tlearn: 1916.5624246\ttotal: 292ms\tremaining: 830ms\n",
      "26:\tlearn: 1910.2538308\ttotal: 302ms\tremaining: 817ms\n",
      "27:\tlearn: 1906.7489883\ttotal: 314ms\tremaining: 808ms\n",
      "28:\tlearn: 1902.6407069\ttotal: 325ms\tremaining: 795ms\n",
      "29:\tlearn: 1899.3873718\ttotal: 336ms\tremaining: 784ms\n",
      "30:\tlearn: 1896.2882271\ttotal: 347ms\tremaining: 771ms\n",
      "31:\tlearn: 1893.5933500\ttotal: 359ms\tremaining: 763ms\n",
      "32:\tlearn: 1887.2723897\ttotal: 368ms\tremaining: 748ms\n",
      "33:\tlearn: 1884.5703334\ttotal: 381ms\tremaining: 739ms\n",
      "34:\tlearn: 1881.3287849\ttotal: 391ms\tremaining: 727ms\n",
      "35:\tlearn: 1877.9881832\ttotal: 402ms\tremaining: 715ms\n",
      "36:\tlearn: 1875.2218998\ttotal: 415ms\tremaining: 706ms\n",
      "37:\tlearn: 1872.4631050\ttotal: 425ms\tremaining: 694ms\n",
      "38:\tlearn: 1870.8842451\ttotal: 437ms\tremaining: 684ms\n",
      "39:\tlearn: 1867.4394781\ttotal: 449ms\tremaining: 674ms\n",
      "40:\tlearn: 1865.3578078\ttotal: 461ms\tremaining: 664ms\n",
      "41:\tlearn: 1863.8839948\ttotal: 473ms\tremaining: 653ms\n",
      "42:\tlearn: 1860.6375647\ttotal: 485ms\tremaining: 643ms\n",
      "43:\tlearn: 1858.8666799\ttotal: 496ms\tremaining: 632ms\n",
      "44:\tlearn: 1856.9887071\ttotal: 508ms\tremaining: 621ms\n",
      "45:\tlearn: 1855.1984882\ttotal: 519ms\tremaining: 609ms\n",
      "46:\tlearn: 1853.2492485\ttotal: 530ms\tremaining: 598ms\n",
      "47:\tlearn: 1851.6876795\ttotal: 541ms\tremaining: 586ms\n",
      "48:\tlearn: 1849.9245731\ttotal: 551ms\tremaining: 574ms\n",
      "49:\tlearn: 1848.1019817\ttotal: 562ms\tremaining: 562ms\n",
      "50:\tlearn: 1846.7991320\ttotal: 575ms\tremaining: 552ms\n",
      "51:\tlearn: 1843.1769984\ttotal: 585ms\tremaining: 540ms\n",
      "52:\tlearn: 1841.1199945\ttotal: 598ms\tremaining: 530ms\n",
      "53:\tlearn: 1838.9078501\ttotal: 616ms\tremaining: 525ms\n",
      "54:\tlearn: 1837.5907041\ttotal: 650ms\tremaining: 532ms\n",
      "55:\tlearn: 1835.1211698\ttotal: 667ms\tremaining: 524ms\n",
      "56:\tlearn: 1831.6803809\ttotal: 678ms\tremaining: 512ms\n",
      "57:\tlearn: 1829.3607632\ttotal: 690ms\tremaining: 500ms\n",
      "58:\tlearn: 1827.2212010\ttotal: 702ms\tremaining: 488ms\n",
      "59:\tlearn: 1825.1971138\ttotal: 714ms\tremaining: 476ms\n",
      "60:\tlearn: 1822.0429692\ttotal: 725ms\tremaining: 463ms\n",
      "61:\tlearn: 1821.2306690\ttotal: 737ms\tremaining: 452ms\n",
      "62:\tlearn: 1819.8512845\ttotal: 750ms\tremaining: 440ms\n",
      "63:\tlearn: 1818.5251216\ttotal: 762ms\tremaining: 429ms\n",
      "64:\tlearn: 1817.1237678\ttotal: 775ms\tremaining: 418ms\n",
      "65:\tlearn: 1815.7910070\ttotal: 788ms\tremaining: 406ms\n",
      "66:\tlearn: 1814.3032242\ttotal: 801ms\tremaining: 395ms\n",
      "67:\tlearn: 1812.9111619\ttotal: 813ms\tremaining: 383ms\n",
      "68:\tlearn: 1811.0187923\ttotal: 825ms\tremaining: 371ms\n",
      "69:\tlearn: 1809.2343737\ttotal: 836ms\tremaining: 358ms\n",
      "70:\tlearn: 1806.0467702\ttotal: 848ms\tremaining: 346ms\n",
      "71:\tlearn: 1804.0012084\ttotal: 861ms\tremaining: 335ms\n",
      "72:\tlearn: 1802.1379700\ttotal: 875ms\tremaining: 324ms\n",
      "73:\tlearn: 1800.8425274\ttotal: 889ms\tremaining: 312ms\n",
      "74:\tlearn: 1799.3033819\ttotal: 901ms\tremaining: 300ms\n",
      "75:\tlearn: 1797.2978370\ttotal: 912ms\tremaining: 288ms\n",
      "76:\tlearn: 1796.1466284\ttotal: 924ms\tremaining: 276ms\n",
      "77:\tlearn: 1795.0940728\ttotal: 935ms\tremaining: 264ms\n",
      "78:\tlearn: 1793.3808851\ttotal: 947ms\tremaining: 252ms\n",
      "79:\tlearn: 1792.1860421\ttotal: 960ms\tremaining: 240ms\n",
      "80:\tlearn: 1790.0142876\ttotal: 971ms\tremaining: 228ms\n",
      "81:\tlearn: 1788.8240050\ttotal: 985ms\tremaining: 216ms\n",
      "82:\tlearn: 1787.5427555\ttotal: 998ms\tremaining: 204ms\n",
      "83:\tlearn: 1786.6156275\ttotal: 1.01s\tremaining: 193ms\n",
      "84:\tlearn: 1784.7342593\ttotal: 1.03s\tremaining: 181ms\n",
      "85:\tlearn: 1784.1637875\ttotal: 1.04s\tremaining: 169ms\n",
      "86:\tlearn: 1783.1213877\ttotal: 1.05s\tremaining: 158ms\n",
      "87:\tlearn: 1782.3412350\ttotal: 1.07s\tremaining: 146ms\n",
      "88:\tlearn: 1781.5637147\ttotal: 1.08s\tremaining: 134ms\n",
      "89:\tlearn: 1779.8145863\ttotal: 1.09s\tremaining: 122ms\n",
      "90:\tlearn: 1778.2389028\ttotal: 1.11s\tremaining: 109ms\n",
      "91:\tlearn: 1777.0787415\ttotal: 1.12s\tremaining: 97.2ms\n",
      "92:\tlearn: 1775.5756070\ttotal: 1.13s\tremaining: 85.2ms\n",
      "93:\tlearn: 1774.3383503\ttotal: 1.14s\tremaining: 73ms\n",
      "94:\tlearn: 1772.4087762\ttotal: 1.16s\tremaining: 60.9ms\n",
      "95:\tlearn: 1771.0211603\ttotal: 1.17s\tremaining: 48.7ms\n",
      "96:\tlearn: 1768.6486907\ttotal: 1.18s\tremaining: 36.5ms\n",
      "97:\tlearn: 1768.0855198\ttotal: 1.19s\tremaining: 24.3ms\n",
      "98:\tlearn: 1767.9951229\ttotal: 1.21s\tremaining: 12.2ms\n",
      "99:\tlearn: 1767.2517141\ttotal: 1.22s\tremaining: 0us\n",
      "[CV] END .....................................iterations=100; total time=   1.4s\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 3285.9474978\ttotal: 12ms\tremaining: 1.18s\n",
      "1:\tlearn: 2720.6734319\ttotal: 23.3ms\tremaining: 1.14s\n",
      "2:\tlearn: 2452.7743510\ttotal: 34.5ms\tremaining: 1.12s\n",
      "3:\tlearn: 2295.3864926\ttotal: 45.8ms\tremaining: 1.1s\n",
      "4:\tlearn: 2213.2239386\ttotal: 57.3ms\tremaining: 1.09s\n",
      "5:\tlearn: 2153.9265370\ttotal: 66.6ms\tremaining: 1.04s\n",
      "6:\tlearn: 2110.0304517\ttotal: 77.6ms\tremaining: 1.03s\n",
      "7:\tlearn: 2087.2648676\ttotal: 112ms\tremaining: 1.29s\n",
      "8:\tlearn: 2069.6429386\ttotal: 125ms\tremaining: 1.26s\n",
      "9:\tlearn: 2046.3217730\ttotal: 141ms\tremaining: 1.27s\n",
      "10:\tlearn: 2035.8580577\ttotal: 156ms\tremaining: 1.26s\n",
      "11:\tlearn: 2021.1061853\ttotal: 168ms\tremaining: 1.23s\n",
      "12:\tlearn: 2009.2620736\ttotal: 179ms\tremaining: 1.2s\n",
      "13:\tlearn: 1995.2607930\ttotal: 194ms\tremaining: 1.19s\n",
      "14:\tlearn: 1987.4458339\ttotal: 209ms\tremaining: 1.19s\n",
      "15:\tlearn: 1981.3107172\ttotal: 226ms\tremaining: 1.19s\n",
      "16:\tlearn: 1972.5733388\ttotal: 240ms\tremaining: 1.17s\n",
      "17:\tlearn: 1964.8884297\ttotal: 253ms\tremaining: 1.15s\n",
      "18:\tlearn: 1961.0738506\ttotal: 266ms\tremaining: 1.13s\n",
      "19:\tlearn: 1953.3931548\ttotal: 276ms\tremaining: 1.1s\n",
      "20:\tlearn: 1944.7008826\ttotal: 287ms\tremaining: 1.08s\n",
      "21:\tlearn: 1941.5799723\ttotal: 297ms\tremaining: 1.05s\n",
      "22:\tlearn: 1937.3889111\ttotal: 307ms\tremaining: 1.03s\n",
      "23:\tlearn: 1932.2607580\ttotal: 319ms\tremaining: 1.01s\n",
      "24:\tlearn: 1927.5191957\ttotal: 331ms\tremaining: 994ms\n",
      "25:\tlearn: 1919.8700613\ttotal: 341ms\tremaining: 970ms\n",
      "26:\tlearn: 1913.3950853\ttotal: 351ms\tremaining: 949ms\n",
      "27:\tlearn: 1907.7614745\ttotal: 361ms\tremaining: 930ms\n",
      "28:\tlearn: 1904.1085545\ttotal: 374ms\tremaining: 915ms\n",
      "29:\tlearn: 1900.8660085\ttotal: 386ms\tremaining: 900ms\n",
      "30:\tlearn: 1897.3145758\ttotal: 399ms\tremaining: 888ms\n",
      "31:\tlearn: 1892.7008821\ttotal: 415ms\tremaining: 881ms\n",
      "32:\tlearn: 1889.4299932\ttotal: 429ms\tremaining: 870ms\n",
      "33:\tlearn: 1886.8816633\ttotal: 441ms\tremaining: 856ms\n",
      "34:\tlearn: 1882.8925501\ttotal: 452ms\tremaining: 840ms\n",
      "35:\tlearn: 1880.1114087\ttotal: 465ms\tremaining: 826ms\n",
      "36:\tlearn: 1876.2861001\ttotal: 475ms\tremaining: 809ms\n",
      "37:\tlearn: 1873.7110765\ttotal: 487ms\tremaining: 795ms\n",
      "38:\tlearn: 1870.2228329\ttotal: 498ms\tremaining: 779ms\n",
      "39:\tlearn: 1867.0191179\ttotal: 510ms\tremaining: 765ms\n",
      "40:\tlearn: 1865.1508931\ttotal: 522ms\tremaining: 752ms\n",
      "41:\tlearn: 1862.6433037\ttotal: 536ms\tremaining: 740ms\n",
      "42:\tlearn: 1860.8224680\ttotal: 564ms\tremaining: 747ms\n",
      "43:\tlearn: 1857.2753385\ttotal: 582ms\tremaining: 741ms\n",
      "44:\tlearn: 1855.0178113\ttotal: 594ms\tremaining: 726ms\n",
      "45:\tlearn: 1853.1623208\ttotal: 606ms\tremaining: 712ms\n",
      "46:\tlearn: 1851.4225354\ttotal: 620ms\tremaining: 699ms\n",
      "47:\tlearn: 1849.8803674\ttotal: 634ms\tremaining: 687ms\n",
      "48:\tlearn: 1847.4277819\ttotal: 645ms\tremaining: 671ms\n",
      "49:\tlearn: 1844.5122201\ttotal: 655ms\tremaining: 655ms\n",
      "50:\tlearn: 1842.5625409\ttotal: 667ms\tremaining: 641ms\n",
      "51:\tlearn: 1840.6691376\ttotal: 678ms\tremaining: 626ms\n",
      "52:\tlearn: 1838.1899146\ttotal: 689ms\tremaining: 611ms\n",
      "53:\tlearn: 1836.4952582\ttotal: 701ms\tremaining: 597ms\n",
      "54:\tlearn: 1834.1419353\ttotal: 710ms\tremaining: 581ms\n",
      "55:\tlearn: 1829.7846985\ttotal: 720ms\tremaining: 566ms\n",
      "56:\tlearn: 1828.4152137\ttotal: 731ms\tremaining: 552ms\n",
      "57:\tlearn: 1826.8025079\ttotal: 744ms\tremaining: 539ms\n",
      "58:\tlearn: 1825.7205512\ttotal: 756ms\tremaining: 525ms\n",
      "59:\tlearn: 1823.9833759\ttotal: 767ms\tremaining: 511ms\n",
      "60:\tlearn: 1822.3873738\ttotal: 778ms\tremaining: 497ms\n",
      "61:\tlearn: 1820.8020814\ttotal: 788ms\tremaining: 483ms\n",
      "62:\tlearn: 1819.3687597\ttotal: 799ms\tremaining: 469ms\n",
      "63:\tlearn: 1817.6476582\ttotal: 810ms\tremaining: 455ms\n",
      "64:\tlearn: 1816.9327269\ttotal: 821ms\tremaining: 442ms\n",
      "65:\tlearn: 1814.0788715\ttotal: 831ms\tremaining: 428ms\n",
      "66:\tlearn: 1811.2845120\ttotal: 841ms\tremaining: 414ms\n",
      "67:\tlearn: 1809.8992243\ttotal: 852ms\tremaining: 401ms\n",
      "68:\tlearn: 1807.7865595\ttotal: 863ms\tremaining: 388ms\n",
      "69:\tlearn: 1806.3233859\ttotal: 875ms\tremaining: 375ms\n",
      "70:\tlearn: 1804.7518240\ttotal: 886ms\tremaining: 362ms\n",
      "71:\tlearn: 1803.6819541\ttotal: 898ms\tremaining: 349ms\n",
      "72:\tlearn: 1802.0534748\ttotal: 910ms\tremaining: 336ms\n",
      "73:\tlearn: 1800.8297938\ttotal: 932ms\tremaining: 327ms\n",
      "74:\tlearn: 1798.2691044\ttotal: 950ms\tremaining: 317ms\n",
      "75:\tlearn: 1797.5398871\ttotal: 961ms\tremaining: 303ms\n",
      "76:\tlearn: 1796.3487696\ttotal: 973ms\tremaining: 291ms\n",
      "77:\tlearn: 1795.5156795\ttotal: 985ms\tremaining: 278ms\n",
      "78:\tlearn: 1793.4896612\ttotal: 995ms\tremaining: 264ms\n",
      "79:\tlearn: 1792.9796747\ttotal: 1.01s\tremaining: 252ms\n",
      "80:\tlearn: 1791.9076205\ttotal: 1.02s\tremaining: 239ms\n",
      "81:\tlearn: 1790.5474299\ttotal: 1.03s\tremaining: 227ms\n",
      "82:\tlearn: 1789.5483944\ttotal: 1.05s\tremaining: 215ms\n",
      "83:\tlearn: 1788.5749942\ttotal: 1.07s\tremaining: 204ms\n",
      "84:\tlearn: 1787.3351152\ttotal: 1.08s\tremaining: 192ms\n",
      "85:\tlearn: 1785.6879884\ttotal: 1.1s\tremaining: 179ms\n",
      "86:\tlearn: 1784.7403986\ttotal: 1.11s\tremaining: 166ms\n",
      "87:\tlearn: 1784.4539967\ttotal: 1.12s\tremaining: 153ms\n",
      "88:\tlearn: 1781.9516686\ttotal: 1.13s\tremaining: 140ms\n",
      "89:\tlearn: 1781.4193071\ttotal: 1.14s\tremaining: 127ms\n",
      "90:\tlearn: 1779.8906503\ttotal: 1.16s\tremaining: 114ms\n",
      "91:\tlearn: 1779.1023653\ttotal: 1.17s\tremaining: 102ms\n",
      "92:\tlearn: 1777.7517593\ttotal: 1.18s\tremaining: 88.7ms\n",
      "93:\tlearn: 1776.0031860\ttotal: 1.19s\tremaining: 75.8ms\n",
      "94:\tlearn: 1774.6942076\ttotal: 1.2s\tremaining: 63.1ms\n",
      "95:\tlearn: 1773.5338404\ttotal: 1.21s\tremaining: 50.4ms\n",
      "96:\tlearn: 1772.8404005\ttotal: 1.22s\tremaining: 37.8ms\n",
      "97:\tlearn: 1772.5221495\ttotal: 1.23s\tremaining: 25.2ms\n",
      "98:\tlearn: 1771.5290598\ttotal: 1.25s\tremaining: 12.6ms\n",
      "99:\tlearn: 1770.4187690\ttotal: 1.27s\tremaining: 0us\n",
      "[CV] END .....................................iterations=100; total time=   1.4s\n",
      "Learning rate set to 0.5\n",
      "0:\tlearn: 3262.9636629\ttotal: 13ms\tremaining: 1.28s\n",
      "1:\tlearn: 2701.4451413\ttotal: 24.7ms\tremaining: 1.21s\n",
      "2:\tlearn: 2439.3291401\ttotal: 37.1ms\tremaining: 1.2s\n",
      "3:\tlearn: 2289.7836550\ttotal: 47.1ms\tremaining: 1.13s\n",
      "4:\tlearn: 2218.8427608\ttotal: 58.5ms\tremaining: 1.11s\n",
      "5:\tlearn: 2154.2040746\ttotal: 68.4ms\tremaining: 1.07s\n",
      "6:\tlearn: 2110.6631168\ttotal: 79.3ms\tremaining: 1.05s\n",
      "7:\tlearn: 2089.1160317\ttotal: 91.9ms\tremaining: 1.06s\n",
      "8:\tlearn: 2065.9422204\ttotal: 103ms\tremaining: 1.04s\n",
      "9:\tlearn: 2052.4527891\ttotal: 114ms\tremaining: 1.03s\n",
      "10:\tlearn: 2037.2570441\ttotal: 126ms\tremaining: 1.02s\n",
      "11:\tlearn: 2028.6138149\ttotal: 138ms\tremaining: 1.01s\n",
      "12:\tlearn: 2014.2563870\ttotal: 148ms\tremaining: 991ms\n",
      "13:\tlearn: 2004.7886967\ttotal: 158ms\tremaining: 973ms\n",
      "14:\tlearn: 1998.0558396\ttotal: 170ms\tremaining: 962ms\n",
      "15:\tlearn: 1986.9567725\ttotal: 180ms\tremaining: 945ms\n",
      "16:\tlearn: 1976.3917899\ttotal: 191ms\tremaining: 933ms\n",
      "17:\tlearn: 1967.2775726\ttotal: 202ms\tremaining: 918ms\n",
      "18:\tlearn: 1959.6936314\ttotal: 213ms\tremaining: 910ms\n",
      "19:\tlearn: 1951.8318716\ttotal: 225ms\tremaining: 898ms\n",
      "20:\tlearn: 1947.1684728\ttotal: 237ms\tremaining: 893ms\n",
      "21:\tlearn: 1941.5341375\ttotal: 252ms\tremaining: 892ms\n",
      "22:\tlearn: 1936.5550970\ttotal: 263ms\tremaining: 882ms\n",
      "23:\tlearn: 1927.5254249\ttotal: 273ms\tremaining: 865ms\n",
      "24:\tlearn: 1924.4298313\ttotal: 285ms\tremaining: 856ms\n",
      "25:\tlearn: 1915.6513506\ttotal: 295ms\tremaining: 840ms\n",
      "26:\tlearn: 1909.2837396\ttotal: 307ms\tremaining: 830ms\n",
      "27:\tlearn: 1904.9277492\ttotal: 318ms\tremaining: 817ms\n",
      "28:\tlearn: 1902.1369660\ttotal: 333ms\tremaining: 815ms\n",
      "29:\tlearn: 1897.8386697\ttotal: 345ms\tremaining: 805ms\n",
      "30:\tlearn: 1894.7015117\ttotal: 358ms\tremaining: 796ms\n",
      "31:\tlearn: 1890.5914666\ttotal: 368ms\tremaining: 782ms\n",
      "32:\tlearn: 1888.6156470\ttotal: 380ms\tremaining: 772ms\n",
      "33:\tlearn: 1884.3758499\ttotal: 391ms\tremaining: 760ms\n",
      "34:\tlearn: 1879.0635485\ttotal: 403ms\tremaining: 748ms\n",
      "35:\tlearn: 1875.4699611\ttotal: 414ms\tremaining: 736ms\n",
      "36:\tlearn: 1872.0757694\ttotal: 439ms\tremaining: 748ms\n",
      "37:\tlearn: 1867.6264921\ttotal: 460ms\tremaining: 751ms\n",
      "38:\tlearn: 1864.6752669\ttotal: 471ms\tremaining: 736ms\n",
      "39:\tlearn: 1862.1559842\ttotal: 481ms\tremaining: 722ms\n",
      "40:\tlearn: 1859.5039276\ttotal: 492ms\tremaining: 708ms\n",
      "41:\tlearn: 1855.3397355\ttotal: 502ms\tremaining: 693ms\n",
      "42:\tlearn: 1853.2620367\ttotal: 514ms\tremaining: 681ms\n",
      "43:\tlearn: 1851.3987349\ttotal: 526ms\tremaining: 669ms\n",
      "44:\tlearn: 1846.3697457\ttotal: 536ms\tremaining: 655ms\n",
      "45:\tlearn: 1843.5980789\ttotal: 547ms\tremaining: 642ms\n",
      "46:\tlearn: 1841.2918298\ttotal: 559ms\tremaining: 631ms\n",
      "47:\tlearn: 1838.9486494\ttotal: 569ms\tremaining: 617ms\n",
      "48:\tlearn: 1835.3408884\ttotal: 580ms\tremaining: 604ms\n",
      "49:\tlearn: 1833.5979472\ttotal: 591ms\tremaining: 591ms\n",
      "50:\tlearn: 1831.8999947\ttotal: 602ms\tremaining: 579ms\n",
      "51:\tlearn: 1830.1063366\ttotal: 614ms\tremaining: 567ms\n",
      "52:\tlearn: 1829.1029165\ttotal: 627ms\tremaining: 556ms\n",
      "53:\tlearn: 1826.8779912\ttotal: 637ms\tremaining: 543ms\n",
      "54:\tlearn: 1823.8353322\ttotal: 649ms\tremaining: 531ms\n",
      "55:\tlearn: 1822.1919449\ttotal: 660ms\tremaining: 519ms\n",
      "56:\tlearn: 1818.7580029\ttotal: 671ms\tremaining: 507ms\n",
      "57:\tlearn: 1817.2186452\ttotal: 684ms\tremaining: 495ms\n",
      "58:\tlearn: 1816.4868169\ttotal: 695ms\tremaining: 483ms\n",
      "59:\tlearn: 1815.4165319\ttotal: 706ms\tremaining: 471ms\n",
      "60:\tlearn: 1814.7420417\ttotal: 718ms\tremaining: 459ms\n",
      "61:\tlearn: 1812.2281384\ttotal: 728ms\tremaining: 446ms\n",
      "62:\tlearn: 1810.0483616\ttotal: 739ms\tremaining: 434ms\n",
      "63:\tlearn: 1808.3925241\ttotal: 749ms\tremaining: 421ms\n",
      "64:\tlearn: 1806.3451390\ttotal: 759ms\tremaining: 409ms\n",
      "65:\tlearn: 1804.4375210\ttotal: 770ms\tremaining: 397ms\n",
      "66:\tlearn: 1802.7647035\ttotal: 780ms\tremaining: 384ms\n",
      "67:\tlearn: 1801.6699515\ttotal: 791ms\tremaining: 372ms\n",
      "68:\tlearn: 1800.5542791\ttotal: 802ms\tremaining: 361ms\n",
      "69:\tlearn: 1799.6705959\ttotal: 814ms\tremaining: 349ms\n",
      "70:\tlearn: 1798.4010014\ttotal: 833ms\tremaining: 340ms\n",
      "71:\tlearn: 1797.2602577\ttotal: 854ms\tremaining: 332ms\n",
      "72:\tlearn: 1796.7282366\ttotal: 873ms\tremaining: 323ms\n",
      "73:\tlearn: 1795.7636756\ttotal: 887ms\tremaining: 312ms\n",
      "74:\tlearn: 1794.7457402\ttotal: 900ms\tremaining: 300ms\n",
      "75:\tlearn: 1791.7120788\ttotal: 911ms\tremaining: 288ms\n",
      "76:\tlearn: 1790.4922231\ttotal: 924ms\tremaining: 276ms\n",
      "77:\tlearn: 1789.1814226\ttotal: 936ms\tremaining: 264ms\n",
      "78:\tlearn: 1787.1905907\ttotal: 947ms\tremaining: 252ms\n",
      "79:\tlearn: 1786.0132200\ttotal: 960ms\tremaining: 240ms\n",
      "80:\tlearn: 1785.2680226\ttotal: 971ms\tremaining: 228ms\n",
      "81:\tlearn: 1783.5858971\ttotal: 982ms\tremaining: 216ms\n",
      "82:\tlearn: 1782.2384766\ttotal: 993ms\tremaining: 203ms\n",
      "83:\tlearn: 1780.6645319\ttotal: 1s\tremaining: 191ms\n",
      "84:\tlearn: 1779.4064914\ttotal: 1.01s\tremaining: 179ms\n",
      "85:\tlearn: 1778.4629912\ttotal: 1.02s\tremaining: 167ms\n",
      "86:\tlearn: 1777.5011493\ttotal: 1.04s\tremaining: 155ms\n",
      "87:\tlearn: 1775.8841864\ttotal: 1.05s\tremaining: 143ms\n",
      "88:\tlearn: 1774.0382708\ttotal: 1.06s\tremaining: 131ms\n",
      "89:\tlearn: 1773.3347862\ttotal: 1.07s\tremaining: 119ms\n",
      "90:\tlearn: 1772.5891041\ttotal: 1.08s\tremaining: 107ms\n",
      "91:\tlearn: 1771.9887587\ttotal: 1.1s\tremaining: 95.5ms\n",
      "92:\tlearn: 1770.6723321\ttotal: 1.11s\tremaining: 83.5ms\n",
      "93:\tlearn: 1769.3925744\ttotal: 1.12s\tremaining: 71.4ms\n",
      "94:\tlearn: 1769.0917634\ttotal: 1.13s\tremaining: 59.5ms\n",
      "95:\tlearn: 1767.9892295\ttotal: 1.14s\tremaining: 47.6ms\n",
      "96:\tlearn: 1766.8518570\ttotal: 1.15s\tremaining: 35.7ms\n",
      "97:\tlearn: 1766.5719370\ttotal: 1.17s\tremaining: 23.8ms\n",
      "98:\tlearn: 1765.5603200\ttotal: 1.18s\tremaining: 11.9ms\n",
      "99:\tlearn: 1764.6234390\ttotal: 1.19s\tremaining: 0us\n",
      "[CV] END .....................................iterations=100; total time=   1.3s\n",
      "Learning rate set to 0.284459\n",
      "0:\tlearn: 3733.2358921\ttotal: 24.5ms\tremaining: 6.09s\n",
      "1:\tlearn: 3219.9453710\ttotal: 47.5ms\tremaining: 5.89s\n",
      "2:\tlearn: 2867.1219313\ttotal: 73.6ms\tremaining: 6.06s\n",
      "3:\tlearn: 2632.0856811\ttotal: 99.7ms\tremaining: 6.13s\n",
      "4:\tlearn: 2473.7948557\ttotal: 128ms\tremaining: 6.28s\n",
      "5:\tlearn: 2364.0998327\ttotal: 156ms\tremaining: 6.36s\n",
      "6:\tlearn: 2270.7600335\ttotal: 184ms\tremaining: 6.38s\n",
      "7:\tlearn: 2199.8819725\ttotal: 206ms\tremaining: 6.24s\n",
      "8:\tlearn: 2155.8775135\ttotal: 231ms\tremaining: 6.18s\n",
      "9:\tlearn: 2118.5093427\ttotal: 253ms\tremaining: 6.08s\n",
      "10:\tlearn: 2096.2973417\ttotal: 277ms\tremaining: 6.01s\n",
      "11:\tlearn: 2080.5681039\ttotal: 303ms\tremaining: 6s\n",
      "12:\tlearn: 2061.9791720\ttotal: 322ms\tremaining: 5.88s\n",
      "13:\tlearn: 2048.5865466\ttotal: 339ms\tremaining: 5.72s\n",
      "14:\tlearn: 2035.2239332\ttotal: 364ms\tremaining: 5.7s\n",
      "15:\tlearn: 2022.9664155\ttotal: 391ms\tremaining: 5.72s\n",
      "16:\tlearn: 2015.8394235\ttotal: 421ms\tremaining: 5.76s\n",
      "17:\tlearn: 2002.7844962\ttotal: 482ms\tremaining: 6.21s\n",
      "18:\tlearn: 1993.3799614\ttotal: 512ms\tremaining: 6.22s\n",
      "19:\tlearn: 1984.7087447\ttotal: 542ms\tremaining: 6.23s\n",
      "20:\tlearn: 1979.0505502\ttotal: 582ms\tremaining: 6.35s\n",
      "21:\tlearn: 1973.1866448\ttotal: 609ms\tremaining: 6.31s\n",
      "22:\tlearn: 1966.8053695\ttotal: 635ms\tremaining: 6.26s\n",
      "23:\tlearn: 1957.2837559\ttotal: 661ms\tremaining: 6.23s\n",
      "24:\tlearn: 1954.4603039\ttotal: 689ms\tremaining: 6.2s\n",
      "25:\tlearn: 1950.8390161\ttotal: 718ms\tremaining: 6.19s\n",
      "26:\tlearn: 1946.9507817\ttotal: 742ms\tremaining: 6.13s\n",
      "27:\tlearn: 1941.1124599\ttotal: 775ms\tremaining: 6.14s\n",
      "28:\tlearn: 1934.8534732\ttotal: 795ms\tremaining: 6.05s\n",
      "29:\tlearn: 1930.8449684\ttotal: 816ms\tremaining: 5.98s\n",
      "30:\tlearn: 1925.7616546\ttotal: 836ms\tremaining: 5.9s\n",
      "31:\tlearn: 1922.1128956\ttotal: 864ms\tremaining: 5.89s\n",
      "32:\tlearn: 1918.2479649\ttotal: 885ms\tremaining: 5.82s\n",
      "33:\tlearn: 1916.5633179\ttotal: 916ms\tremaining: 5.82s\n",
      "34:\tlearn: 1911.5848014\ttotal: 938ms\tremaining: 5.76s\n",
      "35:\tlearn: 1909.4577585\ttotal: 966ms\tremaining: 5.74s\n",
      "36:\tlearn: 1907.1285621\ttotal: 994ms\tremaining: 5.72s\n",
      "37:\tlearn: 1903.1283249\ttotal: 1.02s\tremaining: 5.69s\n",
      "38:\tlearn: 1899.6258609\ttotal: 1.04s\tremaining: 5.63s\n",
      "39:\tlearn: 1897.0990238\ttotal: 1.07s\tremaining: 5.61s\n",
      "40:\tlearn: 1894.2182877\ttotal: 1.1s\tremaining: 5.61s\n",
      "41:\tlearn: 1891.5030468\ttotal: 1.13s\tremaining: 5.61s\n",
      "42:\tlearn: 1887.2639191\ttotal: 1.16s\tremaining: 5.6s\n",
      "43:\tlearn: 1885.6418545\ttotal: 1.2s\tremaining: 5.6s\n",
      "44:\tlearn: 1883.8733057\ttotal: 1.22s\tremaining: 5.58s\n",
      "45:\tlearn: 1882.0764091\ttotal: 1.25s\tremaining: 5.54s\n",
      "46:\tlearn: 1878.3397046\ttotal: 1.27s\tremaining: 5.5s\n",
      "47:\tlearn: 1875.5401185\ttotal: 1.3s\tremaining: 5.47s\n",
      "48:\tlearn: 1873.6334312\ttotal: 1.33s\tremaining: 5.46s\n",
      "49:\tlearn: 1872.2119257\ttotal: 1.36s\tremaining: 5.44s\n",
      "50:\tlearn: 1869.2194762\ttotal: 1.38s\tremaining: 5.4s\n",
      "51:\tlearn: 1866.7471539\ttotal: 1.41s\tremaining: 5.38s\n",
      "52:\tlearn: 1864.2332518\ttotal: 1.43s\tremaining: 5.33s\n",
      "53:\tlearn: 1863.1426511\ttotal: 1.47s\tremaining: 5.35s\n",
      "54:\tlearn: 1859.3667012\ttotal: 1.5s\tremaining: 5.32s\n",
      "55:\tlearn: 1856.4645493\ttotal: 1.52s\tremaining: 5.28s\n",
      "56:\tlearn: 1854.4386463\ttotal: 1.55s\tremaining: 5.25s\n",
      "57:\tlearn: 1853.1702397\ttotal: 1.58s\tremaining: 5.22s\n",
      "58:\tlearn: 1850.7148530\ttotal: 1.6s\tremaining: 5.19s\n",
      "59:\tlearn: 1848.6806091\ttotal: 1.63s\tremaining: 5.16s\n",
      "60:\tlearn: 1847.0180912\ttotal: 1.66s\tremaining: 5.13s\n",
      "61:\tlearn: 1845.5585221\ttotal: 1.68s\tremaining: 5.09s\n",
      "62:\tlearn: 1843.3142241\ttotal: 1.7s\tremaining: 5.04s\n",
      "63:\tlearn: 1840.5178845\ttotal: 1.72s\tremaining: 5.01s\n",
      "64:\tlearn: 1837.8320305\ttotal: 1.74s\tremaining: 4.96s\n",
      "65:\tlearn: 1835.6283229\ttotal: 1.76s\tremaining: 4.92s\n",
      "66:\tlearn: 1833.4700361\ttotal: 1.79s\tremaining: 4.89s\n",
      "67:\tlearn: 1831.5007499\ttotal: 1.82s\tremaining: 4.86s\n",
      "68:\tlearn: 1829.8813724\ttotal: 1.86s\tremaining: 4.87s\n",
      "69:\tlearn: 1829.1215153\ttotal: 1.89s\tremaining: 4.85s\n",
      "70:\tlearn: 1827.4967070\ttotal: 1.91s\tremaining: 4.82s\n",
      "71:\tlearn: 1825.0518462\ttotal: 1.94s\tremaining: 4.79s\n",
      "72:\tlearn: 1824.1944284\ttotal: 1.96s\tremaining: 4.75s\n",
      "73:\tlearn: 1821.9167189\ttotal: 1.98s\tremaining: 4.72s\n",
      "74:\tlearn: 1819.6915715\ttotal: 2s\tremaining: 4.68s\n",
      "75:\tlearn: 1817.9486562\ttotal: 2.02s\tremaining: 4.64s\n",
      "76:\tlearn: 1817.2074565\ttotal: 2.05s\tremaining: 4.6s\n",
      "77:\tlearn: 1815.4691026\ttotal: 2.07s\tremaining: 4.57s\n",
      "78:\tlearn: 1813.7246374\ttotal: 2.13s\tremaining: 4.61s\n",
      "79:\tlearn: 1812.0448026\ttotal: 2.18s\tremaining: 4.63s\n",
      "80:\tlearn: 1811.0008471\ttotal: 2.22s\tremaining: 4.63s\n",
      "81:\tlearn: 1809.4238303\ttotal: 2.25s\tremaining: 4.6s\n",
      "82:\tlearn: 1808.5450721\ttotal: 2.27s\tremaining: 4.58s\n",
      "83:\tlearn: 1807.9577852\ttotal: 2.3s\tremaining: 4.54s\n",
      "84:\tlearn: 1806.9977472\ttotal: 2.33s\tremaining: 4.53s\n",
      "85:\tlearn: 1805.8795818\ttotal: 2.39s\tremaining: 4.56s\n",
      "86:\tlearn: 1804.6190390\ttotal: 2.43s\tremaining: 4.54s\n",
      "87:\tlearn: 1803.7055380\ttotal: 2.45s\tremaining: 4.51s\n",
      "88:\tlearn: 1802.5915478\ttotal: 2.48s\tremaining: 4.48s\n",
      "89:\tlearn: 1801.3900047\ttotal: 2.51s\tremaining: 4.46s\n",
      "90:\tlearn: 1800.1188566\ttotal: 2.56s\tremaining: 4.47s\n",
      "91:\tlearn: 1798.9286241\ttotal: 2.58s\tremaining: 4.43s\n",
      "92:\tlearn: 1797.7712484\ttotal: 2.61s\tremaining: 4.41s\n",
      "93:\tlearn: 1797.1255481\ttotal: 2.64s\tremaining: 4.38s\n",
      "94:\tlearn: 1796.6968895\ttotal: 2.67s\tremaining: 4.36s\n",
      "95:\tlearn: 1795.3511349\ttotal: 2.69s\tremaining: 4.32s\n",
      "96:\tlearn: 1794.8990813\ttotal: 2.73s\tremaining: 4.3s\n",
      "97:\tlearn: 1793.8246921\ttotal: 2.75s\tremaining: 4.26s\n",
      "98:\tlearn: 1792.7906800\ttotal: 2.77s\tremaining: 4.23s\n",
      "99:\tlearn: 1791.9242279\ttotal: 2.8s\tremaining: 4.2s\n",
      "100:\tlearn: 1790.1663446\ttotal: 2.82s\tremaining: 4.17s\n",
      "101:\tlearn: 1787.9940966\ttotal: 2.85s\tremaining: 4.13s\n",
      "102:\tlearn: 1786.8839690\ttotal: 2.87s\tremaining: 4.09s\n",
      "103:\tlearn: 1785.8490668\ttotal: 2.89s\tremaining: 4.06s\n",
      "104:\tlearn: 1785.3333991\ttotal: 2.91s\tremaining: 4.02s\n",
      "105:\tlearn: 1784.3698543\ttotal: 2.94s\tremaining: 3.99s\n",
      "106:\tlearn: 1783.9947907\ttotal: 2.96s\tremaining: 3.96s\n",
      "107:\tlearn: 1782.2424995\ttotal: 2.98s\tremaining: 3.92s\n",
      "108:\tlearn: 1781.6057718\ttotal: 3s\tremaining: 3.88s\n",
      "109:\tlearn: 1780.9094660\ttotal: 3.02s\tremaining: 3.85s\n",
      "110:\tlearn: 1780.1806784\ttotal: 3.04s\tremaining: 3.81s\n",
      "111:\tlearn: 1779.0379752\ttotal: 3.06s\tremaining: 3.78s\n",
      "112:\tlearn: 1778.5047508\ttotal: 3.09s\tremaining: 3.74s\n",
      "113:\tlearn: 1777.4266103\ttotal: 3.12s\tremaining: 3.72s\n",
      "114:\tlearn: 1776.5433153\ttotal: 3.13s\tremaining: 3.68s\n",
      "115:\tlearn: 1775.7104926\ttotal: 3.16s\tremaining: 3.65s\n",
      "116:\tlearn: 1774.7231165\ttotal: 3.18s\tremaining: 3.62s\n",
      "117:\tlearn: 1773.9541493\ttotal: 3.21s\tremaining: 3.59s\n",
      "118:\tlearn: 1773.4723287\ttotal: 3.23s\tremaining: 3.56s\n",
      "119:\tlearn: 1772.2402552\ttotal: 3.26s\tremaining: 3.53s\n",
      "120:\tlearn: 1771.4146277\ttotal: 3.28s\tremaining: 3.5s\n",
      "121:\tlearn: 1770.4305671\ttotal: 3.31s\tremaining: 3.47s\n",
      "122:\tlearn: 1769.6048680\ttotal: 3.33s\tremaining: 3.44s\n",
      "123:\tlearn: 1768.7694972\ttotal: 3.36s\tremaining: 3.42s\n",
      "124:\tlearn: 1767.8395757\ttotal: 3.39s\tremaining: 3.39s\n",
      "125:\tlearn: 1766.7312449\ttotal: 3.41s\tremaining: 3.36s\n",
      "126:\tlearn: 1766.2136319\ttotal: 3.44s\tremaining: 3.33s\n",
      "127:\tlearn: 1765.1376581\ttotal: 3.47s\tremaining: 3.31s\n",
      "128:\tlearn: 1764.3386289\ttotal: 3.49s\tremaining: 3.27s\n",
      "129:\tlearn: 1763.5645423\ttotal: 3.51s\tremaining: 3.24s\n",
      "130:\tlearn: 1762.6746209\ttotal: 3.54s\tremaining: 3.22s\n",
      "131:\tlearn: 1762.0596527\ttotal: 3.56s\tremaining: 3.19s\n",
      "132:\tlearn: 1761.4855044\ttotal: 3.59s\tremaining: 3.16s\n",
      "133:\tlearn: 1761.2111793\ttotal: 3.62s\tremaining: 3.13s\n",
      "134:\tlearn: 1760.1005750\ttotal: 3.64s\tremaining: 3.1s\n",
      "135:\tlearn: 1758.9124238\ttotal: 3.66s\tremaining: 3.07s\n",
      "136:\tlearn: 1758.4624048\ttotal: 3.69s\tremaining: 3.04s\n",
      "137:\tlearn: 1757.6509345\ttotal: 3.73s\tremaining: 3.02s\n",
      "138:\tlearn: 1757.3863310\ttotal: 3.76s\tremaining: 3s\n",
      "139:\tlearn: 1756.9760257\ttotal: 3.79s\tremaining: 2.98s\n",
      "140:\tlearn: 1755.9204307\ttotal: 3.81s\tremaining: 2.95s\n",
      "141:\tlearn: 1755.7247062\ttotal: 3.84s\tremaining: 2.92s\n",
      "142:\tlearn: 1754.8338955\ttotal: 3.87s\tremaining: 2.89s\n",
      "143:\tlearn: 1754.4621935\ttotal: 3.89s\tremaining: 2.86s\n",
      "144:\tlearn: 1754.0934250\ttotal: 3.91s\tremaining: 2.83s\n",
      "145:\tlearn: 1753.7568457\ttotal: 3.94s\tremaining: 2.81s\n",
      "146:\tlearn: 1752.6516922\ttotal: 3.96s\tremaining: 2.77s\n",
      "147:\tlearn: 1751.9978919\ttotal: 3.98s\tremaining: 2.75s\n",
      "148:\tlearn: 1751.2700261\ttotal: 4s\tremaining: 2.71s\n",
      "149:\tlearn: 1750.5479524\ttotal: 4.03s\tremaining: 2.69s\n",
      "150:\tlearn: 1749.9639363\ttotal: 4.06s\tremaining: 2.66s\n",
      "151:\tlearn: 1749.2103665\ttotal: 4.09s\tremaining: 2.63s\n",
      "152:\tlearn: 1748.6370838\ttotal: 4.12s\tremaining: 2.61s\n",
      "153:\tlearn: 1748.3091947\ttotal: 4.14s\tremaining: 2.58s\n",
      "154:\tlearn: 1747.5584068\ttotal: 4.16s\tremaining: 2.55s\n",
      "155:\tlearn: 1747.1461988\ttotal: 4.19s\tremaining: 2.52s\n",
      "156:\tlearn: 1746.8328490\ttotal: 4.21s\tremaining: 2.5s\n",
      "157:\tlearn: 1746.3655111\ttotal: 4.24s\tremaining: 2.47s\n",
      "158:\tlearn: 1745.6832011\ttotal: 4.29s\tremaining: 2.46s\n",
      "159:\tlearn: 1745.3060547\ttotal: 4.32s\tremaining: 2.43s\n",
      "160:\tlearn: 1745.0429341\ttotal: 4.34s\tremaining: 2.4s\n",
      "161:\tlearn: 1744.6294659\ttotal: 4.37s\tremaining: 2.37s\n",
      "162:\tlearn: 1743.8958287\ttotal: 4.39s\tremaining: 2.34s\n",
      "163:\tlearn: 1743.2096576\ttotal: 4.42s\tremaining: 2.32s\n",
      "164:\tlearn: 1742.6411089\ttotal: 4.44s\tremaining: 2.29s\n",
      "165:\tlearn: 1741.9254025\ttotal: 4.47s\tremaining: 2.26s\n",
      "166:\tlearn: 1741.7444772\ttotal: 4.49s\tremaining: 2.23s\n",
      "167:\tlearn: 1741.0093584\ttotal: 4.51s\tremaining: 2.2s\n",
      "168:\tlearn: 1740.5982987\ttotal: 4.54s\tremaining: 2.17s\n",
      "169:\tlearn: 1740.2541993\ttotal: 4.56s\tremaining: 2.15s\n",
      "170:\tlearn: 1739.9068408\ttotal: 4.58s\tremaining: 2.12s\n",
      "171:\tlearn: 1739.6177740\ttotal: 4.61s\tremaining: 2.09s\n",
      "172:\tlearn: 1738.7488748\ttotal: 4.64s\tremaining: 2.07s\n",
      "173:\tlearn: 1737.5788597\ttotal: 4.68s\tremaining: 2.04s\n",
      "174:\tlearn: 1737.4342683\ttotal: 4.71s\tremaining: 2.02s\n",
      "175:\tlearn: 1736.7967920\ttotal: 4.73s\tremaining: 1.99s\n",
      "176:\tlearn: 1736.3779101\ttotal: 4.75s\tremaining: 1.96s\n",
      "177:\tlearn: 1735.9801140\ttotal: 4.77s\tremaining: 1.93s\n",
      "178:\tlearn: 1735.2615111\ttotal: 4.8s\tremaining: 1.9s\n",
      "179:\tlearn: 1734.9484217\ttotal: 4.83s\tremaining: 1.88s\n",
      "180:\tlearn: 1734.3557526\ttotal: 4.86s\tremaining: 1.85s\n",
      "181:\tlearn: 1733.7197920\ttotal: 4.88s\tremaining: 1.82s\n",
      "182:\tlearn: 1732.7689821\ttotal: 4.91s\tremaining: 1.8s\n",
      "183:\tlearn: 1732.4364258\ttotal: 4.93s\tremaining: 1.77s\n",
      "184:\tlearn: 1731.8926188\ttotal: 4.97s\tremaining: 1.75s\n",
      "185:\tlearn: 1731.8767771\ttotal: 4.99s\tremaining: 1.72s\n",
      "186:\tlearn: 1731.8001398\ttotal: 5.01s\tremaining: 1.69s\n",
      "187:\tlearn: 1731.1357353\ttotal: 5.04s\tremaining: 1.66s\n",
      "188:\tlearn: 1731.1265822\ttotal: 5.06s\tremaining: 1.63s\n",
      "189:\tlearn: 1730.3745231\ttotal: 5.09s\tremaining: 1.61s\n",
      "190:\tlearn: 1729.6633802\ttotal: 5.11s\tremaining: 1.58s\n",
      "191:\tlearn: 1729.5833750\ttotal: 5.14s\tremaining: 1.55s\n",
      "192:\tlearn: 1729.3562773\ttotal: 5.16s\tremaining: 1.52s\n",
      "193:\tlearn: 1728.8759214\ttotal: 5.19s\tremaining: 1.5s\n",
      "194:\tlearn: 1728.2926029\ttotal: 5.22s\tremaining: 1.47s\n",
      "195:\tlearn: 1727.9865173\ttotal: 5.25s\tremaining: 1.45s\n",
      "196:\tlearn: 1727.7710933\ttotal: 5.28s\tremaining: 1.42s\n",
      "197:\tlearn: 1727.6384238\ttotal: 5.31s\tremaining: 1.4s\n",
      "198:\tlearn: 1726.6734589\ttotal: 5.34s\tremaining: 1.37s\n",
      "199:\tlearn: 1726.6436152\ttotal: 5.36s\tremaining: 1.34s\n",
      "200:\tlearn: 1725.9497166\ttotal: 5.4s\tremaining: 1.31s\n",
      "201:\tlearn: 1725.4649029\ttotal: 5.42s\tremaining: 1.29s\n",
      "202:\tlearn: 1725.0778387\ttotal: 5.45s\tremaining: 1.26s\n",
      "203:\tlearn: 1724.8464157\ttotal: 5.47s\tremaining: 1.23s\n",
      "204:\tlearn: 1724.5646862\ttotal: 5.51s\tremaining: 1.21s\n",
      "205:\tlearn: 1724.0964713\ttotal: 5.53s\tremaining: 1.18s\n",
      "206:\tlearn: 1723.6191598\ttotal: 5.55s\tremaining: 1.15s\n",
      "207:\tlearn: 1722.9822964\ttotal: 5.58s\tremaining: 1.13s\n",
      "208:\tlearn: 1722.1446856\ttotal: 5.61s\tremaining: 1.1s\n",
      "209:\tlearn: 1721.8161862\ttotal: 5.64s\tremaining: 1.07s\n",
      "210:\tlearn: 1721.1677539\ttotal: 5.67s\tremaining: 1.05s\n",
      "211:\tlearn: 1720.4713833\ttotal: 5.69s\tremaining: 1.02s\n",
      "212:\tlearn: 1720.3171677\ttotal: 5.71s\tremaining: 992ms\n",
      "213:\tlearn: 1720.1424180\ttotal: 5.75s\tremaining: 967ms\n",
      "214:\tlearn: 1719.6722849\ttotal: 5.77s\tremaining: 939ms\n",
      "215:\tlearn: 1719.1618098\ttotal: 5.79s\tremaining: 912ms\n",
      "216:\tlearn: 1718.1337441\ttotal: 5.82s\tremaining: 886ms\n",
      "217:\tlearn: 1717.8120598\ttotal: 5.85s\tremaining: 859ms\n",
      "218:\tlearn: 1717.4334758\ttotal: 5.88s\tremaining: 832ms\n",
      "219:\tlearn: 1716.6574896\ttotal: 5.91s\tremaining: 806ms\n",
      "220:\tlearn: 1715.6751670\ttotal: 5.93s\tremaining: 779ms\n",
      "221:\tlearn: 1714.8805064\ttotal: 5.98s\tremaining: 755ms\n",
      "222:\tlearn: 1714.3088565\ttotal: 6.01s\tremaining: 727ms\n",
      "223:\tlearn: 1713.8298235\ttotal: 6.03s\tremaining: 700ms\n",
      "224:\tlearn: 1713.3376389\ttotal: 6.05s\tremaining: 673ms\n",
      "225:\tlearn: 1712.9656848\ttotal: 6.08s\tremaining: 645ms\n",
      "226:\tlearn: 1711.6852071\ttotal: 6.1s\tremaining: 618ms\n",
      "227:\tlearn: 1711.2627893\ttotal: 6.12s\tremaining: 590ms\n",
      "228:\tlearn: 1710.9858711\ttotal: 6.15s\tremaining: 564ms\n",
      "229:\tlearn: 1710.5208235\ttotal: 6.18s\tremaining: 537ms\n",
      "230:\tlearn: 1710.0351551\ttotal: 6.21s\tremaining: 511ms\n",
      "231:\tlearn: 1709.9037810\ttotal: 6.24s\tremaining: 484ms\n",
      "232:\tlearn: 1709.5680945\ttotal: 6.27s\tremaining: 457ms\n",
      "233:\tlearn: 1709.5462191\ttotal: 6.29s\tremaining: 430ms\n",
      "234:\tlearn: 1709.4336578\ttotal: 6.31s\tremaining: 403ms\n",
      "235:\tlearn: 1708.6826576\ttotal: 6.34s\tremaining: 376ms\n",
      "236:\tlearn: 1708.3627298\ttotal: 6.37s\tremaining: 349ms\n",
      "237:\tlearn: 1708.0417599\ttotal: 6.4s\tremaining: 323ms\n",
      "238:\tlearn: 1707.7169091\ttotal: 6.43s\tremaining: 296ms\n",
      "239:\tlearn: 1707.4099637\ttotal: 6.46s\tremaining: 269ms\n",
      "240:\tlearn: 1707.0247630\ttotal: 6.49s\tremaining: 242ms\n",
      "241:\tlearn: 1706.6496885\ttotal: 6.52s\tremaining: 216ms\n",
      "242:\tlearn: 1706.2757386\ttotal: 6.55s\tremaining: 189ms\n",
      "243:\tlearn: 1705.6441078\ttotal: 6.58s\tremaining: 162ms\n",
      "244:\tlearn: 1705.5526576\ttotal: 6.62s\tremaining: 135ms\n",
      "245:\tlearn: 1705.1240589\ttotal: 6.65s\tremaining: 108ms\n",
      "246:\tlearn: 1704.9201382\ttotal: 6.68s\tremaining: 81.1ms\n",
      "247:\tlearn: 1704.4740258\ttotal: 6.7s\tremaining: 54ms\n",
      "248:\tlearn: 1704.2781269\ttotal: 6.73s\tremaining: 27ms\n",
      "249:\tlearn: 1703.4774421\ttotal: 6.76s\tremaining: 0us\n",
      "[CV] END .....................................iterations=250; total time=   7.0s\n",
      "Learning rate set to 0.284459\n",
      "0:\tlearn: 3741.7191898\ttotal: 35.3ms\tremaining: 8.79s\n",
      "1:\tlearn: 3218.3740618\ttotal: 76.1ms\tremaining: 9.44s\n",
      "2:\tlearn: 2873.6687724\ttotal: 118ms\tremaining: 9.68s\n",
      "3:\tlearn: 2623.9617115\ttotal: 145ms\tremaining: 8.9s\n",
      "4:\tlearn: 2453.5675629\ttotal: 172ms\tremaining: 8.43s\n",
      "5:\tlearn: 2339.2372348\ttotal: 199ms\tremaining: 8.09s\n",
      "6:\tlearn: 2257.0253847\ttotal: 225ms\tremaining: 7.81s\n",
      "7:\tlearn: 2186.6478821\ttotal: 247ms\tremaining: 7.49s\n",
      "8:\tlearn: 2148.9524394\ttotal: 267ms\tremaining: 7.14s\n",
      "9:\tlearn: 2119.8535331\ttotal: 287ms\tremaining: 6.89s\n",
      "10:\tlearn: 2094.9480016\ttotal: 315ms\tremaining: 6.85s\n",
      "11:\tlearn: 2071.3162955\ttotal: 345ms\tremaining: 6.85s\n",
      "12:\tlearn: 2054.2379529\ttotal: 370ms\tremaining: 6.74s\n",
      "13:\tlearn: 2043.3436000\ttotal: 392ms\tremaining: 6.61s\n",
      "14:\tlearn: 2032.2307979\ttotal: 420ms\tremaining: 6.58s\n",
      "15:\tlearn: 2023.0359533\ttotal: 445ms\tremaining: 6.51s\n",
      "16:\tlearn: 2011.6255991\ttotal: 464ms\tremaining: 6.36s\n",
      "17:\tlearn: 2004.9549531\ttotal: 489ms\tremaining: 6.31s\n",
      "18:\tlearn: 1997.8817066\ttotal: 515ms\tremaining: 6.26s\n",
      "19:\tlearn: 1991.5595075\ttotal: 548ms\tremaining: 6.3s\n",
      "20:\tlearn: 1985.5315346\ttotal: 576ms\tremaining: 6.28s\n",
      "21:\tlearn: 1977.8967899\ttotal: 617ms\tremaining: 6.4s\n",
      "22:\tlearn: 1973.5276091\ttotal: 645ms\tremaining: 6.37s\n",
      "23:\tlearn: 1968.0342314\ttotal: 664ms\tremaining: 6.26s\n",
      "24:\tlearn: 1962.2265849\ttotal: 687ms\tremaining: 6.18s\n",
      "25:\tlearn: 1954.3143124\ttotal: 710ms\tremaining: 6.11s\n",
      "26:\tlearn: 1948.4761126\ttotal: 738ms\tremaining: 6.1s\n",
      "27:\tlearn: 1944.0183258\ttotal: 768ms\tremaining: 6.09s\n",
      "28:\tlearn: 1938.2980477\ttotal: 785ms\tremaining: 5.98s\n",
      "29:\tlearn: 1935.4363575\ttotal: 819ms\tremaining: 6s\n",
      "30:\tlearn: 1931.5179336\ttotal: 844ms\tremaining: 5.96s\n",
      "31:\tlearn: 1926.9268750\ttotal: 873ms\tremaining: 5.94s\n",
      "32:\tlearn: 1922.1045867\ttotal: 902ms\tremaining: 5.93s\n",
      "33:\tlearn: 1919.5178050\ttotal: 926ms\tremaining: 5.88s\n",
      "34:\tlearn: 1916.8630409\ttotal: 956ms\tremaining: 5.87s\n",
      "35:\tlearn: 1914.8543417\ttotal: 983ms\tremaining: 5.84s\n",
      "36:\tlearn: 1911.1619346\ttotal: 1.01s\tremaining: 5.81s\n",
      "37:\tlearn: 1907.9883281\ttotal: 1.04s\tremaining: 5.81s\n",
      "38:\tlearn: 1905.5631184\ttotal: 1.07s\tremaining: 5.81s\n",
      "39:\tlearn: 1901.6255805\ttotal: 1.11s\tremaining: 5.81s\n",
      "40:\tlearn: 1898.7649485\ttotal: 1.14s\tremaining: 5.79s\n",
      "41:\tlearn: 1897.0762679\ttotal: 1.16s\tremaining: 5.75s\n",
      "42:\tlearn: 1894.8682093\ttotal: 1.19s\tremaining: 5.74s\n",
      "43:\tlearn: 1892.1060207\ttotal: 1.22s\tremaining: 5.72s\n",
      "44:\tlearn: 1889.8553021\ttotal: 1.25s\tremaining: 5.68s\n",
      "45:\tlearn: 1887.7359702\ttotal: 1.28s\tremaining: 5.67s\n",
      "46:\tlearn: 1885.6964186\ttotal: 1.31s\tremaining: 5.66s\n",
      "47:\tlearn: 1883.8994317\ttotal: 1.34s\tremaining: 5.63s\n",
      "48:\tlearn: 1881.0360436\ttotal: 1.36s\tremaining: 5.58s\n",
      "49:\tlearn: 1879.5349282\ttotal: 1.39s\tremaining: 5.54s\n",
      "50:\tlearn: 1875.3935860\ttotal: 1.41s\tremaining: 5.5s\n",
      "51:\tlearn: 1872.1608126\ttotal: 1.43s\tremaining: 5.46s\n",
      "52:\tlearn: 1870.4776997\ttotal: 1.46s\tremaining: 5.43s\n",
      "53:\tlearn: 1868.3864271\ttotal: 1.49s\tremaining: 5.41s\n",
      "54:\tlearn: 1865.6470852\ttotal: 1.51s\tremaining: 5.37s\n",
      "55:\tlearn: 1863.4374788\ttotal: 1.55s\tremaining: 5.37s\n",
      "56:\tlearn: 1861.3880814\ttotal: 1.58s\tremaining: 5.35s\n",
      "57:\tlearn: 1859.2049860\ttotal: 1.61s\tremaining: 5.35s\n",
      "58:\tlearn: 1857.1814353\ttotal: 1.65s\tremaining: 5.33s\n",
      "59:\tlearn: 1854.2902975\ttotal: 1.67s\tremaining: 5.28s\n",
      "60:\tlearn: 1851.3959510\ttotal: 1.69s\tremaining: 5.23s\n",
      "61:\tlearn: 1849.2595562\ttotal: 1.72s\tremaining: 5.21s\n",
      "62:\tlearn: 1848.6948834\ttotal: 1.75s\tremaining: 5.21s\n",
      "63:\tlearn: 1846.1776929\ttotal: 1.78s\tremaining: 5.17s\n",
      "64:\tlearn: 1844.2817699\ttotal: 1.8s\tremaining: 5.14s\n",
      "65:\tlearn: 1842.6935975\ttotal: 1.83s\tremaining: 5.12s\n",
      "66:\tlearn: 1839.8892915\ttotal: 1.86s\tremaining: 5.08s\n",
      "67:\tlearn: 1838.5758406\ttotal: 1.88s\tremaining: 5.04s\n",
      "68:\tlearn: 1835.5434250\ttotal: 1.91s\tremaining: 5.01s\n",
      "69:\tlearn: 1833.2280426\ttotal: 1.93s\tremaining: 4.97s\n",
      "70:\tlearn: 1832.3811850\ttotal: 1.96s\tremaining: 4.94s\n",
      "71:\tlearn: 1830.0871902\ttotal: 1.99s\tremaining: 4.91s\n",
      "72:\tlearn: 1827.9150407\ttotal: 2.02s\tremaining: 4.89s\n",
      "73:\tlearn: 1826.2705370\ttotal: 2.04s\tremaining: 4.84s\n",
      "74:\tlearn: 1824.5010233\ttotal: 2.09s\tremaining: 4.87s\n",
      "75:\tlearn: 1823.8162757\ttotal: 2.13s\tremaining: 4.87s\n",
      "76:\tlearn: 1822.6952427\ttotal: 2.15s\tremaining: 4.84s\n",
      "77:\tlearn: 1820.1003624\ttotal: 2.18s\tremaining: 4.8s\n",
      "78:\tlearn: 1818.5816035\ttotal: 2.2s\tremaining: 4.77s\n",
      "79:\tlearn: 1817.3587379\ttotal: 2.24s\tremaining: 4.75s\n",
      "80:\tlearn: 1816.3405835\ttotal: 2.27s\tremaining: 4.73s\n",
      "81:\tlearn: 1814.7944759\ttotal: 2.29s\tremaining: 4.7s\n",
      "82:\tlearn: 1814.0945019\ttotal: 2.33s\tremaining: 4.7s\n",
      "83:\tlearn: 1812.0201108\ttotal: 2.38s\tremaining: 4.7s\n",
      "84:\tlearn: 1811.4255204\ttotal: 2.45s\tremaining: 4.75s\n",
      "85:\tlearn: 1810.2180391\ttotal: 2.49s\tremaining: 4.76s\n",
      "86:\tlearn: 1808.0057577\ttotal: 2.56s\tremaining: 4.79s\n",
      "87:\tlearn: 1807.0207540\ttotal: 2.59s\tremaining: 4.76s\n",
      "88:\tlearn: 1805.6323793\ttotal: 2.61s\tremaining: 4.73s\n",
      "89:\tlearn: 1804.5952130\ttotal: 2.64s\tremaining: 4.7s\n",
      "90:\tlearn: 1803.6050238\ttotal: 2.67s\tremaining: 4.67s\n",
      "91:\tlearn: 1802.6354419\ttotal: 2.7s\tremaining: 4.63s\n",
      "92:\tlearn: 1801.2980941\ttotal: 2.72s\tremaining: 4.59s\n",
      "93:\tlearn: 1799.4393592\ttotal: 2.74s\tremaining: 4.55s\n",
      "94:\tlearn: 1798.3502379\ttotal: 2.77s\tremaining: 4.52s\n",
      "95:\tlearn: 1797.6720083\ttotal: 2.79s\tremaining: 4.48s\n",
      "96:\tlearn: 1796.7827077\ttotal: 2.82s\tremaining: 4.44s\n",
      "97:\tlearn: 1795.7937127\ttotal: 2.84s\tremaining: 4.41s\n",
      "98:\tlearn: 1794.8821288\ttotal: 2.87s\tremaining: 4.38s\n",
      "99:\tlearn: 1793.4675805\ttotal: 2.9s\tremaining: 4.34s\n",
      "100:\tlearn: 1792.6836866\ttotal: 2.93s\tremaining: 4.32s\n",
      "101:\tlearn: 1791.6583193\ttotal: 2.95s\tremaining: 4.29s\n",
      "102:\tlearn: 1790.7905496\ttotal: 3s\tremaining: 4.29s\n",
      "103:\tlearn: 1789.0183810\ttotal: 3.03s\tremaining: 4.26s\n",
      "104:\tlearn: 1788.5839491\ttotal: 3.06s\tremaining: 4.22s\n",
      "105:\tlearn: 1787.7807884\ttotal: 3.08s\tremaining: 4.18s\n",
      "106:\tlearn: 1787.2987402\ttotal: 3.1s\tremaining: 4.15s\n",
      "107:\tlearn: 1786.2675341\ttotal: 3.13s\tremaining: 4.11s\n",
      "108:\tlearn: 1785.3682303\ttotal: 3.15s\tremaining: 4.08s\n",
      "109:\tlearn: 1783.8638845\ttotal: 3.18s\tremaining: 4.05s\n",
      "110:\tlearn: 1783.1467780\ttotal: 3.21s\tremaining: 4.02s\n",
      "111:\tlearn: 1782.4293319\ttotal: 3.24s\tremaining: 3.99s\n",
      "112:\tlearn: 1780.6975752\ttotal: 3.26s\tremaining: 3.95s\n",
      "113:\tlearn: 1779.0213186\ttotal: 3.29s\tremaining: 3.92s\n",
      "114:\tlearn: 1778.1812073\ttotal: 3.32s\tremaining: 3.9s\n",
      "115:\tlearn: 1777.2225370\ttotal: 3.35s\tremaining: 3.87s\n",
      "116:\tlearn: 1775.8103953\ttotal: 3.38s\tremaining: 3.84s\n",
      "117:\tlearn: 1774.7403611\ttotal: 3.4s\tremaining: 3.8s\n",
      "118:\tlearn: 1773.1545768\ttotal: 3.44s\tremaining: 3.78s\n",
      "119:\tlearn: 1772.9460462\ttotal: 3.47s\tremaining: 3.76s\n",
      "120:\tlearn: 1772.0214651\ttotal: 3.5s\tremaining: 3.73s\n",
      "121:\tlearn: 1771.0237744\ttotal: 3.52s\tremaining: 3.69s\n",
      "122:\tlearn: 1770.5481964\ttotal: 3.55s\tremaining: 3.67s\n",
      "123:\tlearn: 1769.7499225\ttotal: 3.57s\tremaining: 3.63s\n",
      "124:\tlearn: 1768.9734597\ttotal: 3.6s\tremaining: 3.6s\n",
      "125:\tlearn: 1768.5856786\ttotal: 3.62s\tremaining: 3.56s\n",
      "126:\tlearn: 1768.0011177\ttotal: 3.69s\tremaining: 3.57s\n",
      "127:\tlearn: 1766.9998169\ttotal: 3.73s\tremaining: 3.55s\n",
      "128:\tlearn: 1765.7518885\ttotal: 3.75s\tremaining: 3.52s\n",
      "129:\tlearn: 1764.8710148\ttotal: 3.77s\tremaining: 3.48s\n",
      "130:\tlearn: 1763.9214311\ttotal: 3.8s\tremaining: 3.45s\n",
      "131:\tlearn: 1762.9925117\ttotal: 3.82s\tremaining: 3.42s\n",
      "132:\tlearn: 1762.2284485\ttotal: 3.85s\tremaining: 3.38s\n",
      "133:\tlearn: 1761.6832322\ttotal: 3.89s\tremaining: 3.37s\n",
      "134:\tlearn: 1760.9605913\ttotal: 3.93s\tremaining: 3.34s\n",
      "135:\tlearn: 1760.6111590\ttotal: 3.96s\tremaining: 3.32s\n",
      "136:\tlearn: 1759.6440153\ttotal: 3.98s\tremaining: 3.29s\n",
      "137:\tlearn: 1758.8490997\ttotal: 4.01s\tremaining: 3.25s\n",
      "138:\tlearn: 1758.6365133\ttotal: 4.03s\tremaining: 3.22s\n",
      "139:\tlearn: 1757.7362667\ttotal: 4.05s\tremaining: 3.19s\n",
      "140:\tlearn: 1757.1506767\ttotal: 4.08s\tremaining: 3.15s\n",
      "141:\tlearn: 1756.4764481\ttotal: 4.11s\tremaining: 3.13s\n",
      "142:\tlearn: 1755.9431798\ttotal: 4.15s\tremaining: 3.11s\n",
      "143:\tlearn: 1755.0065338\ttotal: 4.18s\tremaining: 3.08s\n",
      "144:\tlearn: 1754.0705850\ttotal: 4.21s\tremaining: 3.05s\n",
      "145:\tlearn: 1753.7536349\ttotal: 4.24s\tremaining: 3.02s\n",
      "146:\tlearn: 1753.0418809\ttotal: 4.27s\tremaining: 2.99s\n",
      "147:\tlearn: 1752.0569188\ttotal: 4.3s\tremaining: 2.97s\n",
      "148:\tlearn: 1751.2949569\ttotal: 4.34s\tremaining: 2.94s\n",
      "149:\tlearn: 1750.4984108\ttotal: 4.36s\tremaining: 2.91s\n",
      "150:\tlearn: 1749.4149712\ttotal: 4.38s\tremaining: 2.87s\n",
      "151:\tlearn: 1748.2781060\ttotal: 4.4s\tremaining: 2.84s\n",
      "152:\tlearn: 1747.5123194\ttotal: 4.43s\tremaining: 2.81s\n",
      "153:\tlearn: 1746.8474876\ttotal: 4.46s\tremaining: 2.78s\n",
      "154:\tlearn: 1746.4799331\ttotal: 4.48s\tremaining: 2.75s\n",
      "155:\tlearn: 1746.0863564\ttotal: 4.52s\tremaining: 2.72s\n",
      "156:\tlearn: 1745.7489499\ttotal: 4.54s\tremaining: 2.69s\n",
      "157:\tlearn: 1745.0970400\ttotal: 4.57s\tremaining: 2.66s\n",
      "158:\tlearn: 1744.6541473\ttotal: 4.59s\tremaining: 2.63s\n",
      "159:\tlearn: 1744.0169289\ttotal: 4.61s\tremaining: 2.59s\n",
      "160:\tlearn: 1743.8410198\ttotal: 4.66s\tremaining: 2.58s\n",
      "161:\tlearn: 1742.8645727\ttotal: 4.69s\tremaining: 2.54s\n",
      "162:\tlearn: 1742.4722111\ttotal: 4.72s\tremaining: 2.52s\n",
      "163:\tlearn: 1742.0132984\ttotal: 4.75s\tremaining: 2.49s\n",
      "164:\tlearn: 1741.5907157\ttotal: 4.77s\tremaining: 2.46s\n",
      "165:\tlearn: 1740.8058854\ttotal: 4.79s\tremaining: 2.42s\n",
      "166:\tlearn: 1740.3103849\ttotal: 4.82s\tremaining: 2.4s\n",
      "167:\tlearn: 1739.1744007\ttotal: 4.85s\tremaining: 2.37s\n",
      "168:\tlearn: 1738.7984134\ttotal: 4.88s\tremaining: 2.34s\n",
      "169:\tlearn: 1738.0672618\ttotal: 4.9s\tremaining: 2.31s\n",
      "170:\tlearn: 1737.5736927\ttotal: 4.93s\tremaining: 2.28s\n",
      "171:\tlearn: 1737.3986464\ttotal: 4.96s\tremaining: 2.25s\n",
      "172:\tlearn: 1736.6751104\ttotal: 5.02s\tremaining: 2.23s\n",
      "173:\tlearn: 1736.1140469\ttotal: 5.05s\tremaining: 2.21s\n",
      "174:\tlearn: 1735.2203495\ttotal: 5.09s\tremaining: 2.18s\n",
      "175:\tlearn: 1734.4229961\ttotal: 5.13s\tremaining: 2.15s\n",
      "176:\tlearn: 1733.9128020\ttotal: 5.15s\tremaining: 2.12s\n",
      "177:\tlearn: 1733.1514797\ttotal: 5.18s\tremaining: 2.09s\n",
      "178:\tlearn: 1733.1002292\ttotal: 5.21s\tremaining: 2.07s\n",
      "179:\tlearn: 1731.8190630\ttotal: 5.24s\tremaining: 2.04s\n",
      "180:\tlearn: 1731.1207154\ttotal: 5.26s\tremaining: 2.01s\n",
      "181:\tlearn: 1730.5936877\ttotal: 5.29s\tremaining: 1.98s\n",
      "182:\tlearn: 1729.9851715\ttotal: 5.31s\tremaining: 1.94s\n",
      "183:\tlearn: 1729.4336818\ttotal: 5.33s\tremaining: 1.91s\n",
      "184:\tlearn: 1728.9606756\ttotal: 5.36s\tremaining: 1.88s\n",
      "185:\tlearn: 1728.5226412\ttotal: 5.38s\tremaining: 1.85s\n",
      "186:\tlearn: 1728.1465848\ttotal: 5.4s\tremaining: 1.82s\n",
      "187:\tlearn: 1727.7975650\ttotal: 5.43s\tremaining: 1.79s\n",
      "188:\tlearn: 1727.3509110\ttotal: 5.45s\tremaining: 1.76s\n",
      "189:\tlearn: 1726.8029108\ttotal: 5.48s\tremaining: 1.73s\n",
      "190:\tlearn: 1726.1612261\ttotal: 5.51s\tremaining: 1.7s\n",
      "191:\tlearn: 1725.4140513\ttotal: 5.53s\tremaining: 1.67s\n",
      "192:\tlearn: 1725.1602304\ttotal: 5.57s\tremaining: 1.64s\n",
      "193:\tlearn: 1724.2073013\ttotal: 5.61s\tremaining: 1.62s\n",
      "194:\tlearn: 1723.4873448\ttotal: 5.64s\tremaining: 1.59s\n",
      "195:\tlearn: 1722.7899308\ttotal: 5.68s\tremaining: 1.56s\n",
      "196:\tlearn: 1722.3147328\ttotal: 5.71s\tremaining: 1.53s\n",
      "197:\tlearn: 1721.7481042\ttotal: 5.74s\tremaining: 1.51s\n",
      "198:\tlearn: 1721.5755553\ttotal: 5.76s\tremaining: 1.48s\n",
      "199:\tlearn: 1720.7799000\ttotal: 5.79s\tremaining: 1.45s\n",
      "200:\tlearn: 1719.9504896\ttotal: 5.83s\tremaining: 1.42s\n",
      "201:\tlearn: 1719.2466807\ttotal: 5.86s\tremaining: 1.39s\n",
      "202:\tlearn: 1718.7633881\ttotal: 5.89s\tremaining: 1.36s\n",
      "203:\tlearn: 1718.2556306\ttotal: 5.93s\tremaining: 1.34s\n",
      "204:\tlearn: 1717.8748129\ttotal: 5.96s\tremaining: 1.31s\n",
      "205:\tlearn: 1716.8499315\ttotal: 6s\tremaining: 1.28s\n",
      "206:\tlearn: 1715.9738593\ttotal: 6.03s\tremaining: 1.25s\n",
      "207:\tlearn: 1715.6496245\ttotal: 6.07s\tremaining: 1.23s\n",
      "208:\tlearn: 1715.0525034\ttotal: 6.1s\tremaining: 1.2s\n",
      "209:\tlearn: 1714.6683753\ttotal: 6.14s\tremaining: 1.17s\n",
      "210:\tlearn: 1714.3280706\ttotal: 6.17s\tremaining: 1.14s\n",
      "211:\tlearn: 1714.0910567\ttotal: 6.19s\tremaining: 1.11s\n",
      "212:\tlearn: 1713.5134422\ttotal: 6.22s\tremaining: 1.08s\n",
      "213:\tlearn: 1712.4995720\ttotal: 6.25s\tremaining: 1.05s\n",
      "214:\tlearn: 1712.1809374\ttotal: 6.3s\tremaining: 1.03s\n",
      "215:\tlearn: 1711.9688711\ttotal: 6.33s\tremaining: 996ms\n",
      "216:\tlearn: 1711.9418082\ttotal: 6.35s\tremaining: 966ms\n",
      "217:\tlearn: 1711.6162995\ttotal: 6.38s\tremaining: 936ms\n",
      "218:\tlearn: 1711.2043769\ttotal: 6.41s\tremaining: 907ms\n",
      "219:\tlearn: 1710.8237325\ttotal: 6.42s\tremaining: 876ms\n",
      "220:\tlearn: 1710.0345317\ttotal: 6.45s\tremaining: 847ms\n",
      "221:\tlearn: 1709.4496085\ttotal: 6.48s\tremaining: 817ms\n",
      "222:\tlearn: 1708.7980082\ttotal: 6.52s\tremaining: 789ms\n",
      "223:\tlearn: 1708.3304348\ttotal: 6.55s\tremaining: 761ms\n",
      "224:\tlearn: 1707.8864741\ttotal: 6.59s\tremaining: 732ms\n",
      "225:\tlearn: 1707.3669358\ttotal: 6.64s\tremaining: 705ms\n",
      "226:\tlearn: 1706.8646254\ttotal: 6.69s\tremaining: 678ms\n",
      "227:\tlearn: 1706.2158102\ttotal: 6.73s\tremaining: 650ms\n",
      "228:\tlearn: 1705.5938512\ttotal: 6.76s\tremaining: 620ms\n",
      "229:\tlearn: 1705.2576058\ttotal: 6.79s\tremaining: 591ms\n",
      "230:\tlearn: 1705.0350029\ttotal: 6.82s\tremaining: 561ms\n",
      "231:\tlearn: 1704.8350177\ttotal: 6.85s\tremaining: 531ms\n",
      "232:\tlearn: 1704.5411155\ttotal: 6.88s\tremaining: 502ms\n",
      "233:\tlearn: 1704.2494204\ttotal: 6.92s\tremaining: 473ms\n",
      "234:\tlearn: 1703.8063347\ttotal: 6.95s\tremaining: 444ms\n",
      "235:\tlearn: 1703.7729326\ttotal: 7.03s\tremaining: 417ms\n",
      "236:\tlearn: 1703.3945641\ttotal: 7.07s\tremaining: 388ms\n",
      "237:\tlearn: 1703.1018014\ttotal: 7.13s\tremaining: 360ms\n",
      "238:\tlearn: 1702.6388438\ttotal: 7.16s\tremaining: 329ms\n",
      "239:\tlearn: 1702.4970615\ttotal: 7.18s\tremaining: 299ms\n",
      "240:\tlearn: 1701.7043404\ttotal: 7.21s\tremaining: 269ms\n",
      "241:\tlearn: 1701.2592459\ttotal: 7.24s\tremaining: 239ms\n",
      "242:\tlearn: 1700.9423485\ttotal: 7.27s\tremaining: 209ms\n",
      "243:\tlearn: 1700.3126810\ttotal: 7.3s\tremaining: 179ms\n",
      "244:\tlearn: 1700.0359277\ttotal: 7.33s\tremaining: 150ms\n",
      "245:\tlearn: 1699.6570720\ttotal: 7.37s\tremaining: 120ms\n",
      "246:\tlearn: 1699.1460582\ttotal: 7.41s\tremaining: 90ms\n",
      "247:\tlearn: 1698.5589160\ttotal: 7.43s\tremaining: 60ms\n",
      "248:\tlearn: 1697.9665717\ttotal: 7.47s\tremaining: 30ms\n",
      "249:\tlearn: 1697.5596974\ttotal: 7.53s\tremaining: 0us\n",
      "[CV] END .....................................iterations=250; total time=   7.8s\n",
      "Learning rate set to 0.284459\n",
      "0:\tlearn: 3732.7273648\ttotal: 28.8ms\tremaining: 7.16s\n",
      "1:\tlearn: 3211.7461661\ttotal: 56.7ms\tremaining: 7.03s\n",
      "2:\tlearn: 2867.1983111\ttotal: 81.5ms\tremaining: 6.71s\n",
      "3:\tlearn: 2614.9604331\ttotal: 107ms\tremaining: 6.6s\n",
      "4:\tlearn: 2456.7884585\ttotal: 130ms\tremaining: 6.38s\n",
      "5:\tlearn: 2341.3335339\ttotal: 159ms\tremaining: 6.45s\n",
      "6:\tlearn: 2257.7663415\ttotal: 180ms\tremaining: 6.23s\n",
      "7:\tlearn: 2193.0998803\ttotal: 212ms\tremaining: 6.43s\n",
      "8:\tlearn: 2150.1641063\ttotal: 241ms\tremaining: 6.45s\n",
      "9:\tlearn: 2118.9798089\ttotal: 266ms\tremaining: 6.38s\n",
      "10:\tlearn: 2097.6534079\ttotal: 290ms\tremaining: 6.29s\n",
      "11:\tlearn: 2068.4526594\ttotal: 315ms\tremaining: 6.24s\n",
      "12:\tlearn: 2054.8756473\ttotal: 352ms\tremaining: 6.41s\n",
      "13:\tlearn: 2039.7033797\ttotal: 379ms\tremaining: 6.39s\n",
      "14:\tlearn: 2030.3217923\ttotal: 404ms\tremaining: 6.32s\n",
      "15:\tlearn: 2021.1437387\ttotal: 426ms\tremaining: 6.23s\n",
      "16:\tlearn: 2013.1195437\ttotal: 449ms\tremaining: 6.15s\n",
      "17:\tlearn: 2007.7924637\ttotal: 477ms\tremaining: 6.15s\n",
      "18:\tlearn: 1999.4302376\ttotal: 498ms\tremaining: 6.05s\n",
      "19:\tlearn: 1989.5041639\ttotal: 515ms\tremaining: 5.92s\n",
      "20:\tlearn: 1982.8539775\ttotal: 551ms\tremaining: 6.01s\n",
      "21:\tlearn: 1976.0819863\ttotal: 573ms\tremaining: 5.94s\n",
      "22:\tlearn: 1967.2574241\ttotal: 601ms\tremaining: 5.93s\n",
      "23:\tlearn: 1963.4478591\ttotal: 631ms\tremaining: 5.94s\n",
      "24:\tlearn: 1959.6185635\ttotal: 673ms\tremaining: 6.06s\n",
      "25:\tlearn: 1956.1233301\ttotal: 698ms\tremaining: 6.01s\n",
      "26:\tlearn: 1952.0907102\ttotal: 723ms\tremaining: 5.97s\n",
      "27:\tlearn: 1950.1358892\ttotal: 753ms\tremaining: 5.97s\n",
      "28:\tlearn: 1946.5715488\ttotal: 780ms\tremaining: 5.94s\n",
      "29:\tlearn: 1941.8565321\ttotal: 802ms\tremaining: 5.88s\n",
      "30:\tlearn: 1936.7571769\ttotal: 827ms\tremaining: 5.84s\n",
      "31:\tlearn: 1932.5413146\ttotal: 860ms\tremaining: 5.86s\n",
      "32:\tlearn: 1928.6668146\ttotal: 883ms\tremaining: 5.8s\n",
      "33:\tlearn: 1924.7671609\ttotal: 912ms\tremaining: 5.79s\n",
      "34:\tlearn: 1921.6073187\ttotal: 938ms\tremaining: 5.76s\n",
      "35:\tlearn: 1917.1669817\ttotal: 969ms\tremaining: 5.76s\n",
      "36:\tlearn: 1915.7994799\ttotal: 998ms\tremaining: 5.75s\n",
      "37:\tlearn: 1914.6256091\ttotal: 1.02s\tremaining: 5.71s\n",
      "38:\tlearn: 1911.6409034\ttotal: 1.04s\tremaining: 5.65s\n",
      "39:\tlearn: 1907.2516594\ttotal: 1.06s\tremaining: 5.57s\n",
      "40:\tlearn: 1904.5054183\ttotal: 1.1s\tremaining: 5.59s\n",
      "41:\tlearn: 1901.9896479\ttotal: 1.13s\tremaining: 5.58s\n",
      "42:\tlearn: 1898.7646439\ttotal: 1.15s\tremaining: 5.51s\n",
      "43:\tlearn: 1895.9989827\ttotal: 1.17s\tremaining: 5.49s\n",
      "44:\tlearn: 1894.5798759\ttotal: 1.23s\tremaining: 5.62s\n",
      "45:\tlearn: 1890.3725185\ttotal: 1.25s\tremaining: 5.56s\n",
      "46:\tlearn: 1886.8551197\ttotal: 1.28s\tremaining: 5.52s\n",
      "47:\tlearn: 1885.1004458\ttotal: 1.3s\tremaining: 5.48s\n",
      "48:\tlearn: 1881.3397253\ttotal: 1.33s\tremaining: 5.46s\n",
      "49:\tlearn: 1879.0272220\ttotal: 1.36s\tremaining: 5.43s\n",
      "50:\tlearn: 1875.6504135\ttotal: 1.38s\tremaining: 5.38s\n",
      "51:\tlearn: 1873.7295375\ttotal: 1.41s\tremaining: 5.35s\n",
      "52:\tlearn: 1871.4278215\ttotal: 1.44s\tremaining: 5.34s\n",
      "53:\tlearn: 1870.1397326\ttotal: 1.46s\tremaining: 5.3s\n",
      "54:\tlearn: 1866.4981035\ttotal: 1.48s\tremaining: 5.25s\n",
      "55:\tlearn: 1864.8189083\ttotal: 1.5s\tremaining: 5.21s\n",
      "56:\tlearn: 1863.6715990\ttotal: 1.53s\tremaining: 5.18s\n",
      "57:\tlearn: 1862.5184053\ttotal: 1.55s\tremaining: 5.14s\n",
      "58:\tlearn: 1860.6496965\ttotal: 1.58s\tremaining: 5.1s\n",
      "59:\tlearn: 1858.7931907\ttotal: 1.6s\tremaining: 5.07s\n",
      "60:\tlearn: 1856.2442035\ttotal: 1.63s\tremaining: 5.04s\n",
      "61:\tlearn: 1854.4034631\ttotal: 1.65s\tremaining: 5s\n",
      "62:\tlearn: 1852.7125281\ttotal: 1.68s\tremaining: 4.98s\n",
      "63:\tlearn: 1848.9468556\ttotal: 1.7s\tremaining: 4.94s\n",
      "64:\tlearn: 1846.1468393\ttotal: 1.73s\tremaining: 4.91s\n",
      "65:\tlearn: 1844.4432578\ttotal: 1.78s\tremaining: 4.97s\n",
      "66:\tlearn: 1841.3222994\ttotal: 1.81s\tremaining: 4.96s\n",
      "67:\tlearn: 1839.6022393\ttotal: 1.85s\tremaining: 4.95s\n",
      "68:\tlearn: 1837.8563843\ttotal: 1.88s\tremaining: 4.92s\n",
      "69:\tlearn: 1836.7218160\ttotal: 1.9s\tremaining: 4.89s\n",
      "70:\tlearn: 1835.3767098\ttotal: 1.93s\tremaining: 4.86s\n",
      "71:\tlearn: 1833.6762776\ttotal: 1.96s\tremaining: 4.84s\n",
      "72:\tlearn: 1832.3311337\ttotal: 1.99s\tremaining: 4.82s\n",
      "73:\tlearn: 1831.1421862\ttotal: 2.01s\tremaining: 4.79s\n",
      "74:\tlearn: 1830.3841110\ttotal: 2.05s\tremaining: 4.78s\n",
      "75:\tlearn: 1829.1596674\ttotal: 2.08s\tremaining: 4.75s\n",
      "76:\tlearn: 1828.2354763\ttotal: 2.1s\tremaining: 4.72s\n",
      "77:\tlearn: 1826.5818758\ttotal: 2.13s\tremaining: 4.7s\n",
      "78:\tlearn: 1825.2636951\ttotal: 2.16s\tremaining: 4.68s\n",
      "79:\tlearn: 1824.3422965\ttotal: 2.19s\tremaining: 4.65s\n",
      "80:\tlearn: 1823.1066912\ttotal: 2.21s\tremaining: 4.61s\n",
      "81:\tlearn: 1820.6792116\ttotal: 2.23s\tremaining: 4.57s\n",
      "82:\tlearn: 1819.7828540\ttotal: 2.26s\tremaining: 4.54s\n",
      "83:\tlearn: 1818.4387724\ttotal: 2.27s\tremaining: 4.49s\n",
      "84:\tlearn: 1816.1638997\ttotal: 2.29s\tremaining: 4.45s\n",
      "85:\tlearn: 1815.1935174\ttotal: 2.32s\tremaining: 4.42s\n",
      "86:\tlearn: 1813.4758267\ttotal: 2.34s\tremaining: 4.39s\n",
      "87:\tlearn: 1812.5581506\ttotal: 2.37s\tremaining: 4.36s\n",
      "88:\tlearn: 1811.1057792\ttotal: 2.39s\tremaining: 4.32s\n",
      "89:\tlearn: 1810.2114293\ttotal: 2.41s\tremaining: 4.29s\n",
      "90:\tlearn: 1808.6954309\ttotal: 2.44s\tremaining: 4.26s\n",
      "91:\tlearn: 1807.7329347\ttotal: 2.46s\tremaining: 4.23s\n",
      "92:\tlearn: 1806.3835402\ttotal: 2.52s\tremaining: 4.26s\n",
      "93:\tlearn: 1805.6751304\ttotal: 2.55s\tremaining: 4.24s\n",
      "94:\tlearn: 1804.0724104\ttotal: 2.6s\tremaining: 4.24s\n",
      "95:\tlearn: 1802.2723079\ttotal: 2.63s\tremaining: 4.21s\n",
      "96:\tlearn: 1801.2347193\ttotal: 2.65s\tremaining: 4.19s\n",
      "97:\tlearn: 1799.7658831\ttotal: 2.68s\tremaining: 4.16s\n",
      "98:\tlearn: 1799.3059447\ttotal: 2.71s\tremaining: 4.13s\n",
      "99:\tlearn: 1798.6463322\ttotal: 2.74s\tremaining: 4.11s\n",
      "100:\tlearn: 1798.0979337\ttotal: 2.77s\tremaining: 4.08s\n",
      "101:\tlearn: 1797.5961249\ttotal: 2.8s\tremaining: 4.06s\n",
      "102:\tlearn: 1796.5116423\ttotal: 2.82s\tremaining: 4.02s\n",
      "103:\tlearn: 1795.0353410\ttotal: 2.84s\tremaining: 3.99s\n",
      "104:\tlearn: 1793.7563895\ttotal: 2.87s\tremaining: 3.96s\n",
      "105:\tlearn: 1792.2824507\ttotal: 2.89s\tremaining: 3.93s\n",
      "106:\tlearn: 1791.2100519\ttotal: 2.92s\tremaining: 3.91s\n",
      "107:\tlearn: 1790.4951205\ttotal: 2.95s\tremaining: 3.88s\n",
      "108:\tlearn: 1789.1451058\ttotal: 2.98s\tremaining: 3.85s\n",
      "109:\tlearn: 1788.0800251\ttotal: 3s\tremaining: 3.82s\n",
      "110:\tlearn: 1786.4015357\ttotal: 3.02s\tremaining: 3.79s\n",
      "111:\tlearn: 1785.9472991\ttotal: 3.05s\tremaining: 3.75s\n",
      "112:\tlearn: 1785.5212246\ttotal: 3.08s\tremaining: 3.73s\n",
      "113:\tlearn: 1784.6761576\ttotal: 3.1s\tremaining: 3.7s\n",
      "114:\tlearn: 1783.7592296\ttotal: 3.13s\tremaining: 3.67s\n",
      "115:\tlearn: 1782.4822712\ttotal: 3.15s\tremaining: 3.64s\n",
      "116:\tlearn: 1781.7703090\ttotal: 3.17s\tremaining: 3.61s\n",
      "117:\tlearn: 1780.3961405\ttotal: 3.19s\tremaining: 3.57s\n",
      "118:\tlearn: 1779.8917171\ttotal: 3.22s\tremaining: 3.54s\n",
      "119:\tlearn: 1778.3839073\ttotal: 3.24s\tremaining: 3.51s\n",
      "120:\tlearn: 1777.9517364\ttotal: 3.26s\tremaining: 3.48s\n",
      "121:\tlearn: 1777.1289402\ttotal: 3.29s\tremaining: 3.45s\n",
      "122:\tlearn: 1776.0050807\ttotal: 3.32s\tremaining: 3.42s\n",
      "123:\tlearn: 1774.9624942\ttotal: 3.34s\tremaining: 3.39s\n",
      "124:\tlearn: 1774.2119270\ttotal: 3.36s\tremaining: 3.36s\n",
      "125:\tlearn: 1773.5654704\ttotal: 3.39s\tremaining: 3.33s\n",
      "126:\tlearn: 1772.2842779\ttotal: 3.42s\tremaining: 3.31s\n",
      "127:\tlearn: 1771.8645317\ttotal: 3.44s\tremaining: 3.28s\n",
      "128:\tlearn: 1771.4365479\ttotal: 3.47s\tremaining: 3.26s\n",
      "129:\tlearn: 1770.9149659\ttotal: 3.5s\tremaining: 3.23s\n",
      "130:\tlearn: 1770.1552128\ttotal: 3.52s\tremaining: 3.2s\n",
      "131:\tlearn: 1769.7817020\ttotal: 3.56s\tremaining: 3.18s\n",
      "132:\tlearn: 1768.6963132\ttotal: 3.58s\tremaining: 3.15s\n",
      "133:\tlearn: 1767.8505580\ttotal: 3.6s\tremaining: 3.12s\n",
      "134:\tlearn: 1766.8897028\ttotal: 3.63s\tremaining: 3.09s\n",
      "135:\tlearn: 1766.1488333\ttotal: 3.66s\tremaining: 3.07s\n",
      "136:\tlearn: 1765.3840270\ttotal: 3.69s\tremaining: 3.04s\n",
      "137:\tlearn: 1765.1894110\ttotal: 3.71s\tremaining: 3.01s\n",
      "138:\tlearn: 1764.3953174\ttotal: 3.74s\tremaining: 2.99s\n",
      "139:\tlearn: 1763.3275594\ttotal: 3.81s\tremaining: 2.99s\n",
      "140:\tlearn: 1761.8463404\ttotal: 3.83s\tremaining: 2.96s\n",
      "141:\tlearn: 1761.3000879\ttotal: 3.87s\tremaining: 2.94s\n",
      "142:\tlearn: 1760.6938152\ttotal: 3.9s\tremaining: 2.92s\n",
      "143:\tlearn: 1759.9201924\ttotal: 3.92s\tremaining: 2.89s\n",
      "144:\tlearn: 1758.6272620\ttotal: 3.95s\tremaining: 2.86s\n",
      "145:\tlearn: 1757.6233118\ttotal: 3.97s\tremaining: 2.83s\n",
      "146:\tlearn: 1756.9259003\ttotal: 3.99s\tremaining: 2.8s\n",
      "147:\tlearn: 1756.4824799\ttotal: 4.02s\tremaining: 2.77s\n",
      "148:\tlearn: 1755.9923025\ttotal: 4.05s\tremaining: 2.75s\n",
      "149:\tlearn: 1755.8039134\ttotal: 4.08s\tremaining: 2.72s\n",
      "150:\tlearn: 1754.8986509\ttotal: 4.1s\tremaining: 2.69s\n",
      "151:\tlearn: 1753.9935698\ttotal: 4.13s\tremaining: 2.66s\n",
      "152:\tlearn: 1753.7815769\ttotal: 4.16s\tremaining: 2.63s\n",
      "153:\tlearn: 1753.6458082\ttotal: 4.18s\tremaining: 2.61s\n",
      "154:\tlearn: 1753.2374683\ttotal: 4.21s\tremaining: 2.58s\n",
      "155:\tlearn: 1752.8752538\ttotal: 4.24s\tremaining: 2.55s\n",
      "156:\tlearn: 1752.2876621\ttotal: 4.26s\tremaining: 2.53s\n",
      "157:\tlearn: 1751.8753320\ttotal: 4.29s\tremaining: 2.5s\n",
      "158:\tlearn: 1751.4323376\ttotal: 4.32s\tremaining: 2.47s\n",
      "159:\tlearn: 1750.9026879\ttotal: 4.34s\tremaining: 2.44s\n",
      "160:\tlearn: 1749.9415764\ttotal: 4.36s\tremaining: 2.41s\n",
      "161:\tlearn: 1749.3156894\ttotal: 4.39s\tremaining: 2.38s\n",
      "162:\tlearn: 1748.2264636\ttotal: 4.42s\tremaining: 2.36s\n",
      "163:\tlearn: 1747.9402266\ttotal: 4.44s\tremaining: 2.33s\n",
      "164:\tlearn: 1747.6478930\ttotal: 4.47s\tremaining: 2.3s\n",
      "165:\tlearn: 1747.0825603\ttotal: 4.5s\tremaining: 2.27s\n",
      "166:\tlearn: 1746.4685105\ttotal: 4.52s\tremaining: 2.25s\n",
      "167:\tlearn: 1746.1573326\ttotal: 4.55s\tremaining: 2.22s\n",
      "168:\tlearn: 1745.2857427\ttotal: 4.57s\tremaining: 2.19s\n",
      "169:\tlearn: 1745.0531755\ttotal: 4.59s\tremaining: 2.16s\n",
      "170:\tlearn: 1744.4521697\ttotal: 4.62s\tremaining: 2.14s\n",
      "171:\tlearn: 1743.8976952\ttotal: 4.65s\tremaining: 2.11s\n",
      "172:\tlearn: 1742.9196030\ttotal: 4.68s\tremaining: 2.08s\n",
      "173:\tlearn: 1742.6362410\ttotal: 4.7s\tremaining: 2.05s\n",
      "174:\tlearn: 1741.4611873\ttotal: 4.72s\tremaining: 2.02s\n",
      "175:\tlearn: 1740.6170760\ttotal: 4.74s\tremaining: 1.99s\n",
      "176:\tlearn: 1740.2170578\ttotal: 4.76s\tremaining: 1.97s\n",
      "177:\tlearn: 1739.6761705\ttotal: 4.82s\tremaining: 1.95s\n",
      "178:\tlearn: 1738.9371280\ttotal: 4.86s\tremaining: 1.93s\n",
      "179:\tlearn: 1738.2858009\ttotal: 4.88s\tremaining: 1.9s\n",
      "180:\tlearn: 1737.8867384\ttotal: 4.91s\tremaining: 1.87s\n",
      "181:\tlearn: 1737.5576440\ttotal: 4.92s\tremaining: 1.84s\n",
      "182:\tlearn: 1737.3764101\ttotal: 4.96s\tremaining: 1.81s\n",
      "183:\tlearn: 1736.8259405\ttotal: 4.99s\tremaining: 1.79s\n",
      "184:\tlearn: 1736.2304215\ttotal: 5.01s\tremaining: 1.76s\n",
      "185:\tlearn: 1736.1092664\ttotal: 5.04s\tremaining: 1.74s\n",
      "186:\tlearn: 1735.1629944\ttotal: 5.07s\tremaining: 1.71s\n",
      "187:\tlearn: 1734.6411626\ttotal: 5.1s\tremaining: 1.68s\n",
      "188:\tlearn: 1733.9462347\ttotal: 5.13s\tremaining: 1.66s\n",
      "189:\tlearn: 1733.5384346\ttotal: 5.15s\tremaining: 1.63s\n",
      "190:\tlearn: 1732.8169684\ttotal: 5.17s\tremaining: 1.6s\n",
      "191:\tlearn: 1732.0761751\ttotal: 5.2s\tremaining: 1.57s\n",
      "192:\tlearn: 1731.2908966\ttotal: 5.22s\tremaining: 1.54s\n",
      "193:\tlearn: 1731.0246571\ttotal: 5.25s\tremaining: 1.51s\n",
      "194:\tlearn: 1729.6982755\ttotal: 5.28s\tremaining: 1.49s\n",
      "195:\tlearn: 1729.0401865\ttotal: 5.3s\tremaining: 1.46s\n",
      "196:\tlearn: 1728.9032125\ttotal: 5.33s\tremaining: 1.44s\n",
      "197:\tlearn: 1728.6014373\ttotal: 5.36s\tremaining: 1.41s\n",
      "198:\tlearn: 1728.0888842\ttotal: 5.39s\tremaining: 1.38s\n",
      "199:\tlearn: 1727.3333394\ttotal: 5.41s\tremaining: 1.35s\n",
      "200:\tlearn: 1726.3423930\ttotal: 5.44s\tremaining: 1.32s\n",
      "201:\tlearn: 1726.2330614\ttotal: 5.47s\tremaining: 1.3s\n",
      "202:\tlearn: 1725.7407049\ttotal: 5.5s\tremaining: 1.27s\n",
      "203:\tlearn: 1725.1974515\ttotal: 5.53s\tremaining: 1.25s\n",
      "204:\tlearn: 1724.4568343\ttotal: 5.57s\tremaining: 1.22s\n",
      "205:\tlearn: 1723.9124226\ttotal: 5.59s\tremaining: 1.19s\n",
      "206:\tlearn: 1723.1518316\ttotal: 5.61s\tremaining: 1.17s\n",
      "207:\tlearn: 1723.0798461\ttotal: 5.64s\tremaining: 1.14s\n",
      "208:\tlearn: 1722.8086384\ttotal: 5.66s\tremaining: 1.11s\n",
      "209:\tlearn: 1722.0531472\ttotal: 5.69s\tremaining: 1.08s\n",
      "210:\tlearn: 1721.8743313\ttotal: 5.71s\tremaining: 1.06s\n",
      "211:\tlearn: 1721.1857132\ttotal: 5.74s\tremaining: 1.03s\n",
      "212:\tlearn: 1720.9975510\ttotal: 5.76s\tremaining: 1s\n",
      "213:\tlearn: 1720.4612815\ttotal: 5.78s\tremaining: 973ms\n",
      "214:\tlearn: 1719.4472638\ttotal: 5.81s\tremaining: 946ms\n",
      "215:\tlearn: 1718.8060439\ttotal: 5.83s\tremaining: 918ms\n",
      "216:\tlearn: 1718.4456416\ttotal: 5.87s\tremaining: 892ms\n",
      "217:\tlearn: 1718.2031621\ttotal: 5.9s\tremaining: 867ms\n",
      "218:\tlearn: 1717.6263092\ttotal: 5.96s\tremaining: 844ms\n",
      "219:\tlearn: 1717.0149793\ttotal: 5.99s\tremaining: 817ms\n",
      "220:\tlearn: 1716.6796258\ttotal: 6.02s\tremaining: 790ms\n",
      "221:\tlearn: 1716.0473253\ttotal: 6.05s\tremaining: 763ms\n",
      "222:\tlearn: 1715.1604474\ttotal: 6.08s\tremaining: 736ms\n",
      "223:\tlearn: 1714.5127835\ttotal: 6.1s\tremaining: 708ms\n",
      "224:\tlearn: 1713.9338084\ttotal: 6.13s\tremaining: 681ms\n",
      "225:\tlearn: 1713.4681811\ttotal: 6.15s\tremaining: 653ms\n",
      "226:\tlearn: 1713.1252335\ttotal: 6.18s\tremaining: 626ms\n",
      "227:\tlearn: 1712.7671371\ttotal: 6.21s\tremaining: 599ms\n",
      "228:\tlearn: 1712.4834648\ttotal: 6.23s\tremaining: 572ms\n",
      "229:\tlearn: 1712.1100760\ttotal: 6.26s\tremaining: 544ms\n",
      "230:\tlearn: 1711.6410118\ttotal: 6.28s\tremaining: 517ms\n",
      "231:\tlearn: 1710.8519556\ttotal: 6.3s\tremaining: 489ms\n",
      "232:\tlearn: 1710.4410364\ttotal: 6.33s\tremaining: 462ms\n",
      "233:\tlearn: 1709.5965043\ttotal: 6.36s\tremaining: 435ms\n",
      "234:\tlearn: 1709.3372708\ttotal: 6.39s\tremaining: 408ms\n",
      "235:\tlearn: 1708.9494652\ttotal: 6.42s\tremaining: 381ms\n",
      "236:\tlearn: 1708.7348333\ttotal: 6.44s\tremaining: 353ms\n",
      "237:\tlearn: 1707.9704046\ttotal: 6.46s\tremaining: 326ms\n",
      "238:\tlearn: 1707.1884213\ttotal: 6.49s\tremaining: 299ms\n",
      "239:\tlearn: 1707.0603521\ttotal: 6.51s\tremaining: 271ms\n",
      "240:\tlearn: 1706.7816622\ttotal: 6.54s\tremaining: 244ms\n",
      "241:\tlearn: 1706.3734747\ttotal: 6.57s\tremaining: 217ms\n",
      "242:\tlearn: 1705.8801132\ttotal: 6.6s\tremaining: 190ms\n",
      "243:\tlearn: 1705.7574567\ttotal: 6.62s\tremaining: 163ms\n",
      "244:\tlearn: 1705.4880164\ttotal: 6.66s\tremaining: 136ms\n",
      "245:\tlearn: 1705.0871632\ttotal: 6.68s\tremaining: 109ms\n",
      "246:\tlearn: 1704.7791672\ttotal: 6.71s\tremaining: 81.5ms\n",
      "247:\tlearn: 1704.6007020\ttotal: 6.74s\tremaining: 54.4ms\n",
      "248:\tlearn: 1704.2023190\ttotal: 6.76s\tremaining: 27.2ms\n",
      "249:\tlearn: 1704.0951495\ttotal: 6.79s\tremaining: 0us\n",
      "[CV] END .....................................iterations=250; total time=   7.0s\n",
      "Learning rate set to 0.284459\n",
      "0:\tlearn: 3751.6851053\ttotal: 74.6ms\tremaining: 18.6s\n",
      "1:\tlearn: 3214.9113741\ttotal: 100ms\tremaining: 12.4s\n",
      "2:\tlearn: 2858.5578488\ttotal: 125ms\tremaining: 10.3s\n",
      "3:\tlearn: 2642.1167150\ttotal: 147ms\tremaining: 9.06s\n",
      "4:\tlearn: 2467.8131948\ttotal: 171ms\tremaining: 8.39s\n",
      "5:\tlearn: 2354.7581249\ttotal: 196ms\tremaining: 7.96s\n",
      "6:\tlearn: 2273.4748919\ttotal: 227ms\tremaining: 7.89s\n",
      "7:\tlearn: 2212.2554033\ttotal: 252ms\tremaining: 7.64s\n",
      "8:\tlearn: 2165.0897944\ttotal: 278ms\tremaining: 7.45s\n",
      "9:\tlearn: 2133.4638064\ttotal: 300ms\tremaining: 7.2s\n",
      "10:\tlearn: 2106.8722514\ttotal: 322ms\tremaining: 6.99s\n",
      "11:\tlearn: 2093.0210434\ttotal: 341ms\tremaining: 6.75s\n",
      "12:\tlearn: 2075.8972027\ttotal: 370ms\tremaining: 6.74s\n",
      "13:\tlearn: 2063.9264822\ttotal: 402ms\tremaining: 6.77s\n",
      "14:\tlearn: 2046.4161460\ttotal: 429ms\tremaining: 6.72s\n",
      "15:\tlearn: 2033.9082474\ttotal: 455ms\tremaining: 6.66s\n",
      "16:\tlearn: 2017.7789415\ttotal: 478ms\tremaining: 6.56s\n",
      "17:\tlearn: 2006.8240406\ttotal: 503ms\tremaining: 6.48s\n",
      "18:\tlearn: 2000.1307394\ttotal: 525ms\tremaining: 6.38s\n",
      "19:\tlearn: 1992.7345701\ttotal: 545ms\tremaining: 6.27s\n",
      "20:\tlearn: 1985.6863499\ttotal: 570ms\tremaining: 6.22s\n",
      "21:\tlearn: 1981.8620704\ttotal: 596ms\tremaining: 6.17s\n",
      "22:\tlearn: 1974.9280500\ttotal: 619ms\tremaining: 6.11s\n",
      "23:\tlearn: 1970.0917947\ttotal: 651ms\tremaining: 6.13s\n",
      "24:\tlearn: 1962.5396680\ttotal: 673ms\tremaining: 6.05s\n",
      "25:\tlearn: 1957.6540175\ttotal: 698ms\tremaining: 6.01s\n",
      "26:\tlearn: 1953.3434557\ttotal: 727ms\tremaining: 6s\n",
      "27:\tlearn: 1949.5588567\ttotal: 757ms\tremaining: 6s\n",
      "28:\tlearn: 1946.3104536\ttotal: 788ms\tremaining: 6s\n",
      "29:\tlearn: 1941.8135206\ttotal: 818ms\tremaining: 6s\n",
      "30:\tlearn: 1938.4370547\ttotal: 849ms\tremaining: 5.99s\n",
      "31:\tlearn: 1935.7641918\ttotal: 874ms\tremaining: 5.96s\n",
      "32:\tlearn: 1932.7781471\ttotal: 907ms\tremaining: 5.96s\n",
      "33:\tlearn: 1927.8469786\ttotal: 935ms\tremaining: 5.94s\n",
      "34:\tlearn: 1924.8164728\ttotal: 960ms\tremaining: 5.89s\n",
      "35:\tlearn: 1921.7110164\ttotal: 986ms\tremaining: 5.86s\n",
      "36:\tlearn: 1919.8041594\ttotal: 1.01s\tremaining: 5.84s\n",
      "37:\tlearn: 1918.9348534\ttotal: 1.04s\tremaining: 5.83s\n",
      "38:\tlearn: 1917.1855609\ttotal: 1.08s\tremaining: 5.86s\n",
      "39:\tlearn: 1912.0659010\ttotal: 1.13s\tremaining: 5.94s\n",
      "40:\tlearn: 1908.9663250\ttotal: 1.15s\tremaining: 5.88s\n",
      "41:\tlearn: 1905.9328111\ttotal: 1.18s\tremaining: 5.85s\n",
      "42:\tlearn: 1903.4827120\ttotal: 1.2s\tremaining: 5.79s\n",
      "43:\tlearn: 1898.6805293\ttotal: 1.23s\tremaining: 5.76s\n",
      "44:\tlearn: 1895.7752554\ttotal: 1.26s\tremaining: 5.73s\n",
      "45:\tlearn: 1892.2337721\ttotal: 1.29s\tremaining: 5.7s\n",
      "46:\tlearn: 1889.8031899\ttotal: 1.31s\tremaining: 5.67s\n",
      "47:\tlearn: 1886.9658795\ttotal: 1.33s\tremaining: 5.61s\n",
      "48:\tlearn: 1883.7861289\ttotal: 1.35s\tremaining: 5.56s\n",
      "49:\tlearn: 1880.7502525\ttotal: 1.38s\tremaining: 5.51s\n",
      "50:\tlearn: 1878.9599810\ttotal: 1.4s\tremaining: 5.47s\n",
      "51:\tlearn: 1876.8742319\ttotal: 1.43s\tremaining: 5.45s\n",
      "52:\tlearn: 1875.5227522\ttotal: 1.46s\tremaining: 5.42s\n",
      "53:\tlearn: 1873.6860551\ttotal: 1.48s\tremaining: 5.37s\n",
      "54:\tlearn: 1871.1325658\ttotal: 1.51s\tremaining: 5.35s\n",
      "55:\tlearn: 1868.4962302\ttotal: 1.53s\tremaining: 5.32s\n",
      "56:\tlearn: 1866.7923412\ttotal: 1.56s\tremaining: 5.27s\n",
      "57:\tlearn: 1864.1596723\ttotal: 1.58s\tremaining: 5.24s\n",
      "58:\tlearn: 1861.3246019\ttotal: 1.61s\tremaining: 5.2s\n",
      "59:\tlearn: 1858.7886957\ttotal: 1.63s\tremaining: 5.16s\n",
      "60:\tlearn: 1856.9345800\ttotal: 1.65s\tremaining: 5.12s\n",
      "61:\tlearn: 1854.9387937\ttotal: 1.68s\tremaining: 5.09s\n",
      "62:\tlearn: 1852.5409366\ttotal: 1.7s\tremaining: 5.04s\n",
      "63:\tlearn: 1850.1958620\ttotal: 1.73s\tremaining: 5.02s\n",
      "64:\tlearn: 1849.1782394\ttotal: 1.77s\tremaining: 5.03s\n",
      "65:\tlearn: 1847.2890937\ttotal: 1.79s\tremaining: 4.99s\n",
      "66:\tlearn: 1846.1714946\ttotal: 1.81s\tremaining: 4.95s\n",
      "67:\tlearn: 1844.2722068\ttotal: 1.84s\tremaining: 4.94s\n",
      "68:\tlearn: 1842.4319174\ttotal: 1.86s\tremaining: 4.88s\n",
      "69:\tlearn: 1840.5266901\ttotal: 1.89s\tremaining: 4.85s\n",
      "70:\tlearn: 1839.1028361\ttotal: 1.91s\tremaining: 4.81s\n",
      "71:\tlearn: 1837.4590373\ttotal: 1.93s\tremaining: 4.78s\n",
      "72:\tlearn: 1835.5772575\ttotal: 1.96s\tremaining: 4.74s\n",
      "73:\tlearn: 1834.0412491\ttotal: 1.98s\tremaining: 4.72s\n",
      "74:\tlearn: 1832.8305907\ttotal: 2.01s\tremaining: 4.69s\n",
      "75:\tlearn: 1832.2241843\ttotal: 2.04s\tremaining: 4.66s\n",
      "76:\tlearn: 1830.0847679\ttotal: 2.06s\tremaining: 4.62s\n",
      "77:\tlearn: 1828.8948824\ttotal: 2.08s\tremaining: 4.59s\n",
      "78:\tlearn: 1827.9014027\ttotal: 2.11s\tremaining: 4.56s\n",
      "79:\tlearn: 1826.3163748\ttotal: 2.13s\tremaining: 4.53s\n",
      "80:\tlearn: 1825.5632628\ttotal: 2.17s\tremaining: 4.52s\n",
      "81:\tlearn: 1823.4751201\ttotal: 2.21s\tremaining: 4.53s\n",
      "82:\tlearn: 1822.3765677\ttotal: 2.23s\tremaining: 4.49s\n",
      "83:\tlearn: 1820.7921895\ttotal: 2.25s\tremaining: 4.45s\n",
      "84:\tlearn: 1820.1253375\ttotal: 2.28s\tremaining: 4.42s\n",
      "85:\tlearn: 1819.2001079\ttotal: 2.3s\tremaining: 4.38s\n",
      "86:\tlearn: 1817.9132460\ttotal: 2.32s\tremaining: 4.34s\n",
      "87:\tlearn: 1816.7024553\ttotal: 2.34s\tremaining: 4.3s\n",
      "88:\tlearn: 1815.5506607\ttotal: 2.37s\tremaining: 4.28s\n",
      "89:\tlearn: 1814.2072313\ttotal: 2.4s\tremaining: 4.26s\n",
      "90:\tlearn: 1812.8414410\ttotal: 2.42s\tremaining: 4.24s\n",
      "91:\tlearn: 1812.2481758\ttotal: 2.45s\tremaining: 4.2s\n",
      "92:\tlearn: 1810.9860557\ttotal: 2.47s\tremaining: 4.18s\n",
      "93:\tlearn: 1809.9256394\ttotal: 2.5s\tremaining: 4.14s\n",
      "94:\tlearn: 1808.8793347\ttotal: 2.52s\tremaining: 4.11s\n",
      "95:\tlearn: 1807.3298070\ttotal: 2.54s\tremaining: 4.08s\n",
      "96:\tlearn: 1806.4157854\ttotal: 2.57s\tremaining: 4.06s\n",
      "97:\tlearn: 1805.8182459\ttotal: 2.6s\tremaining: 4.04s\n",
      "98:\tlearn: 1804.5556311\ttotal: 2.63s\tremaining: 4.01s\n",
      "99:\tlearn: 1802.7578649\ttotal: 2.65s\tremaining: 3.98s\n",
      "100:\tlearn: 1801.7471096\ttotal: 2.68s\tremaining: 3.96s\n",
      "101:\tlearn: 1800.5088416\ttotal: 2.71s\tremaining: 3.93s\n",
      "102:\tlearn: 1799.5270120\ttotal: 2.73s\tremaining: 3.9s\n",
      "103:\tlearn: 1798.7096117\ttotal: 2.77s\tremaining: 3.89s\n",
      "104:\tlearn: 1797.8512127\ttotal: 2.8s\tremaining: 3.86s\n",
      "105:\tlearn: 1796.6120242\ttotal: 2.83s\tremaining: 3.84s\n",
      "106:\tlearn: 1795.4382855\ttotal: 2.85s\tremaining: 3.81s\n",
      "107:\tlearn: 1794.4333823\ttotal: 2.88s\tremaining: 3.78s\n",
      "108:\tlearn: 1793.9594709\ttotal: 2.9s\tremaining: 3.75s\n",
      "109:\tlearn: 1793.2531680\ttotal: 2.93s\tremaining: 3.73s\n",
      "110:\tlearn: 1792.0825646\ttotal: 2.95s\tremaining: 3.69s\n",
      "111:\tlearn: 1791.4259135\ttotal: 2.97s\tremaining: 3.66s\n",
      "112:\tlearn: 1790.1480220\ttotal: 3s\tremaining: 3.63s\n",
      "113:\tlearn: 1789.1797602\ttotal: 3.02s\tremaining: 3.6s\n",
      "114:\tlearn: 1788.3961672\ttotal: 3.04s\tremaining: 3.57s\n",
      "115:\tlearn: 1787.7162089\ttotal: 3.07s\tremaining: 3.55s\n",
      "116:\tlearn: 1787.4439668\ttotal: 3.1s\tremaining: 3.52s\n",
      "117:\tlearn: 1786.2698393\ttotal: 3.12s\tremaining: 3.49s\n",
      "118:\tlearn: 1785.6581736\ttotal: 3.14s\tremaining: 3.46s\n",
      "119:\tlearn: 1784.9808696\ttotal: 3.16s\tremaining: 3.42s\n",
      "120:\tlearn: 1784.2452297\ttotal: 3.19s\tremaining: 3.4s\n",
      "121:\tlearn: 1783.6762567\ttotal: 3.25s\tremaining: 3.41s\n",
      "122:\tlearn: 1781.9544500\ttotal: 3.27s\tremaining: 3.38s\n",
      "123:\tlearn: 1780.8465394\ttotal: 3.3s\tremaining: 3.35s\n",
      "124:\tlearn: 1779.3701747\ttotal: 3.32s\tremaining: 3.32s\n",
      "125:\tlearn: 1778.6546615\ttotal: 3.35s\tremaining: 3.3s\n",
      "126:\tlearn: 1777.5002254\ttotal: 3.38s\tremaining: 3.27s\n",
      "127:\tlearn: 1776.7745856\ttotal: 3.41s\tremaining: 3.25s\n",
      "128:\tlearn: 1775.6632761\ttotal: 3.44s\tremaining: 3.23s\n",
      "129:\tlearn: 1774.6087563\ttotal: 3.46s\tremaining: 3.19s\n",
      "130:\tlearn: 1774.0349152\ttotal: 3.48s\tremaining: 3.17s\n",
      "131:\tlearn: 1772.7790076\ttotal: 3.51s\tremaining: 3.14s\n",
      "132:\tlearn: 1771.4076069\ttotal: 3.53s\tremaining: 3.11s\n",
      "133:\tlearn: 1770.5993513\ttotal: 3.56s\tremaining: 3.08s\n",
      "134:\tlearn: 1769.7544006\ttotal: 3.58s\tremaining: 3.05s\n",
      "135:\tlearn: 1768.8664145\ttotal: 3.6s\tremaining: 3.02s\n",
      "136:\tlearn: 1768.0613063\ttotal: 3.63s\tremaining: 2.99s\n",
      "137:\tlearn: 1767.4327388\ttotal: 3.66s\tremaining: 2.97s\n",
      "138:\tlearn: 1766.4072140\ttotal: 3.68s\tremaining: 2.94s\n",
      "139:\tlearn: 1764.8024303\ttotal: 3.71s\tremaining: 2.91s\n",
      "140:\tlearn: 1764.4711082\ttotal: 3.73s\tremaining: 2.88s\n",
      "141:\tlearn: 1764.3906722\ttotal: 3.76s\tremaining: 2.86s\n",
      "142:\tlearn: 1763.5410921\ttotal: 3.78s\tremaining: 2.83s\n",
      "143:\tlearn: 1762.8885555\ttotal: 3.81s\tremaining: 2.81s\n",
      "144:\tlearn: 1762.0053283\ttotal: 3.84s\tremaining: 2.78s\n",
      "145:\tlearn: 1761.3342833\ttotal: 3.87s\tremaining: 2.75s\n",
      "146:\tlearn: 1760.6167959\ttotal: 3.89s\tremaining: 2.73s\n",
      "147:\tlearn: 1759.7295753\ttotal: 3.91s\tremaining: 2.7s\n",
      "148:\tlearn: 1758.9443896\ttotal: 3.94s\tremaining: 2.67s\n",
      "149:\tlearn: 1757.8452408\ttotal: 3.97s\tremaining: 2.65s\n",
      "150:\tlearn: 1757.1369216\ttotal: 4s\tremaining: 2.62s\n",
      "151:\tlearn: 1756.5917180\ttotal: 4.03s\tremaining: 2.6s\n",
      "152:\tlearn: 1755.7215408\ttotal: 4.06s\tremaining: 2.57s\n",
      "153:\tlearn: 1755.2517466\ttotal: 4.08s\tremaining: 2.55s\n",
      "154:\tlearn: 1754.3090724\ttotal: 4.11s\tremaining: 2.52s\n",
      "155:\tlearn: 1753.7592464\ttotal: 4.13s\tremaining: 2.49s\n",
      "156:\tlearn: 1753.3176627\ttotal: 4.16s\tremaining: 2.46s\n",
      "157:\tlearn: 1752.8593880\ttotal: 4.18s\tremaining: 2.44s\n",
      "158:\tlearn: 1751.6099539\ttotal: 4.21s\tremaining: 2.41s\n",
      "159:\tlearn: 1751.3747446\ttotal: 4.23s\tremaining: 2.38s\n",
      "160:\tlearn: 1750.3837800\ttotal: 4.25s\tremaining: 2.35s\n",
      "161:\tlearn: 1749.7849516\ttotal: 4.29s\tremaining: 2.33s\n",
      "162:\tlearn: 1748.9107106\ttotal: 4.35s\tremaining: 2.32s\n",
      "163:\tlearn: 1748.0372392\ttotal: 4.38s\tremaining: 2.29s\n",
      "164:\tlearn: 1747.6219519\ttotal: 4.4s\tremaining: 2.27s\n",
      "165:\tlearn: 1746.7517571\ttotal: 4.43s\tremaining: 2.24s\n",
      "166:\tlearn: 1746.2711466\ttotal: 4.46s\tremaining: 2.21s\n",
      "167:\tlearn: 1745.7647180\ttotal: 4.48s\tremaining: 2.19s\n",
      "168:\tlearn: 1745.5768017\ttotal: 4.5s\tremaining: 2.16s\n",
      "169:\tlearn: 1745.4226310\ttotal: 4.53s\tremaining: 2.13s\n",
      "170:\tlearn: 1744.7759722\ttotal: 4.56s\tremaining: 2.11s\n",
      "171:\tlearn: 1744.3880650\ttotal: 4.6s\tremaining: 2.08s\n",
      "172:\tlearn: 1743.8294064\ttotal: 4.63s\tremaining: 2.06s\n",
      "173:\tlearn: 1743.1428370\ttotal: 4.65s\tremaining: 2.03s\n",
      "174:\tlearn: 1743.1065190\ttotal: 4.68s\tremaining: 2.01s\n",
      "175:\tlearn: 1743.0891857\ttotal: 4.71s\tremaining: 1.98s\n",
      "176:\tlearn: 1742.8930829\ttotal: 4.75s\tremaining: 1.96s\n",
      "177:\tlearn: 1742.3746979\ttotal: 4.78s\tremaining: 1.93s\n",
      "178:\tlearn: 1741.2929494\ttotal: 4.82s\tremaining: 1.91s\n",
      "179:\tlearn: 1740.8642433\ttotal: 4.85s\tremaining: 1.89s\n",
      "180:\tlearn: 1740.0078955\ttotal: 4.87s\tremaining: 1.86s\n",
      "181:\tlearn: 1739.1721083\ttotal: 4.9s\tremaining: 1.83s\n",
      "182:\tlearn: 1738.6421547\ttotal: 4.92s\tremaining: 1.8s\n",
      "183:\tlearn: 1737.8616156\ttotal: 4.95s\tremaining: 1.78s\n",
      "184:\tlearn: 1737.4411065\ttotal: 4.98s\tremaining: 1.75s\n",
      "185:\tlearn: 1736.4143260\ttotal: 5.01s\tremaining: 1.72s\n",
      "186:\tlearn: 1736.0828999\ttotal: 5.03s\tremaining: 1.7s\n",
      "187:\tlearn: 1735.5246161\ttotal: 5.05s\tremaining: 1.67s\n",
      "188:\tlearn: 1735.1049447\ttotal: 5.08s\tremaining: 1.64s\n",
      "189:\tlearn: 1734.3218933\ttotal: 5.11s\tremaining: 1.61s\n",
      "190:\tlearn: 1734.0937318\ttotal: 5.13s\tremaining: 1.58s\n",
      "191:\tlearn: 1733.7676622\ttotal: 5.15s\tremaining: 1.56s\n",
      "192:\tlearn: 1733.5659617\ttotal: 5.18s\tremaining: 1.53s\n",
      "193:\tlearn: 1732.8895286\ttotal: 5.21s\tremaining: 1.5s\n",
      "194:\tlearn: 1732.6646974\ttotal: 5.24s\tremaining: 1.48s\n",
      "195:\tlearn: 1731.9999643\ttotal: 5.26s\tremaining: 1.45s\n",
      "196:\tlearn: 1731.3026518\ttotal: 5.28s\tremaining: 1.42s\n",
      "197:\tlearn: 1730.7340514\ttotal: 5.31s\tremaining: 1.39s\n",
      "198:\tlearn: 1730.6291372\ttotal: 5.34s\tremaining: 1.37s\n",
      "199:\tlearn: 1730.0726548\ttotal: 5.39s\tremaining: 1.35s\n",
      "200:\tlearn: 1729.5702659\ttotal: 5.43s\tremaining: 1.32s\n",
      "201:\tlearn: 1729.2481888\ttotal: 5.45s\tremaining: 1.3s\n",
      "202:\tlearn: 1728.2820231\ttotal: 5.47s\tremaining: 1.27s\n",
      "203:\tlearn: 1728.0856472\ttotal: 5.5s\tremaining: 1.24s\n",
      "204:\tlearn: 1727.1646579\ttotal: 5.53s\tremaining: 1.21s\n",
      "205:\tlearn: 1727.0922518\ttotal: 5.55s\tremaining: 1.19s\n",
      "206:\tlearn: 1726.4567498\ttotal: 5.58s\tremaining: 1.16s\n",
      "207:\tlearn: 1725.8786889\ttotal: 5.6s\tremaining: 1.13s\n",
      "208:\tlearn: 1725.1558127\ttotal: 5.63s\tremaining: 1.1s\n",
      "209:\tlearn: 1724.8156630\ttotal: 5.66s\tremaining: 1.08s\n",
      "210:\tlearn: 1724.3430368\ttotal: 5.69s\tremaining: 1.05s\n",
      "211:\tlearn: 1723.7517163\ttotal: 5.72s\tremaining: 1.02s\n",
      "212:\tlearn: 1723.1420672\ttotal: 5.74s\tremaining: 998ms\n",
      "213:\tlearn: 1722.6619514\ttotal: 5.78s\tremaining: 972ms\n",
      "214:\tlearn: 1722.2078309\ttotal: 5.8s\tremaining: 944ms\n",
      "215:\tlearn: 1721.5181550\ttotal: 5.83s\tremaining: 917ms\n",
      "216:\tlearn: 1721.5086622\ttotal: 5.85s\tremaining: 890ms\n",
      "217:\tlearn: 1721.2972347\ttotal: 5.87s\tremaining: 861ms\n",
      "218:\tlearn: 1721.0182886\ttotal: 5.9s\tremaining: 835ms\n",
      "219:\tlearn: 1720.7591523\ttotal: 5.92s\tremaining: 808ms\n",
      "220:\tlearn: 1720.5512629\ttotal: 5.96s\tremaining: 782ms\n",
      "221:\tlearn: 1720.3687310\ttotal: 5.97s\tremaining: 754ms\n",
      "222:\tlearn: 1719.8442996\ttotal: 6s\tremaining: 727ms\n",
      "223:\tlearn: 1719.4010563\ttotal: 6.03s\tremaining: 700ms\n",
      "224:\tlearn: 1718.6418220\ttotal: 6.06s\tremaining: 674ms\n",
      "225:\tlearn: 1718.3785666\ttotal: 6.09s\tremaining: 647ms\n",
      "226:\tlearn: 1717.9234774\ttotal: 6.11s\tremaining: 619ms\n",
      "227:\tlearn: 1717.5316030\ttotal: 6.13s\tremaining: 592ms\n",
      "228:\tlearn: 1717.4107168\ttotal: 6.16s\tremaining: 565ms\n",
      "229:\tlearn: 1717.4088406\ttotal: 6.18s\tremaining: 537ms\n",
      "230:\tlearn: 1717.3402407\ttotal: 6.19s\tremaining: 509ms\n",
      "231:\tlearn: 1716.8181011\ttotal: 6.21s\tremaining: 482ms\n",
      "232:\tlearn: 1716.4887595\ttotal: 6.25s\tremaining: 456ms\n",
      "233:\tlearn: 1716.1790974\ttotal: 6.29s\tremaining: 430ms\n",
      "234:\tlearn: 1716.0051588\ttotal: 6.32s\tremaining: 403ms\n",
      "235:\tlearn: 1715.3541751\ttotal: 6.34s\tremaining: 376ms\n",
      "236:\tlearn: 1715.1967832\ttotal: 6.37s\tremaining: 349ms\n",
      "237:\tlearn: 1715.0291279\ttotal: 6.4s\tremaining: 323ms\n",
      "238:\tlearn: 1714.6801497\ttotal: 6.44s\tremaining: 296ms\n",
      "239:\tlearn: 1714.4031451\ttotal: 6.47s\tremaining: 270ms\n",
      "240:\tlearn: 1714.3822566\ttotal: 6.49s\tremaining: 243ms\n",
      "241:\tlearn: 1714.1979980\ttotal: 6.53s\tremaining: 216ms\n",
      "242:\tlearn: 1713.5302939\ttotal: 6.56s\tremaining: 189ms\n",
      "243:\tlearn: 1713.2650631\ttotal: 6.59s\tremaining: 162ms\n",
      "244:\tlearn: 1713.0468892\ttotal: 6.61s\tremaining: 135ms\n",
      "245:\tlearn: 1712.8836057\ttotal: 6.64s\tremaining: 108ms\n",
      "246:\tlearn: 1712.3946476\ttotal: 6.67s\tremaining: 81ms\n",
      "247:\tlearn: 1712.1543422\ttotal: 6.7s\tremaining: 54ms\n",
      "248:\tlearn: 1711.6440120\ttotal: 6.73s\tremaining: 27ms\n",
      "249:\tlearn: 1711.4718589\ttotal: 6.75s\tremaining: 0us\n",
      "[CV] END .....................................iterations=250; total time=   7.0s\n",
      "Learning rate set to 0.284459\n",
      "0:\tlearn: 3736.8580650\ttotal: 32ms\tremaining: 7.96s\n",
      "1:\tlearn: 3208.6618789\ttotal: 61.8ms\tremaining: 7.66s\n",
      "2:\tlearn: 2866.8716547\ttotal: 87.2ms\tremaining: 7.18s\n",
      "3:\tlearn: 2649.9296952\ttotal: 116ms\tremaining: 7.11s\n",
      "4:\tlearn: 2472.3002653\ttotal: 147ms\tremaining: 7.22s\n",
      "5:\tlearn: 2368.9385802\ttotal: 181ms\tremaining: 7.34s\n",
      "6:\tlearn: 2286.0456199\ttotal: 217ms\tremaining: 7.55s\n",
      "7:\tlearn: 2207.7775902\ttotal: 276ms\tremaining: 8.33s\n",
      "8:\tlearn: 2163.1263878\ttotal: 304ms\tremaining: 8.15s\n",
      "9:\tlearn: 2128.5425673\ttotal: 331ms\tremaining: 7.94s\n",
      "10:\tlearn: 2106.5245817\ttotal: 361ms\tremaining: 7.84s\n",
      "11:\tlearn: 2085.6019979\ttotal: 388ms\tremaining: 7.7s\n",
      "12:\tlearn: 2067.4348299\ttotal: 409ms\tremaining: 7.46s\n",
      "13:\tlearn: 2050.2374254\ttotal: 437ms\tremaining: 7.37s\n",
      "14:\tlearn: 2041.3644362\ttotal: 466ms\tremaining: 7.3s\n",
      "15:\tlearn: 2029.5502780\ttotal: 485ms\tremaining: 7.1s\n",
      "16:\tlearn: 2019.6837577\ttotal: 511ms\tremaining: 7s\n",
      "17:\tlearn: 2010.9643649\ttotal: 548ms\tremaining: 7.06s\n",
      "18:\tlearn: 2001.3343215\ttotal: 575ms\tremaining: 6.99s\n",
      "19:\tlearn: 1993.0754731\ttotal: 590ms\tremaining: 6.79s\n",
      "20:\tlearn: 1983.2222594\ttotal: 614ms\tremaining: 6.7s\n",
      "21:\tlearn: 1977.6430723\ttotal: 643ms\tremaining: 6.66s\n",
      "22:\tlearn: 1971.7884443\ttotal: 671ms\tremaining: 6.63s\n",
      "23:\tlearn: 1963.4736860\ttotal: 694ms\tremaining: 6.54s\n",
      "24:\tlearn: 1959.8042069\ttotal: 721ms\tremaining: 6.49s\n",
      "25:\tlearn: 1955.7192966\ttotal: 760ms\tremaining: 6.54s\n",
      "26:\tlearn: 1952.3870339\ttotal: 788ms\tremaining: 6.5s\n",
      "27:\tlearn: 1947.0762023\ttotal: 815ms\tremaining: 6.46s\n",
      "28:\tlearn: 1942.6673681\ttotal: 833ms\tremaining: 6.35s\n",
      "29:\tlearn: 1936.2190161\ttotal: 858ms\tremaining: 6.29s\n",
      "30:\tlearn: 1932.1728059\ttotal: 888ms\tremaining: 6.28s\n",
      "31:\tlearn: 1928.3471018\ttotal: 916ms\tremaining: 6.24s\n",
      "32:\tlearn: 1923.9082660\ttotal: 946ms\tremaining: 6.22s\n",
      "33:\tlearn: 1919.5751571\ttotal: 969ms\tremaining: 6.15s\n",
      "34:\tlearn: 1917.4901393\ttotal: 994ms\tremaining: 6.1s\n",
      "35:\tlearn: 1914.3416604\ttotal: 1.02s\tremaining: 6.04s\n",
      "36:\tlearn: 1910.9892049\ttotal: 1.04s\tremaining: 6.01s\n",
      "37:\tlearn: 1908.1918217\ttotal: 1.07s\tremaining: 5.97s\n",
      "38:\tlearn: 1906.0863489\ttotal: 1.12s\tremaining: 6.05s\n",
      "39:\tlearn: 1901.5150726\ttotal: 1.15s\tremaining: 6.01s\n",
      "40:\tlearn: 1899.1626646\ttotal: 1.17s\tremaining: 5.96s\n",
      "41:\tlearn: 1897.2444301\ttotal: 1.19s\tremaining: 5.91s\n",
      "42:\tlearn: 1895.2555907\ttotal: 1.22s\tremaining: 5.86s\n",
      "43:\tlearn: 1893.3992350\ttotal: 1.25s\tremaining: 5.83s\n",
      "44:\tlearn: 1889.9886543\ttotal: 1.27s\tremaining: 5.77s\n",
      "45:\tlearn: 1888.1609922\ttotal: 1.3s\tremaining: 5.78s\n",
      "46:\tlearn: 1884.6006637\ttotal: 1.33s\tremaining: 5.73s\n",
      "47:\tlearn: 1882.0568839\ttotal: 1.35s\tremaining: 5.68s\n",
      "48:\tlearn: 1879.5780881\ttotal: 1.38s\tremaining: 5.65s\n",
      "49:\tlearn: 1876.3958622\ttotal: 1.4s\tremaining: 5.6s\n",
      "50:\tlearn: 1874.2562054\ttotal: 1.43s\tremaining: 5.58s\n",
      "51:\tlearn: 1870.9015726\ttotal: 1.45s\tremaining: 5.52s\n",
      "52:\tlearn: 1868.3417796\ttotal: 1.48s\tremaining: 5.48s\n",
      "53:\tlearn: 1864.5015991\ttotal: 1.5s\tremaining: 5.45s\n",
      "54:\tlearn: 1862.3217287\ttotal: 1.53s\tremaining: 5.45s\n",
      "55:\tlearn: 1860.1509583\ttotal: 1.56s\tremaining: 5.41s\n",
      "56:\tlearn: 1858.0954818\ttotal: 1.58s\tremaining: 5.36s\n",
      "57:\tlearn: 1855.5290081\ttotal: 1.61s\tremaining: 5.32s\n",
      "58:\tlearn: 1853.9782000\ttotal: 1.64s\tremaining: 5.3s\n",
      "59:\tlearn: 1852.5144265\ttotal: 1.66s\tremaining: 5.25s\n",
      "60:\tlearn: 1850.8379791\ttotal: 1.68s\tremaining: 5.22s\n",
      "61:\tlearn: 1849.5353869\ttotal: 1.71s\tremaining: 5.19s\n",
      "62:\tlearn: 1847.4475717\ttotal: 1.74s\tremaining: 5.17s\n",
      "63:\tlearn: 1844.9556248\ttotal: 1.79s\tremaining: 5.21s\n",
      "64:\tlearn: 1842.0336908\ttotal: 1.81s\tremaining: 5.16s\n",
      "65:\tlearn: 1840.9390137\ttotal: 1.85s\tremaining: 5.15s\n",
      "66:\tlearn: 1839.8456175\ttotal: 1.87s\tremaining: 5.11s\n",
      "67:\tlearn: 1838.6269745\ttotal: 1.9s\tremaining: 5.08s\n",
      "68:\tlearn: 1836.5960679\ttotal: 1.92s\tremaining: 5.04s\n",
      "69:\tlearn: 1835.1945957\ttotal: 1.95s\tremaining: 5.01s\n",
      "70:\tlearn: 1832.4829573\ttotal: 2s\tremaining: 5.04s\n",
      "71:\tlearn: 1831.2341400\ttotal: 2.03s\tremaining: 5.02s\n",
      "72:\tlearn: 1829.6071730\ttotal: 2.05s\tremaining: 4.97s\n",
      "73:\tlearn: 1828.6349451\ttotal: 2.07s\tremaining: 4.92s\n",
      "74:\tlearn: 1826.4107180\ttotal: 2.09s\tremaining: 4.88s\n",
      "75:\tlearn: 1825.7254879\ttotal: 2.11s\tremaining: 4.83s\n",
      "76:\tlearn: 1824.1089138\ttotal: 2.13s\tremaining: 4.8s\n",
      "77:\tlearn: 1823.1708125\ttotal: 2.17s\tremaining: 4.79s\n",
      "78:\tlearn: 1821.5963890\ttotal: 2.2s\tremaining: 4.76s\n",
      "79:\tlearn: 1820.5566369\ttotal: 2.23s\tremaining: 4.74s\n",
      "80:\tlearn: 1819.3230524\ttotal: 2.25s\tremaining: 4.71s\n",
      "81:\tlearn: 1817.5323679\ttotal: 2.28s\tremaining: 4.67s\n",
      "82:\tlearn: 1816.7714155\ttotal: 2.31s\tremaining: 4.65s\n",
      "83:\tlearn: 1816.2594190\ttotal: 2.34s\tremaining: 4.62s\n",
      "84:\tlearn: 1814.3971653\ttotal: 2.37s\tremaining: 4.59s\n",
      "85:\tlearn: 1813.5408586\ttotal: 2.39s\tremaining: 4.56s\n",
      "86:\tlearn: 1811.8638403\ttotal: 2.42s\tremaining: 4.54s\n",
      "87:\tlearn: 1810.8426528\ttotal: 2.45s\tremaining: 4.51s\n",
      "88:\tlearn: 1808.8797835\ttotal: 2.47s\tremaining: 4.47s\n",
      "89:\tlearn: 1807.4659112\ttotal: 2.5s\tremaining: 4.44s\n",
      "90:\tlearn: 1805.4692514\ttotal: 2.52s\tremaining: 4.4s\n",
      "91:\tlearn: 1803.4298561\ttotal: 2.54s\tremaining: 4.36s\n",
      "92:\tlearn: 1801.1584715\ttotal: 2.56s\tremaining: 4.33s\n",
      "93:\tlearn: 1800.3686839\ttotal: 2.59s\tremaining: 4.3s\n",
      "94:\tlearn: 1799.6757391\ttotal: 2.62s\tremaining: 4.27s\n",
      "95:\tlearn: 1798.8896429\ttotal: 2.65s\tremaining: 4.25s\n",
      "96:\tlearn: 1797.7801352\ttotal: 2.67s\tremaining: 4.22s\n",
      "97:\tlearn: 1796.7636409\ttotal: 2.69s\tremaining: 4.18s\n",
      "98:\tlearn: 1795.2793760\ttotal: 2.71s\tremaining: 4.14s\n",
      "99:\tlearn: 1794.5079370\ttotal: 2.74s\tremaining: 4.11s\n",
      "100:\tlearn: 1793.7526060\ttotal: 2.77s\tremaining: 4.09s\n",
      "101:\tlearn: 1792.5674546\ttotal: 2.8s\tremaining: 4.06s\n",
      "102:\tlearn: 1791.9393292\ttotal: 2.83s\tremaining: 4.03s\n",
      "103:\tlearn: 1791.5536569\ttotal: 2.86s\tremaining: 4.02s\n",
      "104:\tlearn: 1791.2182292\ttotal: 2.9s\tremaining: 4s\n",
      "105:\tlearn: 1789.9609596\ttotal: 2.92s\tremaining: 3.96s\n",
      "106:\tlearn: 1788.1531665\ttotal: 2.94s\tremaining: 3.93s\n",
      "107:\tlearn: 1787.6523532\ttotal: 2.97s\tremaining: 3.9s\n",
      "108:\tlearn: 1786.6458942\ttotal: 3s\tremaining: 3.88s\n",
      "109:\tlearn: 1785.5700691\ttotal: 3.02s\tremaining: 3.84s\n",
      "110:\tlearn: 1784.3669413\ttotal: 3.04s\tremaining: 3.81s\n",
      "111:\tlearn: 1783.7494639\ttotal: 3.07s\tremaining: 3.78s\n",
      "112:\tlearn: 1783.0873903\ttotal: 3.1s\tremaining: 3.76s\n",
      "113:\tlearn: 1781.9806704\ttotal: 3.12s\tremaining: 3.72s\n",
      "114:\tlearn: 1781.3622078\ttotal: 3.15s\tremaining: 3.7s\n",
      "115:\tlearn: 1780.5498472\ttotal: 3.18s\tremaining: 3.67s\n",
      "116:\tlearn: 1779.3317233\ttotal: 3.2s\tremaining: 3.64s\n",
      "117:\tlearn: 1778.7077504\ttotal: 3.23s\tremaining: 3.61s\n",
      "118:\tlearn: 1777.3439498\ttotal: 3.25s\tremaining: 3.58s\n",
      "119:\tlearn: 1776.9168711\ttotal: 3.28s\tremaining: 3.56s\n",
      "120:\tlearn: 1776.4532778\ttotal: 3.31s\tremaining: 3.52s\n",
      "121:\tlearn: 1775.9172115\ttotal: 3.33s\tremaining: 3.5s\n",
      "122:\tlearn: 1775.1771874\ttotal: 3.35s\tremaining: 3.46s\n",
      "123:\tlearn: 1773.1085776\ttotal: 3.38s\tremaining: 3.43s\n",
      "124:\tlearn: 1772.5713898\ttotal: 3.4s\tremaining: 3.4s\n",
      "125:\tlearn: 1771.5513977\ttotal: 3.44s\tremaining: 3.38s\n",
      "126:\tlearn: 1770.9760750\ttotal: 3.46s\tremaining: 3.35s\n",
      "127:\tlearn: 1770.7120798\ttotal: 3.5s\tremaining: 3.33s\n",
      "128:\tlearn: 1770.2596143\ttotal: 3.52s\tremaining: 3.31s\n",
      "129:\tlearn: 1769.2403281\ttotal: 3.55s\tremaining: 3.28s\n",
      "130:\tlearn: 1768.2847328\ttotal: 3.57s\tremaining: 3.25s\n",
      "131:\tlearn: 1767.4811257\ttotal: 3.6s\tremaining: 3.22s\n",
      "132:\tlearn: 1766.8453022\ttotal: 3.63s\tremaining: 3.19s\n",
      "133:\tlearn: 1765.0086929\ttotal: 3.65s\tremaining: 3.16s\n",
      "134:\tlearn: 1764.3534317\ttotal: 3.69s\tremaining: 3.14s\n",
      "135:\tlearn: 1763.8570434\ttotal: 3.74s\tremaining: 3.14s\n",
      "136:\tlearn: 1763.2452319\ttotal: 3.77s\tremaining: 3.11s\n",
      "137:\tlearn: 1762.3618503\ttotal: 3.8s\tremaining: 3.08s\n",
      "138:\tlearn: 1761.5192382\ttotal: 3.83s\tremaining: 3.06s\n",
      "139:\tlearn: 1760.9603980\ttotal: 3.85s\tremaining: 3.02s\n",
      "140:\tlearn: 1760.3513508\ttotal: 3.88s\tremaining: 3s\n",
      "141:\tlearn: 1759.7365213\ttotal: 3.91s\tremaining: 2.97s\n",
      "142:\tlearn: 1758.6677347\ttotal: 3.94s\tremaining: 2.94s\n",
      "143:\tlearn: 1757.9439806\ttotal: 3.97s\tremaining: 2.92s\n",
      "144:\tlearn: 1757.4952006\ttotal: 3.99s\tremaining: 2.89s\n",
      "145:\tlearn: 1757.1608419\ttotal: 4.02s\tremaining: 2.86s\n",
      "146:\tlearn: 1757.0831847\ttotal: 4.04s\tremaining: 2.83s\n",
      "147:\tlearn: 1756.1864909\ttotal: 4.06s\tremaining: 2.8s\n",
      "148:\tlearn: 1755.3632656\ttotal: 4.09s\tremaining: 2.77s\n",
      "149:\tlearn: 1753.8474037\ttotal: 4.11s\tremaining: 2.74s\n",
      "150:\tlearn: 1753.5990998\ttotal: 4.14s\tremaining: 2.71s\n",
      "151:\tlearn: 1753.1703761\ttotal: 4.16s\tremaining: 2.68s\n",
      "152:\tlearn: 1752.4592009\ttotal: 4.18s\tremaining: 2.65s\n",
      "153:\tlearn: 1752.1453806\ttotal: 4.21s\tremaining: 2.62s\n",
      "154:\tlearn: 1751.9270994\ttotal: 4.23s\tremaining: 2.59s\n",
      "155:\tlearn: 1751.5094290\ttotal: 4.25s\tremaining: 2.56s\n",
      "156:\tlearn: 1750.5713849\ttotal: 4.28s\tremaining: 2.53s\n",
      "157:\tlearn: 1750.0547297\ttotal: 4.3s\tremaining: 2.5s\n",
      "158:\tlearn: 1749.2011661\ttotal: 4.33s\tremaining: 2.48s\n",
      "159:\tlearn: 1748.8993223\ttotal: 4.36s\tremaining: 2.46s\n",
      "160:\tlearn: 1748.5101629\ttotal: 4.39s\tremaining: 2.43s\n",
      "161:\tlearn: 1747.6663694\ttotal: 4.42s\tremaining: 2.4s\n",
      "162:\tlearn: 1747.3184113\ttotal: 4.44s\tremaining: 2.37s\n",
      "163:\tlearn: 1746.5310968\ttotal: 4.47s\tremaining: 2.35s\n",
      "164:\tlearn: 1745.6804070\ttotal: 4.49s\tremaining: 2.31s\n",
      "165:\tlearn: 1745.1728067\ttotal: 4.52s\tremaining: 2.29s\n",
      "166:\tlearn: 1744.1058585\ttotal: 4.55s\tremaining: 2.26s\n",
      "167:\tlearn: 1743.7041215\ttotal: 4.6s\tremaining: 2.25s\n",
      "168:\tlearn: 1743.1210920\ttotal: 4.64s\tremaining: 2.22s\n",
      "169:\tlearn: 1742.9401164\ttotal: 4.66s\tremaining: 2.19s\n",
      "170:\tlearn: 1742.6107679\ttotal: 4.69s\tremaining: 2.17s\n",
      "171:\tlearn: 1742.1012996\ttotal: 4.71s\tremaining: 2.13s\n",
      "172:\tlearn: 1741.5924719\ttotal: 4.74s\tremaining: 2.11s\n",
      "173:\tlearn: 1740.4420921\ttotal: 4.76s\tremaining: 2.08s\n",
      "174:\tlearn: 1739.9786224\ttotal: 4.8s\tremaining: 2.06s\n",
      "175:\tlearn: 1739.5980549\ttotal: 4.83s\tremaining: 2.03s\n",
      "176:\tlearn: 1739.1171018\ttotal: 4.85s\tremaining: 2s\n",
      "177:\tlearn: 1738.5337322\ttotal: 4.88s\tremaining: 1.97s\n",
      "178:\tlearn: 1738.1567709\ttotal: 4.9s\tremaining: 1.94s\n",
      "179:\tlearn: 1737.4749526\ttotal: 4.92s\tremaining: 1.92s\n",
      "180:\tlearn: 1737.2579974\ttotal: 4.95s\tremaining: 1.89s\n",
      "181:\tlearn: 1736.1380377\ttotal: 4.97s\tremaining: 1.86s\n",
      "182:\tlearn: 1735.5223020\ttotal: 5s\tremaining: 1.83s\n",
      "183:\tlearn: 1734.7043202\ttotal: 5.02s\tremaining: 1.8s\n",
      "184:\tlearn: 1734.4372658\ttotal: 5.05s\tremaining: 1.78s\n",
      "185:\tlearn: 1733.6667314\ttotal: 5.08s\tremaining: 1.75s\n",
      "186:\tlearn: 1733.2871611\ttotal: 5.1s\tremaining: 1.72s\n",
      "187:\tlearn: 1732.9755212\ttotal: 5.12s\tremaining: 1.69s\n",
      "188:\tlearn: 1732.6426420\ttotal: 5.15s\tremaining: 1.66s\n",
      "189:\tlearn: 1731.8416453\ttotal: 5.17s\tremaining: 1.63s\n",
      "190:\tlearn: 1731.1714311\ttotal: 5.2s\tremaining: 1.61s\n",
      "191:\tlearn: 1731.0905710\ttotal: 5.24s\tremaining: 1.58s\n",
      "192:\tlearn: 1730.6993533\ttotal: 5.26s\tremaining: 1.55s\n",
      "193:\tlearn: 1730.3480682\ttotal: 5.28s\tremaining: 1.52s\n",
      "194:\tlearn: 1729.8809100\ttotal: 5.3s\tremaining: 1.5s\n",
      "195:\tlearn: 1729.1297892\ttotal: 5.33s\tremaining: 1.47s\n",
      "196:\tlearn: 1728.5628861\ttotal: 5.36s\tremaining: 1.44s\n",
      "197:\tlearn: 1728.4982784\ttotal: 5.38s\tremaining: 1.41s\n",
      "198:\tlearn: 1727.8601876\ttotal: 5.42s\tremaining: 1.39s\n",
      "199:\tlearn: 1726.8625929\ttotal: 5.46s\tremaining: 1.36s\n",
      "200:\tlearn: 1726.4752414\ttotal: 5.48s\tremaining: 1.34s\n",
      "201:\tlearn: 1726.1450260\ttotal: 5.51s\tremaining: 1.31s\n",
      "202:\tlearn: 1725.1601562\ttotal: 5.55s\tremaining: 1.28s\n",
      "203:\tlearn: 1724.5076409\ttotal: 5.57s\tremaining: 1.26s\n",
      "204:\tlearn: 1723.9743997\ttotal: 5.61s\tremaining: 1.23s\n",
      "205:\tlearn: 1723.3048116\ttotal: 5.63s\tremaining: 1.2s\n",
      "206:\tlearn: 1722.8680314\ttotal: 5.65s\tremaining: 1.17s\n",
      "207:\tlearn: 1722.2559607\ttotal: 5.68s\tremaining: 1.15s\n",
      "208:\tlearn: 1721.2762367\ttotal: 5.71s\tremaining: 1.12s\n",
      "209:\tlearn: 1720.9860082\ttotal: 5.79s\tremaining: 1.1s\n",
      "210:\tlearn: 1720.3097087\ttotal: 5.96s\tremaining: 1.1s\n",
      "211:\tlearn: 1719.0778205\ttotal: 5.98s\tremaining: 1.07s\n",
      "212:\tlearn: 1718.6615655\ttotal: 6s\tremaining: 1.04s\n",
      "213:\tlearn: 1718.3023462\ttotal: 6.03s\tremaining: 1.01s\n",
      "214:\tlearn: 1718.1145053\ttotal: 6.05s\tremaining: 986ms\n",
      "215:\tlearn: 1717.4299591\ttotal: 6.08s\tremaining: 958ms\n",
      "216:\tlearn: 1717.1841795\ttotal: 6.12s\tremaining: 930ms\n",
      "217:\tlearn: 1716.7211417\ttotal: 6.15s\tremaining: 903ms\n",
      "218:\tlearn: 1716.2419895\ttotal: 6.19s\tremaining: 876ms\n",
      "219:\tlearn: 1715.2052245\ttotal: 6.23s\tremaining: 849ms\n",
      "220:\tlearn: 1713.5979411\ttotal: 6.28s\tremaining: 824ms\n",
      "221:\tlearn: 1712.2409278\ttotal: 6.36s\tremaining: 802ms\n",
      "222:\tlearn: 1711.6432171\ttotal: 6.39s\tremaining: 774ms\n",
      "223:\tlearn: 1710.8345339\ttotal: 6.45s\tremaining: 748ms\n",
      "224:\tlearn: 1710.1916648\ttotal: 6.48s\tremaining: 720ms\n",
      "225:\tlearn: 1709.9575223\ttotal: 6.51s\tremaining: 691ms\n",
      "226:\tlearn: 1709.8076346\ttotal: 6.54s\tremaining: 663ms\n",
      "227:\tlearn: 1709.6567417\ttotal: 6.56s\tremaining: 633ms\n",
      "228:\tlearn: 1709.4023408\ttotal: 6.59s\tremaining: 605ms\n",
      "229:\tlearn: 1708.7113548\ttotal: 6.62s\tremaining: 576ms\n",
      "230:\tlearn: 1708.1181802\ttotal: 6.64s\tremaining: 547ms\n",
      "231:\tlearn: 1707.4718497\ttotal: 6.67s\tremaining: 517ms\n",
      "232:\tlearn: 1707.1352159\ttotal: 6.7s\tremaining: 489ms\n",
      "233:\tlearn: 1707.0272548\ttotal: 6.73s\tremaining: 460ms\n",
      "234:\tlearn: 1706.5767452\ttotal: 6.76s\tremaining: 432ms\n",
      "235:\tlearn: 1705.9921645\ttotal: 6.79s\tremaining: 403ms\n",
      "236:\tlearn: 1705.9035183\ttotal: 6.82s\tremaining: 374ms\n",
      "237:\tlearn: 1705.7291071\ttotal: 6.84s\tremaining: 345ms\n",
      "238:\tlearn: 1705.5660880\ttotal: 6.87s\tremaining: 316ms\n",
      "239:\tlearn: 1705.2781579\ttotal: 6.89s\tremaining: 287ms\n",
      "240:\tlearn: 1704.6562874\ttotal: 6.92s\tremaining: 258ms\n",
      "241:\tlearn: 1704.1795956\ttotal: 6.95s\tremaining: 230ms\n",
      "242:\tlearn: 1703.4430933\ttotal: 6.97s\tremaining: 201ms\n",
      "243:\tlearn: 1702.7825989\ttotal: 6.99s\tremaining: 172ms\n",
      "244:\tlearn: 1702.0360925\ttotal: 7.01s\tremaining: 143ms\n",
      "245:\tlearn: 1701.6000410\ttotal: 7.04s\tremaining: 114ms\n",
      "246:\tlearn: 1701.2801668\ttotal: 7.07s\tremaining: 85.9ms\n",
      "247:\tlearn: 1701.2243137\ttotal: 7.1s\tremaining: 57.3ms\n",
      "248:\tlearn: 1701.0676401\ttotal: 7.13s\tremaining: 28.7ms\n",
      "249:\tlearn: 1700.3811572\ttotal: 7.16s\tremaining: 0us\n",
      "[CV] END .....................................iterations=250; total time=   7.4s\n",
      "Learning rate set to 0.294667\n",
      "0:\tlearn: 3726.4822065\ttotal: 31ms\tremaining: 7.73s\n",
      "1:\tlearn: 3192.4371511\ttotal: 66.5ms\tremaining: 8.24s\n",
      "2:\tlearn: 2834.1380924\ttotal: 99.4ms\tremaining: 8.18s\n",
      "3:\tlearn: 2592.9364633\ttotal: 137ms\tremaining: 8.43s\n",
      "4:\tlearn: 2442.8776518\ttotal: 164ms\tremaining: 8.05s\n",
      "5:\tlearn: 2325.0349249\ttotal: 197ms\tremaining: 8.03s\n",
      "6:\tlearn: 2237.9464271\ttotal: 229ms\tremaining: 7.95s\n",
      "7:\tlearn: 2182.7782181\ttotal: 258ms\tremaining: 7.81s\n",
      "8:\tlearn: 2140.2243957\ttotal: 288ms\tremaining: 7.71s\n",
      "9:\tlearn: 2108.6534168\ttotal: 315ms\tremaining: 7.55s\n",
      "10:\tlearn: 2084.4874068\ttotal: 350ms\tremaining: 7.61s\n",
      "11:\tlearn: 2063.2139066\ttotal: 372ms\tremaining: 7.37s\n",
      "12:\tlearn: 2048.7500533\ttotal: 404ms\tremaining: 7.37s\n",
      "13:\tlearn: 2038.2319824\ttotal: 451ms\tremaining: 7.6s\n",
      "14:\tlearn: 2029.6252208\ttotal: 493ms\tremaining: 7.72s\n",
      "15:\tlearn: 2021.0194726\ttotal: 531ms\tremaining: 7.76s\n",
      "16:\tlearn: 2006.8363514\ttotal: 565ms\tremaining: 7.74s\n",
      "17:\tlearn: 2001.3845635\ttotal: 603ms\tremaining: 7.77s\n",
      "18:\tlearn: 1991.4834782\ttotal: 642ms\tremaining: 7.81s\n",
      "19:\tlearn: 1983.2446262\ttotal: 668ms\tremaining: 7.68s\n",
      "20:\tlearn: 1976.9151781\ttotal: 697ms\tremaining: 7.6s\n",
      "21:\tlearn: 1971.2885764\ttotal: 721ms\tremaining: 7.48s\n",
      "22:\tlearn: 1963.5059133\ttotal: 781ms\tremaining: 7.71s\n",
      "23:\tlearn: 1958.7552636\ttotal: 812ms\tremaining: 7.64s\n",
      "24:\tlearn: 1951.6241994\ttotal: 841ms\tremaining: 7.57s\n",
      "25:\tlearn: 1946.4480739\ttotal: 871ms\tremaining: 7.51s\n",
      "26:\tlearn: 1943.7457364\ttotal: 911ms\tremaining: 7.53s\n",
      "27:\tlearn: 1939.9617850\ttotal: 955ms\tremaining: 7.58s\n",
      "28:\tlearn: 1937.0785814\ttotal: 996ms\tremaining: 7.59s\n",
      "29:\tlearn: 1932.1346289\ttotal: 1.03s\tremaining: 7.53s\n",
      "30:\tlearn: 1927.7419134\ttotal: 1.04s\tremaining: 7.37s\n",
      "31:\tlearn: 1924.0213378\ttotal: 1.08s\tremaining: 7.38s\n",
      "32:\tlearn: 1919.6499370\ttotal: 1.13s\tremaining: 7.42s\n",
      "33:\tlearn: 1916.5231277\ttotal: 1.16s\tremaining: 7.39s\n",
      "34:\tlearn: 1912.5485542\ttotal: 1.19s\tremaining: 7.31s\n",
      "35:\tlearn: 1908.6580450\ttotal: 1.22s\tremaining: 7.26s\n",
      "36:\tlearn: 1905.7123721\ttotal: 1.25s\tremaining: 7.22s\n",
      "37:\tlearn: 1902.1449332\ttotal: 1.29s\tremaining: 7.18s\n",
      "38:\tlearn: 1899.4662118\ttotal: 1.32s\tremaining: 7.16s\n",
      "39:\tlearn: 1897.0081641\ttotal: 1.35s\tremaining: 7.09s\n",
      "40:\tlearn: 1891.9706983\ttotal: 1.39s\tremaining: 7.06s\n",
      "41:\tlearn: 1888.4562170\ttotal: 1.43s\tremaining: 7.07s\n",
      "42:\tlearn: 1887.5018759\ttotal: 1.46s\tremaining: 7.05s\n",
      "43:\tlearn: 1884.8042735\ttotal: 1.5s\tremaining: 7.02s\n",
      "44:\tlearn: 1881.9184820\ttotal: 1.53s\tremaining: 6.96s\n",
      "45:\tlearn: 1879.3137817\ttotal: 1.56s\tremaining: 6.91s\n",
      "46:\tlearn: 1877.2376069\ttotal: 1.63s\tremaining: 7.05s\n",
      "47:\tlearn: 1874.7922984\ttotal: 1.67s\tremaining: 7.02s\n",
      "48:\tlearn: 1872.3514378\ttotal: 1.71s\tremaining: 7s\n",
      "49:\tlearn: 1870.2083593\ttotal: 1.74s\tremaining: 6.97s\n",
      "50:\tlearn: 1867.0880568\ttotal: 1.77s\tremaining: 6.92s\n",
      "51:\tlearn: 1865.3938015\ttotal: 1.81s\tremaining: 6.88s\n",
      "52:\tlearn: 1863.6277904\ttotal: 1.84s\tremaining: 6.83s\n",
      "53:\tlearn: 1861.5501100\ttotal: 1.86s\tremaining: 6.77s\n",
      "54:\tlearn: 1860.3107500\ttotal: 1.9s\tremaining: 6.72s\n",
      "55:\tlearn: 1858.4347514\ttotal: 1.93s\tremaining: 6.67s\n",
      "56:\tlearn: 1855.9373717\ttotal: 1.96s\tremaining: 6.63s\n",
      "57:\tlearn: 1853.4912722\ttotal: 1.98s\tremaining: 6.56s\n",
      "58:\tlearn: 1851.8021673\ttotal: 2.02s\tremaining: 6.54s\n",
      "59:\tlearn: 1850.0965589\ttotal: 2.06s\tremaining: 6.52s\n",
      "60:\tlearn: 1848.2102195\ttotal: 2.08s\tremaining: 6.44s\n",
      "61:\tlearn: 1845.5446973\ttotal: 2.12s\tremaining: 6.41s\n",
      "62:\tlearn: 1844.6649589\ttotal: 2.16s\tremaining: 6.41s\n",
      "63:\tlearn: 1842.8431138\ttotal: 2.19s\tremaining: 6.38s\n",
      "64:\tlearn: 1841.6190448\ttotal: 2.23s\tremaining: 6.36s\n",
      "65:\tlearn: 1839.8548285\ttotal: 2.27s\tremaining: 6.33s\n",
      "66:\tlearn: 1838.9752267\ttotal: 2.3s\tremaining: 6.29s\n",
      "67:\tlearn: 1838.0080616\ttotal: 2.33s\tremaining: 6.25s\n",
      "68:\tlearn: 1836.5536840\ttotal: 2.36s\tremaining: 6.2s\n",
      "69:\tlearn: 1833.9989791\ttotal: 2.39s\tremaining: 6.16s\n",
      "70:\tlearn: 1831.9696774\ttotal: 2.43s\tremaining: 6.13s\n",
      "71:\tlearn: 1831.1639520\ttotal: 2.49s\tremaining: 6.16s\n",
      "72:\tlearn: 1829.2829800\ttotal: 2.52s\tremaining: 6.12s\n",
      "73:\tlearn: 1828.4676712\ttotal: 2.56s\tremaining: 6.09s\n",
      "74:\tlearn: 1827.1406991\ttotal: 2.59s\tremaining: 6.05s\n",
      "75:\tlearn: 1825.8178999\ttotal: 2.62s\tremaining: 6.01s\n",
      "76:\tlearn: 1824.2971158\ttotal: 2.66s\tremaining: 5.97s\n",
      "77:\tlearn: 1822.4999212\ttotal: 2.69s\tremaining: 5.94s\n",
      "78:\tlearn: 1821.2629382\ttotal: 2.73s\tremaining: 5.91s\n",
      "79:\tlearn: 1819.9622770\ttotal: 2.76s\tremaining: 5.87s\n",
      "80:\tlearn: 1819.1771363\ttotal: 2.79s\tremaining: 5.83s\n",
      "81:\tlearn: 1817.5296303\ttotal: 2.82s\tremaining: 5.78s\n",
      "82:\tlearn: 1815.9355878\ttotal: 2.85s\tremaining: 5.74s\n",
      "83:\tlearn: 1814.8224729\ttotal: 2.89s\tremaining: 5.71s\n",
      "84:\tlearn: 1813.5214084\ttotal: 2.92s\tremaining: 5.68s\n",
      "85:\tlearn: 1812.2431993\ttotal: 2.95s\tremaining: 5.63s\n",
      "86:\tlearn: 1811.4256431\ttotal: 2.99s\tremaining: 5.6s\n",
      "87:\tlearn: 1809.5791261\ttotal: 3.02s\tremaining: 5.55s\n",
      "88:\tlearn: 1807.4721626\ttotal: 3.05s\tremaining: 5.52s\n",
      "89:\tlearn: 1805.8722807\ttotal: 3.08s\tremaining: 5.48s\n",
      "90:\tlearn: 1804.6611086\ttotal: 3.11s\tremaining: 5.44s\n",
      "91:\tlearn: 1803.4816952\ttotal: 3.15s\tremaining: 5.4s\n",
      "92:\tlearn: 1802.7829694\ttotal: 3.17s\tremaining: 5.36s\n",
      "93:\tlearn: 1802.0663760\ttotal: 3.21s\tremaining: 5.32s\n",
      "94:\tlearn: 1800.6178384\ttotal: 3.23s\tremaining: 5.27s\n",
      "95:\tlearn: 1799.5383352\ttotal: 3.26s\tremaining: 5.23s\n",
      "96:\tlearn: 1798.4680167\ttotal: 3.29s\tremaining: 5.19s\n",
      "97:\tlearn: 1797.7124672\ttotal: 3.32s\tremaining: 5.15s\n",
      "98:\tlearn: 1796.4197716\ttotal: 3.39s\tremaining: 5.17s\n",
      "99:\tlearn: 1795.4320744\ttotal: 3.42s\tremaining: 5.13s\n",
      "100:\tlearn: 1794.1931867\ttotal: 3.45s\tremaining: 5.09s\n",
      "101:\tlearn: 1792.6858540\ttotal: 3.47s\tremaining: 5.04s\n",
      "102:\tlearn: 1792.3191050\ttotal: 3.52s\tremaining: 5.02s\n",
      "103:\tlearn: 1791.0809662\ttotal: 3.55s\tremaining: 4.98s\n",
      "104:\tlearn: 1790.1011691\ttotal: 3.58s\tremaining: 4.94s\n",
      "105:\tlearn: 1789.1860508\ttotal: 3.6s\tremaining: 4.9s\n",
      "106:\tlearn: 1788.6180017\ttotal: 3.63s\tremaining: 4.86s\n",
      "107:\tlearn: 1787.9671605\ttotal: 3.67s\tremaining: 4.82s\n",
      "108:\tlearn: 1787.8205629\ttotal: 3.7s\tremaining: 4.79s\n",
      "109:\tlearn: 1786.8144496\ttotal: 3.74s\tremaining: 4.76s\n",
      "110:\tlearn: 1786.4869595\ttotal: 3.78s\tremaining: 4.73s\n",
      "111:\tlearn: 1785.7981705\ttotal: 3.81s\tremaining: 4.7s\n",
      "112:\tlearn: 1785.1636773\ttotal: 3.85s\tremaining: 4.66s\n",
      "113:\tlearn: 1784.4384796\ttotal: 3.88s\tremaining: 4.63s\n",
      "114:\tlearn: 1783.4296409\ttotal: 3.91s\tremaining: 4.59s\n",
      "115:\tlearn: 1782.5197877\ttotal: 3.94s\tremaining: 4.56s\n",
      "116:\tlearn: 1781.7201587\ttotal: 3.98s\tremaining: 4.52s\n",
      "117:\tlearn: 1780.8412201\ttotal: 4.01s\tremaining: 4.49s\n",
      "118:\tlearn: 1779.6553449\ttotal: 4.04s\tremaining: 4.45s\n",
      "119:\tlearn: 1779.3691822\ttotal: 4.08s\tremaining: 4.42s\n",
      "120:\tlearn: 1778.5028709\ttotal: 4.1s\tremaining: 4.37s\n",
      "121:\tlearn: 1777.5213531\ttotal: 4.13s\tremaining: 4.33s\n",
      "122:\tlearn: 1776.6024465\ttotal: 4.17s\tremaining: 4.3s\n",
      "123:\tlearn: 1775.2425233\ttotal: 4.2s\tremaining: 4.27s\n",
      "124:\tlearn: 1775.0185840\ttotal: 4.27s\tremaining: 4.27s\n",
      "125:\tlearn: 1774.2529252\ttotal: 4.3s\tremaining: 4.23s\n",
      "126:\tlearn: 1773.3502940\ttotal: 4.33s\tremaining: 4.2s\n",
      "127:\tlearn: 1772.7212423\ttotal: 4.37s\tremaining: 4.17s\n",
      "128:\tlearn: 1772.3802685\ttotal: 4.4s\tremaining: 4.13s\n",
      "129:\tlearn: 1771.6658826\ttotal: 4.43s\tremaining: 4.09s\n",
      "130:\tlearn: 1770.8376132\ttotal: 4.47s\tremaining: 4.06s\n",
      "131:\tlearn: 1769.7426084\ttotal: 4.5s\tremaining: 4.02s\n",
      "132:\tlearn: 1768.6805331\ttotal: 4.53s\tremaining: 3.99s\n",
      "133:\tlearn: 1768.0758658\ttotal: 4.57s\tremaining: 3.95s\n",
      "134:\tlearn: 1767.8011621\ttotal: 4.6s\tremaining: 3.92s\n",
      "135:\tlearn: 1766.4992098\ttotal: 4.63s\tremaining: 3.88s\n",
      "136:\tlearn: 1765.2374065\ttotal: 4.66s\tremaining: 3.84s\n",
      "137:\tlearn: 1764.5881758\ttotal: 4.7s\tremaining: 3.81s\n",
      "138:\tlearn: 1763.7821878\ttotal: 4.73s\tremaining: 3.77s\n",
      "139:\tlearn: 1763.3367219\ttotal: 4.76s\tremaining: 3.74s\n",
      "140:\tlearn: 1762.5138012\ttotal: 4.79s\tremaining: 3.7s\n",
      "141:\tlearn: 1761.5503408\ttotal: 4.82s\tremaining: 3.66s\n",
      "142:\tlearn: 1760.5340205\ttotal: 4.84s\tremaining: 3.62s\n",
      "143:\tlearn: 1759.7664050\ttotal: 4.87s\tremaining: 3.59s\n",
      "144:\tlearn: 1759.2199411\ttotal: 4.92s\tremaining: 3.56s\n",
      "145:\tlearn: 1758.9255357\ttotal: 4.96s\tremaining: 3.53s\n",
      "146:\tlearn: 1758.5423466\ttotal: 5s\tremaining: 3.5s\n",
      "147:\tlearn: 1757.7059410\ttotal: 5.03s\tremaining: 3.47s\n",
      "148:\tlearn: 1757.3323751\ttotal: 5.06s\tremaining: 3.43s\n",
      "149:\tlearn: 1756.4452424\ttotal: 5.09s\tremaining: 3.4s\n",
      "150:\tlearn: 1755.9767843\ttotal: 5.13s\tremaining: 3.36s\n",
      "151:\tlearn: 1755.3488533\ttotal: 5.16s\tremaining: 3.33s\n",
      "152:\tlearn: 1754.9639085\ttotal: 5.19s\tremaining: 3.29s\n",
      "153:\tlearn: 1754.8688319\ttotal: 5.23s\tremaining: 3.26s\n",
      "154:\tlearn: 1754.2836174\ttotal: 5.27s\tremaining: 3.23s\n",
      "155:\tlearn: 1753.3534651\ttotal: 5.3s\tremaining: 3.19s\n",
      "156:\tlearn: 1752.6262182\ttotal: 5.33s\tremaining: 3.16s\n",
      "157:\tlearn: 1752.3985097\ttotal: 5.37s\tremaining: 3.12s\n",
      "158:\tlearn: 1751.7301442\ttotal: 5.4s\tremaining: 3.09s\n",
      "159:\tlearn: 1751.5675330\ttotal: 5.44s\tremaining: 3.06s\n",
      "160:\tlearn: 1751.2995726\ttotal: 5.47s\tremaining: 3.02s\n",
      "161:\tlearn: 1750.8840827\ttotal: 5.51s\tremaining: 2.99s\n",
      "162:\tlearn: 1750.6476476\ttotal: 5.54s\tremaining: 2.96s\n",
      "163:\tlearn: 1750.4036484\ttotal: 5.6s\tremaining: 2.94s\n",
      "164:\tlearn: 1750.1803442\ttotal: 5.63s\tremaining: 2.9s\n",
      "165:\tlearn: 1750.1098943\ttotal: 5.65s\tremaining: 2.86s\n",
      "166:\tlearn: 1749.6655509\ttotal: 5.69s\tremaining: 2.83s\n",
      "167:\tlearn: 1748.6593099\ttotal: 5.72s\tremaining: 2.79s\n",
      "168:\tlearn: 1748.2882706\ttotal: 5.75s\tremaining: 2.75s\n",
      "169:\tlearn: 1747.9538739\ttotal: 5.79s\tremaining: 2.72s\n",
      "170:\tlearn: 1747.8980931\ttotal: 5.82s\tremaining: 2.69s\n",
      "171:\tlearn: 1747.0003154\ttotal: 5.84s\tremaining: 2.65s\n",
      "172:\tlearn: 1746.2123571\ttotal: 5.87s\tremaining: 2.61s\n",
      "173:\tlearn: 1745.7317197\ttotal: 5.92s\tremaining: 2.59s\n",
      "174:\tlearn: 1745.1193526\ttotal: 5.96s\tremaining: 2.55s\n",
      "175:\tlearn: 1744.5143749\ttotal: 6s\tremaining: 2.52s\n",
      "176:\tlearn: 1744.1744922\ttotal: 6.04s\tremaining: 2.49s\n",
      "177:\tlearn: 1743.3432496\ttotal: 6.07s\tremaining: 2.45s\n",
      "178:\tlearn: 1742.8445762\ttotal: 6.09s\tremaining: 2.42s\n",
      "179:\tlearn: 1742.6098806\ttotal: 6.12s\tremaining: 2.38s\n",
      "180:\tlearn: 1741.8444314\ttotal: 6.16s\tremaining: 2.35s\n",
      "181:\tlearn: 1740.5395666\ttotal: 6.21s\tremaining: 2.32s\n",
      "182:\tlearn: 1740.0527326\ttotal: 6.24s\tremaining: 2.28s\n",
      "183:\tlearn: 1739.1841764\ttotal: 6.26s\tremaining: 2.25s\n",
      "184:\tlearn: 1738.6236762\ttotal: 6.29s\tremaining: 2.21s\n",
      "185:\tlearn: 1738.2569271\ttotal: 6.33s\tremaining: 2.18s\n",
      "186:\tlearn: 1737.3263153\ttotal: 6.36s\tremaining: 2.14s\n",
      "187:\tlearn: 1736.7439358\ttotal: 6.39s\tremaining: 2.11s\n",
      "188:\tlearn: 1736.3602341\ttotal: 6.42s\tremaining: 2.07s\n",
      "189:\tlearn: 1735.8296524\ttotal: 6.45s\tremaining: 2.04s\n",
      "190:\tlearn: 1734.9798432\ttotal: 6.48s\tremaining: 2s\n",
      "191:\tlearn: 1734.3697193\ttotal: 6.51s\tremaining: 1.97s\n",
      "192:\tlearn: 1734.1781748\ttotal: 6.54s\tremaining: 1.93s\n",
      "193:\tlearn: 1733.6284622\ttotal: 6.59s\tremaining: 1.9s\n",
      "194:\tlearn: 1733.1552675\ttotal: 6.62s\tremaining: 1.87s\n",
      "195:\tlearn: 1732.7965103\ttotal: 6.66s\tremaining: 1.83s\n",
      "196:\tlearn: 1732.7718472\ttotal: 6.69s\tremaining: 1.8s\n",
      "197:\tlearn: 1732.1317769\ttotal: 6.72s\tremaining: 1.76s\n",
      "198:\tlearn: 1731.4704001\ttotal: 6.76s\tremaining: 1.73s\n",
      "199:\tlearn: 1730.9241206\ttotal: 6.83s\tremaining: 1.71s\n",
      "200:\tlearn: 1730.6790735\ttotal: 6.86s\tremaining: 1.67s\n",
      "201:\tlearn: 1729.5294318\ttotal: 6.89s\tremaining: 1.64s\n",
      "202:\tlearn: 1728.8816116\ttotal: 6.92s\tremaining: 1.6s\n",
      "203:\tlearn: 1728.4742889\ttotal: 6.96s\tremaining: 1.57s\n",
      "204:\tlearn: 1728.1909100\ttotal: 7s\tremaining: 1.54s\n",
      "205:\tlearn: 1727.3938101\ttotal: 7.04s\tremaining: 1.5s\n",
      "206:\tlearn: 1726.8817129\ttotal: 7.08s\tremaining: 1.47s\n",
      "207:\tlearn: 1726.2030065\ttotal: 7.12s\tremaining: 1.44s\n",
      "208:\tlearn: 1726.0868973\ttotal: 7.15s\tremaining: 1.4s\n",
      "209:\tlearn: 1725.3799328\ttotal: 7.18s\tremaining: 1.37s\n",
      "210:\tlearn: 1724.8238133\ttotal: 7.21s\tremaining: 1.33s\n",
      "211:\tlearn: 1724.3500465\ttotal: 7.25s\tremaining: 1.3s\n",
      "212:\tlearn: 1723.8053893\ttotal: 7.29s\tremaining: 1.26s\n",
      "213:\tlearn: 1723.6124210\ttotal: 7.32s\tremaining: 1.23s\n",
      "214:\tlearn: 1723.5125461\ttotal: 7.35s\tremaining: 1.2s\n",
      "215:\tlearn: 1722.8034331\ttotal: 7.38s\tremaining: 1.16s\n",
      "216:\tlearn: 1722.7089391\ttotal: 7.42s\tremaining: 1.13s\n",
      "217:\tlearn: 1721.9281952\ttotal: 7.46s\tremaining: 1.09s\n",
      "218:\tlearn: 1721.5383591\ttotal: 7.51s\tremaining: 1.06s\n",
      "219:\tlearn: 1721.4450134\ttotal: 7.54s\tremaining: 1.03s\n",
      "220:\tlearn: 1721.3911185\ttotal: 7.57s\tremaining: 994ms\n",
      "221:\tlearn: 1720.8735681\ttotal: 7.6s\tremaining: 958ms\n",
      "222:\tlearn: 1720.3127427\ttotal: 7.63s\tremaining: 924ms\n",
      "223:\tlearn: 1719.7275349\ttotal: 7.66s\tremaining: 889ms\n",
      "224:\tlearn: 1719.6980614\ttotal: 7.69s\tremaining: 855ms\n",
      "225:\tlearn: 1718.8503234\ttotal: 7.72s\tremaining: 820ms\n",
      "226:\tlearn: 1718.8298804\ttotal: 7.76s\tremaining: 786ms\n",
      "227:\tlearn: 1718.4404769\ttotal: 7.79s\tremaining: 752ms\n",
      "228:\tlearn: 1718.4236015\ttotal: 7.83s\tremaining: 718ms\n",
      "229:\tlearn: 1717.9523425\ttotal: 7.86s\tremaining: 683ms\n",
      "230:\tlearn: 1717.3456879\ttotal: 7.89s\tremaining: 649ms\n",
      "231:\tlearn: 1717.1745049\ttotal: 7.97s\tremaining: 619ms\n",
      "232:\tlearn: 1716.8269187\ttotal: 8.03s\tremaining: 586ms\n",
      "233:\tlearn: 1716.2101972\ttotal: 8.06s\tremaining: 551ms\n",
      "234:\tlearn: 1715.7227716\ttotal: 8.09s\tremaining: 517ms\n",
      "235:\tlearn: 1715.4819534\ttotal: 8.15s\tremaining: 484ms\n",
      "236:\tlearn: 1715.0438741\ttotal: 8.18s\tremaining: 449ms\n",
      "237:\tlearn: 1714.9112040\ttotal: 8.21s\tremaining: 414ms\n",
      "238:\tlearn: 1714.6631894\ttotal: 8.25s\tremaining: 380ms\n",
      "239:\tlearn: 1713.7107021\ttotal: 8.28s\tremaining: 345ms\n",
      "240:\tlearn: 1713.6531724\ttotal: 8.32s\tremaining: 311ms\n",
      "241:\tlearn: 1713.0784278\ttotal: 8.35s\tremaining: 276ms\n",
      "242:\tlearn: 1712.3645480\ttotal: 8.39s\tremaining: 242ms\n",
      "243:\tlearn: 1712.0125899\ttotal: 8.43s\tremaining: 207ms\n",
      "244:\tlearn: 1711.6113412\ttotal: 8.47s\tremaining: 173ms\n",
      "245:\tlearn: 1711.3030509\ttotal: 8.5s\tremaining: 138ms\n",
      "246:\tlearn: 1710.9141016\ttotal: 8.55s\tremaining: 104ms\n",
      "247:\tlearn: 1710.3153897\ttotal: 8.59s\tremaining: 69.3ms\n",
      "248:\tlearn: 1709.9447963\ttotal: 8.62s\tremaining: 34.6ms\n",
      "249:\tlearn: 1709.7083293\ttotal: 8.67s\tremaining: 0us\n",
      "Best RMSE score: 1781.58\n",
      "Best params: {'iterations': 250}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid_cb = {\n",
    "    'iterations': [10, 100, 250]\n",
    "}\n",
    "\n",
    "model_cb = CatBoostRegressor(\n",
    "    cat_features=cat_features,\n",
    "    random_state=12345\n",
    ")\n",
    "\n",
    "print('CatBoost:')\n",
    "\n",
    "tune_hyperparameters(model_cb, param_grid_cb, features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost:\n",
      "Learning rate set to 0.275108\n",
      "0:\tlearn: 3774.2745985\ttest: 3767.5888093\tbest: 3767.5888093 (0)\ttotal: 37.2ms\tremaining: 9.25s\n",
      "1:\tlearn: 3257.2948457\ttest: 3257.6338768\tbest: 3257.6338768 (1)\ttotal: 91.4ms\tremaining: 11.3s\n",
      "2:\tlearn: 2897.6229187\ttest: 2904.1276593\tbest: 2904.1276593 (2)\ttotal: 121ms\tremaining: 9.94s\n",
      "3:\tlearn: 2654.4328900\ttest: 2665.8977588\tbest: 2665.8977588 (3)\ttotal: 160ms\tremaining: 9.86s\n",
      "4:\tlearn: 2487.1841958\ttest: 2502.8236496\tbest: 2502.8236496 (4)\ttotal: 190ms\tremaining: 9.33s\n",
      "5:\tlearn: 2362.3345281\ttest: 2379.4401809\tbest: 2379.4401809 (5)\ttotal: 233ms\tremaining: 9.47s\n",
      "6:\tlearn: 2271.8334402\ttest: 2289.5691800\tbest: 2289.5691800 (6)\ttotal: 270ms\tremaining: 9.36s\n",
      "7:\tlearn: 2205.1950824\ttest: 2224.9378156\tbest: 2224.9378156 (7)\ttotal: 312ms\tremaining: 9.44s\n",
      "8:\tlearn: 2159.9034517\ttest: 2182.0635288\tbest: 2182.0635288 (8)\ttotal: 349ms\tremaining: 9.35s\n",
      "9:\tlearn: 2126.8585494\ttest: 2148.3768550\tbest: 2148.3768550 (9)\ttotal: 384ms\tremaining: 9.21s\n",
      "10:\tlearn: 2096.2940880\ttest: 2118.4703812\tbest: 2118.4703812 (10)\ttotal: 416ms\tremaining: 9.04s\n",
      "11:\tlearn: 2074.9190074\ttest: 2096.8849930\tbest: 2096.8849930 (11)\ttotal: 460ms\tremaining: 9.13s\n",
      "12:\tlearn: 2059.5436371\ttest: 2081.3620171\tbest: 2081.3620171 (12)\ttotal: 505ms\tremaining: 9.21s\n",
      "13:\tlearn: 2046.5616045\ttest: 2068.0993004\tbest: 2068.0993004 (13)\ttotal: 536ms\tremaining: 9.04s\n",
      "14:\tlearn: 2035.5001080\ttest: 2056.3203752\tbest: 2056.3203752 (14)\ttotal: 572ms\tremaining: 8.96s\n",
      "15:\tlearn: 2024.3839850\ttest: 2045.7965320\tbest: 2045.7965320 (15)\ttotal: 610ms\tremaining: 8.93s\n",
      "16:\tlearn: 2015.9077474\ttest: 2037.2544639\tbest: 2037.2544639 (16)\ttotal: 658ms\tremaining: 9.02s\n",
      "17:\tlearn: 2003.6520378\ttest: 2025.8207872\tbest: 2025.8207872 (17)\ttotal: 691ms\tremaining: 8.9s\n",
      "18:\tlearn: 1996.6182945\ttest: 2019.3020801\tbest: 2019.3020801 (18)\ttotal: 740ms\tremaining: 8.99s\n",
      "19:\tlearn: 1988.3200896\ttest: 2011.8906681\tbest: 2011.8906681 (19)\ttotal: 774ms\tremaining: 8.9s\n",
      "20:\tlearn: 1982.8008339\ttest: 2006.8597063\tbest: 2006.8597063 (20)\ttotal: 801ms\tremaining: 8.74s\n",
      "21:\tlearn: 1978.3469138\ttest: 2002.8137853\tbest: 2002.8137853 (21)\ttotal: 837ms\tremaining: 8.67s\n",
      "22:\tlearn: 1970.7891419\ttest: 1995.2637727\tbest: 1995.2637727 (22)\ttotal: 877ms\tremaining: 8.65s\n",
      "23:\tlearn: 1967.0599279\ttest: 1991.3508302\tbest: 1991.3508302 (23)\ttotal: 920ms\tremaining: 8.66s\n",
      "24:\tlearn: 1961.4864464\ttest: 1985.1898003\tbest: 1985.1898003 (24)\ttotal: 963ms\tremaining: 8.67s\n",
      "25:\tlearn: 1956.3582241\ttest: 1980.2076683\tbest: 1980.2076683 (25)\ttotal: 1s\tremaining: 8.64s\n",
      "26:\tlearn: 1949.4586485\ttest: 1972.5366252\tbest: 1972.5366252 (26)\ttotal: 1.04s\tremaining: 8.58s\n",
      "27:\tlearn: 1944.2003233\ttest: 1966.6318401\tbest: 1966.6318401 (27)\ttotal: 1.06s\tremaining: 8.43s\n",
      "28:\tlearn: 1942.3780695\ttest: 1965.1039642\tbest: 1965.1039642 (28)\ttotal: 1.1s\tremaining: 8.4s\n",
      "29:\tlearn: 1939.0757841\ttest: 1962.0443722\tbest: 1962.0443722 (29)\ttotal: 1.14s\tremaining: 8.33s\n",
      "30:\tlearn: 1934.4002556\ttest: 1957.1763664\tbest: 1957.1763664 (30)\ttotal: 1.18s\tremaining: 8.32s\n",
      "31:\tlearn: 1930.5329543\ttest: 1953.8259256\tbest: 1953.8259256 (31)\ttotal: 1.21s\tremaining: 8.26s\n",
      "32:\tlearn: 1925.5313543\ttest: 1947.8368840\tbest: 1947.8368840 (32)\ttotal: 1.25s\tremaining: 8.22s\n",
      "33:\tlearn: 1922.3512524\ttest: 1945.1689916\tbest: 1945.1689916 (33)\ttotal: 1.29s\tremaining: 8.18s\n",
      "34:\tlearn: 1918.1885364\ttest: 1940.8526670\tbest: 1940.8526670 (34)\ttotal: 1.33s\tremaining: 8.15s\n",
      "35:\tlearn: 1913.6847184\ttest: 1937.3460063\tbest: 1937.3460063 (35)\ttotal: 1.37s\tremaining: 8.15s\n",
      "36:\tlearn: 1910.6184690\ttest: 1936.0381484\tbest: 1936.0381484 (36)\ttotal: 1.41s\tremaining: 8.14s\n",
      "37:\tlearn: 1907.6791423\ttest: 1933.3475980\tbest: 1933.3475980 (37)\ttotal: 1.45s\tremaining: 8.07s\n",
      "38:\tlearn: 1903.3829375\ttest: 1929.1680393\tbest: 1929.1680393 (38)\ttotal: 1.49s\tremaining: 8.04s\n",
      "39:\tlearn: 1900.1004360\ttest: 1926.1323869\tbest: 1926.1323869 (39)\ttotal: 1.52s\tremaining: 8.01s\n",
      "40:\tlearn: 1898.3973371\ttest: 1924.8825569\tbest: 1924.8825569 (40)\ttotal: 1.56s\tremaining: 7.97s\n",
      "41:\tlearn: 1895.4194116\ttest: 1922.0183814\tbest: 1922.0183814 (41)\ttotal: 1.61s\tremaining: 7.96s\n",
      "42:\tlearn: 1892.8418061\ttest: 1919.6905260\tbest: 1919.6905260 (42)\ttotal: 1.64s\tremaining: 7.89s\n",
      "43:\tlearn: 1890.2435208\ttest: 1917.1976677\tbest: 1917.1976677 (43)\ttotal: 1.67s\tremaining: 7.8s\n",
      "44:\tlearn: 1887.8328415\ttest: 1915.0568409\tbest: 1915.0568409 (44)\ttotal: 1.7s\tremaining: 7.74s\n",
      "45:\tlearn: 1886.4732197\ttest: 1913.9309358\tbest: 1913.9309358 (45)\ttotal: 1.73s\tremaining: 7.67s\n",
      "46:\tlearn: 1883.3619565\ttest: 1910.9460303\tbest: 1910.9460303 (46)\ttotal: 1.76s\tremaining: 7.61s\n",
      "47:\tlearn: 1880.2662349\ttest: 1907.4449435\tbest: 1907.4449435 (47)\ttotal: 1.8s\tremaining: 7.57s\n",
      "48:\tlearn: 1877.6836473\ttest: 1904.9506379\tbest: 1904.9506379 (48)\ttotal: 1.83s\tremaining: 7.53s\n",
      "49:\tlearn: 1873.5650143\ttest: 1902.7235183\tbest: 1902.7235183 (49)\ttotal: 1.87s\tremaining: 7.48s\n",
      "50:\tlearn: 1871.7662307\ttest: 1900.6341950\tbest: 1900.6341950 (50)\ttotal: 1.92s\tremaining: 7.49s\n",
      "51:\tlearn: 1870.2082805\ttest: 1899.5568073\tbest: 1899.5568073 (51)\ttotal: 1.96s\tremaining: 7.48s\n",
      "52:\tlearn: 1866.7947283\ttest: 1896.0209408\tbest: 1896.0209408 (52)\ttotal: 2s\tremaining: 7.42s\n",
      "53:\tlearn: 1864.9431815\ttest: 1894.9873000\tbest: 1894.9873000 (53)\ttotal: 2.03s\tremaining: 7.37s\n",
      "54:\tlearn: 1863.2417323\ttest: 1892.8754670\tbest: 1892.8754670 (54)\ttotal: 2.07s\tremaining: 7.34s\n",
      "55:\tlearn: 1860.4137509\ttest: 1889.8913035\tbest: 1889.8913035 (55)\ttotal: 2.11s\tremaining: 7.31s\n",
      "56:\tlearn: 1858.5194260\ttest: 1888.7069717\tbest: 1888.7069717 (56)\ttotal: 2.14s\tremaining: 7.25s\n",
      "57:\tlearn: 1856.3369205\ttest: 1886.8632307\tbest: 1886.8632307 (57)\ttotal: 2.18s\tremaining: 7.23s\n",
      "58:\tlearn: 1855.0023438\ttest: 1885.9703907\tbest: 1885.9703907 (58)\ttotal: 2.23s\tremaining: 7.21s\n",
      "59:\tlearn: 1852.8642421\ttest: 1883.8724175\tbest: 1883.8724175 (59)\ttotal: 2.26s\tremaining: 7.15s\n",
      "60:\tlearn: 1850.6072771\ttest: 1881.7404160\tbest: 1881.7404160 (60)\ttotal: 2.28s\tremaining: 7.08s\n",
      "61:\tlearn: 1848.4356396\ttest: 1880.0183499\tbest: 1880.0183499 (61)\ttotal: 2.32s\tremaining: 7.03s\n",
      "62:\tlearn: 1847.7140172\ttest: 1879.7582059\tbest: 1879.7582059 (62)\ttotal: 2.35s\tremaining: 6.99s\n",
      "63:\tlearn: 1846.3287030\ttest: 1878.0251942\tbest: 1878.0251942 (63)\ttotal: 2.39s\tremaining: 6.95s\n",
      "64:\tlearn: 1844.1771471\ttest: 1875.9999810\tbest: 1875.9999810 (64)\ttotal: 2.44s\tremaining: 6.94s\n",
      "65:\tlearn: 1842.4984629\ttest: 1874.4133094\tbest: 1874.4133094 (65)\ttotal: 2.54s\tremaining: 7.08s\n",
      "66:\tlearn: 1841.0173398\ttest: 1872.9914835\tbest: 1872.9914835 (66)\ttotal: 2.63s\tremaining: 7.17s\n",
      "67:\tlearn: 1840.2364302\ttest: 1872.4098389\tbest: 1872.4098389 (67)\ttotal: 2.68s\tremaining: 7.18s\n",
      "68:\tlearn: 1838.9624540\ttest: 1871.2489234\tbest: 1871.2489234 (68)\ttotal: 2.71s\tremaining: 7.11s\n",
      "69:\tlearn: 1837.3329462\ttest: 1869.8718407\tbest: 1869.8718407 (69)\ttotal: 2.75s\tremaining: 7.06s\n",
      "70:\tlearn: 1835.0507777\ttest: 1868.3692858\tbest: 1868.3692858 (70)\ttotal: 2.78s\tremaining: 7s\n",
      "71:\tlearn: 1833.5538184\ttest: 1867.7729326\tbest: 1867.7729326 (71)\ttotal: 2.81s\tremaining: 6.96s\n",
      "72:\tlearn: 1832.1128644\ttest: 1866.8189620\tbest: 1866.8189620 (72)\ttotal: 2.85s\tremaining: 6.92s\n",
      "73:\tlearn: 1829.9931436\ttest: 1864.6147934\tbest: 1864.6147934 (73)\ttotal: 2.89s\tremaining: 6.87s\n",
      "74:\tlearn: 1828.4702820\ttest: 1863.0617961\tbest: 1863.0617961 (74)\ttotal: 2.92s\tremaining: 6.8s\n",
      "75:\tlearn: 1828.0400146\ttest: 1862.7362815\tbest: 1862.7362815 (75)\ttotal: 2.96s\tremaining: 6.77s\n",
      "76:\tlearn: 1827.1397028\ttest: 1862.3163686\tbest: 1862.3163686 (76)\ttotal: 3s\tremaining: 6.74s\n",
      "77:\tlearn: 1825.7977508\ttest: 1860.8747590\tbest: 1860.8747590 (77)\ttotal: 3.04s\tremaining: 6.7s\n",
      "78:\tlearn: 1824.8944527\ttest: 1860.1732645\tbest: 1860.1732645 (78)\ttotal: 3.07s\tremaining: 6.64s\n",
      "79:\tlearn: 1823.9290189\ttest: 1859.4696510\tbest: 1859.4696510 (79)\ttotal: 3.11s\tremaining: 6.6s\n",
      "80:\tlearn: 1822.8713495\ttest: 1858.5267281\tbest: 1858.5267281 (80)\ttotal: 3.14s\tremaining: 6.55s\n",
      "81:\tlearn: 1821.3669208\ttest: 1857.6956258\tbest: 1857.6956258 (81)\ttotal: 3.18s\tremaining: 6.51s\n",
      "82:\tlearn: 1820.3111504\ttest: 1856.7966573\tbest: 1856.7966573 (82)\ttotal: 3.23s\tremaining: 6.49s\n",
      "83:\tlearn: 1819.1949447\ttest: 1855.5394318\tbest: 1855.5394318 (83)\ttotal: 3.26s\tremaining: 6.44s\n",
      "84:\tlearn: 1817.9565544\ttest: 1854.7088140\tbest: 1854.7088140 (84)\ttotal: 3.29s\tremaining: 6.39s\n",
      "85:\tlearn: 1816.2543916\ttest: 1853.1820228\tbest: 1853.1820228 (85)\ttotal: 3.34s\tremaining: 6.38s\n",
      "86:\tlearn: 1813.6730829\ttest: 1850.5027306\tbest: 1850.5027306 (86)\ttotal: 3.38s\tremaining: 6.33s\n",
      "87:\tlearn: 1812.3592616\ttest: 1849.0928033\tbest: 1849.0928033 (87)\ttotal: 3.42s\tremaining: 6.29s\n",
      "88:\tlearn: 1811.3035777\ttest: 1848.1790504\tbest: 1848.1790504 (88)\ttotal: 3.5s\tremaining: 6.34s\n",
      "89:\tlearn: 1809.6306363\ttest: 1847.1737407\tbest: 1847.1737407 (89)\ttotal: 3.54s\tremaining: 6.29s\n",
      "90:\tlearn: 1808.7901005\ttest: 1846.1571896\tbest: 1846.1571896 (90)\ttotal: 3.57s\tremaining: 6.24s\n",
      "91:\tlearn: 1807.4135830\ttest: 1845.0253170\tbest: 1845.0253170 (91)\ttotal: 3.61s\tremaining: 6.19s\n",
      "92:\tlearn: 1806.1351886\ttest: 1844.1131044\tbest: 1844.1131044 (92)\ttotal: 3.64s\tremaining: 6.14s\n",
      "93:\tlearn: 1805.7357062\ttest: 1843.9999546\tbest: 1843.9999546 (93)\ttotal: 3.67s\tremaining: 6.09s\n",
      "94:\tlearn: 1804.9088274\ttest: 1843.3224752\tbest: 1843.3224752 (94)\ttotal: 3.72s\tremaining: 6.06s\n",
      "95:\tlearn: 1803.4195375\ttest: 1842.4122749\tbest: 1842.4122749 (95)\ttotal: 3.75s\tremaining: 6.02s\n",
      "96:\tlearn: 1802.6457169\ttest: 1841.7056650\tbest: 1841.7056650 (96)\ttotal: 3.79s\tremaining: 5.98s\n",
      "97:\tlearn: 1801.9723443\ttest: 1841.3610184\tbest: 1841.3610184 (97)\ttotal: 3.82s\tremaining: 5.93s\n",
      "98:\tlearn: 1800.5214832\ttest: 1840.3790197\tbest: 1840.3790197 (98)\ttotal: 3.85s\tremaining: 5.88s\n",
      "99:\tlearn: 1799.4425844\ttest: 1839.7292581\tbest: 1839.7292581 (99)\ttotal: 3.89s\tremaining: 5.83s\n",
      "100:\tlearn: 1797.8025646\ttest: 1838.3853035\tbest: 1838.3853035 (100)\ttotal: 3.92s\tremaining: 5.79s\n",
      "101:\tlearn: 1796.3657459\ttest: 1836.8667629\tbest: 1836.8667629 (101)\ttotal: 3.95s\tremaining: 5.73s\n",
      "102:\tlearn: 1795.3312353\ttest: 1835.6309239\tbest: 1835.6309239 (102)\ttotal: 3.98s\tremaining: 5.68s\n",
      "103:\tlearn: 1794.5121326\ttest: 1835.0086588\tbest: 1835.0086588 (103)\ttotal: 4.03s\tremaining: 5.65s\n",
      "104:\tlearn: 1793.7882195\ttest: 1834.5148460\tbest: 1834.5148460 (104)\ttotal: 4.07s\tremaining: 5.61s\n",
      "105:\tlearn: 1793.0119555\ttest: 1834.1034742\tbest: 1834.1034742 (105)\ttotal: 4.1s\tremaining: 5.58s\n",
      "106:\tlearn: 1791.7879232\ttest: 1833.1130247\tbest: 1833.1130247 (106)\ttotal: 4.14s\tremaining: 5.53s\n",
      "107:\tlearn: 1790.9163101\ttest: 1832.6364793\tbest: 1832.6364793 (107)\ttotal: 4.17s\tremaining: 5.49s\n",
      "108:\tlearn: 1789.7468476\ttest: 1832.6581110\tbest: 1832.6364793 (107)\ttotal: 4.22s\tremaining: 5.46s\n",
      "109:\tlearn: 1788.6533889\ttest: 1831.9070492\tbest: 1831.9070492 (109)\ttotal: 4.27s\tremaining: 5.43s\n",
      "110:\tlearn: 1787.6717593\ttest: 1831.1081326\tbest: 1831.1081326 (110)\ttotal: 4.3s\tremaining: 5.38s\n",
      "111:\tlearn: 1786.8461742\ttest: 1830.4034086\tbest: 1830.4034086 (111)\ttotal: 4.35s\tremaining: 5.36s\n",
      "112:\tlearn: 1785.9595693\ttest: 1829.4766732\tbest: 1829.4766732 (112)\ttotal: 4.39s\tremaining: 5.32s\n",
      "113:\tlearn: 1785.1139093\ttest: 1828.9725145\tbest: 1828.9725145 (113)\ttotal: 4.42s\tremaining: 5.27s\n",
      "114:\tlearn: 1784.7612211\ttest: 1828.6715388\tbest: 1828.6715388 (114)\ttotal: 4.46s\tremaining: 5.24s\n",
      "115:\tlearn: 1783.8699366\ttest: 1828.0138120\tbest: 1828.0138120 (115)\ttotal: 4.49s\tremaining: 5.19s\n",
      "116:\tlearn: 1782.8446158\ttest: 1827.2317064\tbest: 1827.2317064 (116)\ttotal: 4.52s\tremaining: 5.14s\n",
      "117:\tlearn: 1782.1159088\ttest: 1826.7947819\tbest: 1826.7947819 (117)\ttotal: 4.55s\tremaining: 5.09s\n",
      "118:\tlearn: 1781.3549397\ttest: 1826.3555915\tbest: 1826.3555915 (118)\ttotal: 4.59s\tremaining: 5.05s\n",
      "119:\tlearn: 1780.9796219\ttest: 1826.3096406\tbest: 1826.3096406 (119)\ttotal: 4.62s\tremaining: 5.01s\n",
      "120:\tlearn: 1779.4447740\ttest: 1825.4691708\tbest: 1825.4691708 (120)\ttotal: 4.65s\tremaining: 4.96s\n",
      "121:\tlearn: 1779.1226232\ttest: 1825.4119517\tbest: 1825.4119517 (121)\ttotal: 4.68s\tremaining: 4.91s\n",
      "122:\tlearn: 1777.7765787\ttest: 1824.1699874\tbest: 1824.1699874 (122)\ttotal: 4.71s\tremaining: 4.86s\n",
      "123:\tlearn: 1777.2722457\ttest: 1823.8641489\tbest: 1823.8641489 (123)\ttotal: 4.75s\tremaining: 4.83s\n",
      "124:\tlearn: 1776.5593018\ttest: 1823.6350758\tbest: 1823.6350758 (124)\ttotal: 4.79s\tremaining: 4.79s\n",
      "125:\tlearn: 1775.9888738\ttest: 1823.5823622\tbest: 1823.5823622 (125)\ttotal: 4.82s\tremaining: 4.75s\n",
      "126:\tlearn: 1775.6577984\ttest: 1823.4557202\tbest: 1823.4557202 (126)\ttotal: 4.86s\tremaining: 4.71s\n",
      "127:\tlearn: 1774.2994164\ttest: 1822.2699434\tbest: 1822.2699434 (127)\ttotal: 4.9s\tremaining: 4.67s\n",
      "128:\tlearn: 1773.8427280\ttest: 1822.1291999\tbest: 1822.1291999 (128)\ttotal: 4.94s\tremaining: 4.63s\n",
      "129:\tlearn: 1773.0980814\ttest: 1821.8980134\tbest: 1821.8980134 (129)\ttotal: 4.97s\tremaining: 4.59s\n",
      "130:\tlearn: 1772.6616180\ttest: 1821.6307939\tbest: 1821.6307939 (130)\ttotal: 5s\tremaining: 4.54s\n",
      "131:\tlearn: 1771.5015818\ttest: 1820.8608763\tbest: 1820.8608763 (131)\ttotal: 5.04s\tremaining: 4.51s\n",
      "132:\tlearn: 1770.1384649\ttest: 1819.3159819\tbest: 1819.3159819 (132)\ttotal: 5.08s\tremaining: 4.47s\n",
      "133:\tlearn: 1769.2611989\ttest: 1818.9176137\tbest: 1818.9176137 (133)\ttotal: 5.11s\tremaining: 4.42s\n",
      "134:\tlearn: 1768.9261802\ttest: 1818.8901894\tbest: 1818.8901894 (134)\ttotal: 5.14s\tremaining: 4.38s\n",
      "135:\tlearn: 1767.9550008\ttest: 1818.1154045\tbest: 1818.1154045 (135)\ttotal: 5.17s\tremaining: 4.34s\n",
      "136:\tlearn: 1766.8041614\ttest: 1817.5767132\tbest: 1817.5767132 (136)\ttotal: 5.2s\tremaining: 4.29s\n",
      "137:\tlearn: 1766.1535219\ttest: 1817.0878414\tbest: 1817.0878414 (137)\ttotal: 5.25s\tremaining: 4.26s\n",
      "138:\tlearn: 1765.7468871\ttest: 1816.9987477\tbest: 1816.9987477 (138)\ttotal: 5.29s\tremaining: 4.22s\n",
      "139:\tlearn: 1764.7258978\ttest: 1816.6287872\tbest: 1816.6287872 (139)\ttotal: 5.33s\tremaining: 4.18s\n",
      "140:\tlearn: 1763.8359133\ttest: 1816.3507469\tbest: 1816.3507469 (140)\ttotal: 5.36s\tremaining: 4.14s\n",
      "141:\tlearn: 1762.7391725\ttest: 1815.4715831\tbest: 1815.4715831 (141)\ttotal: 5.4s\tremaining: 4.11s\n",
      "142:\tlearn: 1761.6096957\ttest: 1814.5560675\tbest: 1814.5560675 (142)\ttotal: 5.43s\tremaining: 4.06s\n",
      "143:\tlearn: 1761.3644324\ttest: 1814.3443275\tbest: 1814.3443275 (143)\ttotal: 5.47s\tremaining: 4.03s\n",
      "144:\tlearn: 1760.4303754\ttest: 1813.5373669\tbest: 1813.5373669 (144)\ttotal: 5.51s\tremaining: 3.99s\n",
      "145:\tlearn: 1759.5686562\ttest: 1812.9995994\tbest: 1812.9995994 (145)\ttotal: 5.54s\tremaining: 3.95s\n",
      "146:\tlearn: 1758.9242709\ttest: 1812.6075244\tbest: 1812.6075244 (146)\ttotal: 5.58s\tremaining: 3.91s\n",
      "147:\tlearn: 1758.4648878\ttest: 1812.3280293\tbest: 1812.3280293 (147)\ttotal: 5.63s\tremaining: 3.88s\n",
      "148:\tlearn: 1758.1151424\ttest: 1811.9275141\tbest: 1811.9275141 (148)\ttotal: 5.66s\tremaining: 3.83s\n",
      "149:\tlearn: 1757.9907739\ttest: 1811.7588370\tbest: 1811.7588370 (149)\ttotal: 5.7s\tremaining: 3.8s\n",
      "150:\tlearn: 1757.5097507\ttest: 1811.8315873\tbest: 1811.7588370 (149)\ttotal: 5.74s\tremaining: 3.76s\n",
      "151:\tlearn: 1757.0478905\ttest: 1811.6493389\tbest: 1811.6493389 (151)\ttotal: 5.78s\tremaining: 3.73s\n",
      "152:\tlearn: 1756.5297400\ttest: 1811.2030477\tbest: 1811.2030477 (152)\ttotal: 5.82s\tremaining: 3.69s\n",
      "153:\tlearn: 1755.7351163\ttest: 1810.7572356\tbest: 1810.7572356 (153)\ttotal: 5.86s\tremaining: 3.65s\n",
      "154:\tlearn: 1755.2715933\ttest: 1810.3132052\tbest: 1810.3132052 (154)\ttotal: 5.89s\tremaining: 3.61s\n",
      "155:\tlearn: 1754.7264072\ttest: 1810.2185744\tbest: 1810.2185744 (155)\ttotal: 5.93s\tremaining: 3.57s\n",
      "156:\tlearn: 1753.9755789\ttest: 1809.8305443\tbest: 1809.8305443 (156)\ttotal: 5.96s\tremaining: 3.53s\n",
      "157:\tlearn: 1753.0666855\ttest: 1808.9981377\tbest: 1808.9981377 (157)\ttotal: 6s\tremaining: 3.49s\n",
      "158:\tlearn: 1752.8173443\ttest: 1808.8858239\tbest: 1808.8858239 (158)\ttotal: 6.04s\tremaining: 3.46s\n",
      "159:\tlearn: 1751.8510584\ttest: 1808.4005257\tbest: 1808.4005257 (159)\ttotal: 6.06s\tremaining: 3.41s\n",
      "160:\tlearn: 1751.4279032\ttest: 1808.1767986\tbest: 1808.1767986 (160)\ttotal: 6.09s\tremaining: 3.37s\n",
      "161:\tlearn: 1750.7553823\ttest: 1807.8536814\tbest: 1807.8536814 (161)\ttotal: 6.13s\tremaining: 3.33s\n",
      "162:\tlearn: 1750.0752004\ttest: 1807.2701224\tbest: 1807.2701224 (162)\ttotal: 6.16s\tremaining: 3.29s\n",
      "163:\tlearn: 1749.2515440\ttest: 1806.7064154\tbest: 1806.7064154 (163)\ttotal: 6.18s\tremaining: 3.24s\n",
      "164:\tlearn: 1748.8260817\ttest: 1806.6715394\tbest: 1806.6715394 (164)\ttotal: 6.21s\tremaining: 3.2s\n",
      "165:\tlearn: 1748.1962955\ttest: 1806.5451673\tbest: 1806.5451673 (165)\ttotal: 6.24s\tremaining: 3.16s\n",
      "166:\tlearn: 1747.8264298\ttest: 1806.5679953\tbest: 1806.5451673 (165)\ttotal: 6.28s\tremaining: 3.12s\n",
      "167:\tlearn: 1747.2957405\ttest: 1806.3039206\tbest: 1806.3039206 (167)\ttotal: 6.32s\tremaining: 3.08s\n",
      "168:\tlearn: 1746.9197337\ttest: 1806.0828070\tbest: 1806.0828070 (168)\ttotal: 6.35s\tremaining: 3.04s\n",
      "169:\tlearn: 1746.0783971\ttest: 1805.1950276\tbest: 1805.1950276 (169)\ttotal: 6.38s\tremaining: 3s\n",
      "170:\tlearn: 1745.3401623\ttest: 1804.9098738\tbest: 1804.9098738 (170)\ttotal: 6.42s\tremaining: 2.97s\n",
      "171:\tlearn: 1744.7451593\ttest: 1804.2778004\tbest: 1804.2778004 (171)\ttotal: 6.46s\tremaining: 2.93s\n",
      "172:\tlearn: 1744.4493221\ttest: 1804.0572379\tbest: 1804.0572379 (172)\ttotal: 6.48s\tremaining: 2.88s\n",
      "173:\tlearn: 1744.0333249\ttest: 1803.6036331\tbest: 1803.6036331 (173)\ttotal: 6.53s\tremaining: 2.85s\n",
      "174:\tlearn: 1743.7767017\ttest: 1803.7080148\tbest: 1803.6036331 (173)\ttotal: 6.57s\tremaining: 2.81s\n",
      "175:\tlearn: 1743.2953457\ttest: 1803.8376470\tbest: 1803.6036331 (173)\ttotal: 6.61s\tremaining: 2.78s\n",
      "176:\tlearn: 1742.7041232\ttest: 1803.6113066\tbest: 1803.6036331 (173)\ttotal: 6.64s\tremaining: 2.74s\n",
      "177:\tlearn: 1742.4402024\ttest: 1803.5599635\tbest: 1803.5599635 (177)\ttotal: 6.68s\tremaining: 2.7s\n",
      "178:\tlearn: 1742.0785289\ttest: 1803.3786925\tbest: 1803.3786925 (178)\ttotal: 6.73s\tremaining: 2.67s\n",
      "179:\tlearn: 1741.3194638\ttest: 1802.9691356\tbest: 1802.9691356 (179)\ttotal: 6.77s\tremaining: 2.63s\n",
      "180:\tlearn: 1740.9884564\ttest: 1802.8206316\tbest: 1802.8206316 (180)\ttotal: 6.8s\tremaining: 2.59s\n",
      "181:\tlearn: 1740.6576175\ttest: 1802.7217413\tbest: 1802.7217413 (181)\ttotal: 6.84s\tremaining: 2.56s\n",
      "182:\tlearn: 1740.1614519\ttest: 1802.3940419\tbest: 1802.3940419 (182)\ttotal: 6.89s\tremaining: 2.52s\n",
      "183:\tlearn: 1740.0622786\ttest: 1802.4009917\tbest: 1802.3940419 (182)\ttotal: 6.93s\tremaining: 2.49s\n",
      "184:\tlearn: 1739.3800291\ttest: 1802.1308324\tbest: 1802.1308324 (184)\ttotal: 6.97s\tremaining: 2.45s\n",
      "185:\tlearn: 1738.8058815\ttest: 1801.9674437\tbest: 1801.9674437 (185)\ttotal: 7.01s\tremaining: 2.41s\n",
      "186:\tlearn: 1738.4300491\ttest: 1801.7302984\tbest: 1801.7302984 (186)\ttotal: 7.04s\tremaining: 2.37s\n",
      "187:\tlearn: 1737.3928731\ttest: 1801.1879506\tbest: 1801.1879506 (187)\ttotal: 7.08s\tremaining: 2.33s\n",
      "188:\tlearn: 1736.7675520\ttest: 1800.6921702\tbest: 1800.6921702 (188)\ttotal: 7.12s\tremaining: 2.3s\n",
      "189:\tlearn: 1736.1011968\ttest: 1800.2456422\tbest: 1800.2456422 (189)\ttotal: 7.16s\tremaining: 2.26s\n",
      "190:\tlearn: 1735.1998535\ttest: 1799.4568851\tbest: 1799.4568851 (190)\ttotal: 7.19s\tremaining: 2.22s\n",
      "191:\tlearn: 1735.1640600\ttest: 1799.4429815\tbest: 1799.4429815 (191)\ttotal: 7.23s\tremaining: 2.18s\n",
      "192:\tlearn: 1734.2071816\ttest: 1799.0006485\tbest: 1799.0006485 (192)\ttotal: 7.27s\tremaining: 2.15s\n",
      "193:\tlearn: 1733.4300870\ttest: 1798.2879754\tbest: 1798.2879754 (193)\ttotal: 7.3s\tremaining: 2.11s\n",
      "194:\tlearn: 1732.5355379\ttest: 1797.9879888\tbest: 1797.9879888 (194)\ttotal: 7.38s\tremaining: 2.08s\n",
      "195:\tlearn: 1732.3118155\ttest: 1797.9383329\tbest: 1797.9383329 (195)\ttotal: 7.43s\tremaining: 2.05s\n",
      "196:\tlearn: 1731.9847570\ttest: 1797.9886064\tbest: 1797.9383329 (195)\ttotal: 7.47s\tremaining: 2.01s\n",
      "197:\tlearn: 1731.6571421\ttest: 1797.8801050\tbest: 1797.8801050 (197)\ttotal: 7.5s\tremaining: 1.97s\n",
      "198:\tlearn: 1731.3978998\ttest: 1797.8150504\tbest: 1797.8150504 (198)\ttotal: 7.53s\tremaining: 1.93s\n",
      "199:\tlearn: 1730.9538688\ttest: 1797.7718906\tbest: 1797.7718906 (199)\ttotal: 7.57s\tremaining: 1.89s\n",
      "200:\tlearn: 1730.5873175\ttest: 1797.7010068\tbest: 1797.7010068 (200)\ttotal: 7.6s\tremaining: 1.85s\n",
      "201:\tlearn: 1729.9289871\ttest: 1797.2364887\tbest: 1797.2364887 (201)\ttotal: 7.63s\tremaining: 1.81s\n",
      "202:\tlearn: 1729.5705157\ttest: 1797.0245922\tbest: 1797.0245922 (202)\ttotal: 7.66s\tremaining: 1.77s\n",
      "203:\tlearn: 1729.3760156\ttest: 1796.9465336\tbest: 1796.9465336 (203)\ttotal: 7.7s\tremaining: 1.74s\n",
      "204:\tlearn: 1728.7330696\ttest: 1796.4126561\tbest: 1796.4126561 (204)\ttotal: 7.73s\tremaining: 1.7s\n",
      "205:\tlearn: 1728.1215617\ttest: 1795.9836177\tbest: 1795.9836177 (205)\ttotal: 7.76s\tremaining: 1.66s\n",
      "206:\tlearn: 1727.5915529\ttest: 1795.5560826\tbest: 1795.5560826 (206)\ttotal: 7.8s\tremaining: 1.62s\n",
      "207:\tlearn: 1727.3319325\ttest: 1795.2772246\tbest: 1795.2772246 (207)\ttotal: 7.84s\tremaining: 1.58s\n",
      "208:\tlearn: 1727.0104981\ttest: 1794.9680055\tbest: 1794.9680055 (208)\ttotal: 7.88s\tremaining: 1.54s\n",
      "209:\tlearn: 1726.6285670\ttest: 1794.7644851\tbest: 1794.7644851 (209)\ttotal: 7.9s\tremaining: 1.5s\n",
      "210:\tlearn: 1726.2916231\ttest: 1794.4825103\tbest: 1794.4825103 (210)\ttotal: 7.93s\tremaining: 1.47s\n",
      "211:\tlearn: 1726.0766489\ttest: 1794.2804521\tbest: 1794.2804521 (211)\ttotal: 7.96s\tremaining: 1.43s\n",
      "212:\tlearn: 1725.7945585\ttest: 1794.1860757\tbest: 1794.1860757 (212)\ttotal: 8s\tremaining: 1.39s\n",
      "213:\tlearn: 1725.3298159\ttest: 1794.0430332\tbest: 1794.0430332 (213)\ttotal: 8.04s\tremaining: 1.35s\n",
      "214:\tlearn: 1724.9272082\ttest: 1793.8830319\tbest: 1793.8830319 (214)\ttotal: 8.06s\tremaining: 1.31s\n",
      "215:\tlearn: 1723.9868309\ttest: 1793.1071735\tbest: 1793.1071735 (215)\ttotal: 8.09s\tremaining: 1.27s\n",
      "216:\tlearn: 1723.5527665\ttest: 1792.9512947\tbest: 1792.9512947 (216)\ttotal: 8.13s\tremaining: 1.24s\n",
      "217:\tlearn: 1723.3817879\ttest: 1792.9674738\tbest: 1792.9512947 (216)\ttotal: 8.17s\tremaining: 1.2s\n",
      "218:\tlearn: 1722.9000051\ttest: 1792.6736553\tbest: 1792.6736553 (218)\ttotal: 8.2s\tremaining: 1.16s\n",
      "219:\tlearn: 1722.3415601\ttest: 1792.5461381\tbest: 1792.5461381 (219)\ttotal: 8.24s\tremaining: 1.12s\n",
      "220:\tlearn: 1722.1255541\ttest: 1792.5208179\tbest: 1792.5208179 (220)\ttotal: 8.28s\tremaining: 1.09s\n",
      "221:\tlearn: 1721.6753704\ttest: 1792.4196465\tbest: 1792.4196465 (221)\ttotal: 8.31s\tremaining: 1.05s\n",
      "222:\tlearn: 1721.2328941\ttest: 1792.1889174\tbest: 1792.1889174 (222)\ttotal: 8.34s\tremaining: 1.01s\n",
      "223:\tlearn: 1720.7525377\ttest: 1791.8601482\tbest: 1791.8601482 (223)\ttotal: 8.37s\tremaining: 971ms\n",
      "224:\tlearn: 1720.6543164\ttest: 1791.6541348\tbest: 1791.6541348 (224)\ttotal: 8.4s\tremaining: 934ms\n",
      "225:\tlearn: 1720.2829231\ttest: 1791.1682718\tbest: 1791.1682718 (225)\ttotal: 8.43s\tremaining: 896ms\n",
      "226:\tlearn: 1719.9913339\ttest: 1791.1234005\tbest: 1791.1234005 (226)\ttotal: 8.46s\tremaining: 857ms\n",
      "227:\tlearn: 1719.4643663\ttest: 1790.6446892\tbest: 1790.6446892 (227)\ttotal: 8.49s\tremaining: 819ms\n",
      "228:\tlearn: 1719.0624892\ttest: 1790.4139639\tbest: 1790.4139639 (228)\ttotal: 8.52s\tremaining: 782ms\n",
      "229:\tlearn: 1718.6876106\ttest: 1790.1772859\tbest: 1790.1772859 (229)\ttotal: 8.56s\tremaining: 744ms\n",
      "230:\tlearn: 1718.1304124\ttest: 1790.0498983\tbest: 1790.0498983 (230)\ttotal: 8.58s\tremaining: 706ms\n",
      "231:\tlearn: 1717.9559005\ttest: 1789.9476562\tbest: 1789.9476562 (231)\ttotal: 8.62s\tremaining: 669ms\n",
      "232:\tlearn: 1717.8465063\ttest: 1789.8824270\tbest: 1789.8824270 (232)\ttotal: 8.66s\tremaining: 632ms\n",
      "233:\tlearn: 1717.5609453\ttest: 1789.7861187\tbest: 1789.7861187 (233)\ttotal: 8.7s\tremaining: 595ms\n",
      "234:\tlearn: 1717.1746039\ttest: 1789.7331800\tbest: 1789.7331800 (234)\ttotal: 8.74s\tremaining: 558ms\n",
      "235:\tlearn: 1717.0444439\ttest: 1789.7218524\tbest: 1789.7218524 (235)\ttotal: 8.77s\tremaining: 520ms\n",
      "236:\tlearn: 1716.5379091\ttest: 1789.4155806\tbest: 1789.4155806 (236)\ttotal: 8.8s\tremaining: 483ms\n",
      "237:\tlearn: 1716.4359810\ttest: 1789.4381596\tbest: 1789.4155806 (236)\ttotal: 8.83s\tremaining: 445ms\n",
      "238:\tlearn: 1716.1539196\ttest: 1789.3860246\tbest: 1789.3860246 (238)\ttotal: 8.86s\tremaining: 408ms\n",
      "239:\tlearn: 1716.0526872\ttest: 1789.3331466\tbest: 1789.3331466 (239)\ttotal: 8.89s\tremaining: 370ms\n",
      "240:\tlearn: 1715.7399487\ttest: 1789.5324786\tbest: 1789.3331466 (239)\ttotal: 8.94s\tremaining: 334ms\n",
      "241:\tlearn: 1715.2183090\ttest: 1789.4456237\tbest: 1789.3331466 (239)\ttotal: 8.98s\tremaining: 297ms\n",
      "242:\tlearn: 1715.0105147\ttest: 1789.4164709\tbest: 1789.3331466 (239)\ttotal: 9.01s\tremaining: 260ms\n",
      "243:\tlearn: 1714.9569853\ttest: 1789.3172284\tbest: 1789.3172284 (243)\ttotal: 9.05s\tremaining: 223ms\n",
      "244:\tlearn: 1714.7661931\ttest: 1789.2904702\tbest: 1789.2904702 (244)\ttotal: 9.1s\tremaining: 186ms\n",
      "245:\tlearn: 1714.4948227\ttest: 1789.1292316\tbest: 1789.1292316 (245)\ttotal: 9.13s\tremaining: 148ms\n",
      "246:\tlearn: 1714.0552516\ttest: 1788.8601065\tbest: 1788.8601065 (246)\ttotal: 9.17s\tremaining: 111ms\n",
      "247:\tlearn: 1713.5400731\ttest: 1788.4249282\tbest: 1788.4249282 (247)\ttotal: 9.2s\tremaining: 74.2ms\n",
      "248:\tlearn: 1713.0763009\ttest: 1788.2203399\tbest: 1788.2203399 (248)\ttotal: 9.24s\tremaining: 37.1ms\n",
      "249:\tlearn: 1712.6629561\ttest: 1787.9443778\tbest: 1787.9443778 (249)\ttotal: 9.28s\tremaining: 0us\n",
      "\n",
      "bestTest = 1787.944378\n",
      "bestIteration = 249\n",
      "\n",
      "RMSE score: 1787.94\n",
      "Training time: 9.57s\n",
      "Prediction time: 0.05s\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "params_cb = {\n",
    "    'iterations': 250,\n",
    "    'cat_features': cat_features,\n",
    "    'random_state': 12345\n",
    "}\n",
    "\n",
    "print('CatBoost:')\n",
    "\n",
    "evaluate_model(\n",
    "    CatBoostRegressor,\n",
    "    features_train,\n",
    "    target_train,\n",
    "    features_valid,\n",
    "    target_valid,\n",
    "    params_cb,\n",
    "    eval_set=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=1; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=1; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=1; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=5; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=5; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=10; total time=   0.6s\n",
      "[CV] END learning_rate=0.01, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=1; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=1; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=1; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=5; total time=   0.4s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=1; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=1; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=1; total time=   0.6s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.05, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END .learning_rate=0.1, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, max_depth=3, min_child_weight=1; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, max_depth=3, min_child_weight=5; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=10; total time=   0.2s\n",
      "[CV] END .learning_rate=0.1, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END .learning_rate=0.1, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END .learning_rate=0.1, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END .learning_rate=0.1, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END .learning_rate=0.1, max_depth=6, min_child_weight=1; total time=   0.3s\n",
      "[CV] END .learning_rate=0.1, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END .learning_rate=0.1, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END .learning_rate=0.1, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END .learning_rate=0.1, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END .learning_rate=0.1, max_depth=6, min_child_weight=5; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END learning_rate=0.1, max_depth=6, min_child_weight=10; total time=   0.3s\n",
      "[CV] END .learning_rate=0.1, max_depth=9, min_child_weight=1; total time=   0.6s\n",
      "[CV] END .learning_rate=0.1, max_depth=9, min_child_weight=1; total time=   0.5s\n",
      "[CV] END .learning_rate=0.1, max_depth=9, min_child_weight=1; total time=   1.1s\n",
      "[CV] END .learning_rate=0.1, max_depth=9, min_child_weight=1; total time=   0.5s\n",
      "[CV] END .learning_rate=0.1, max_depth=9, min_child_weight=1; total time=   0.5s\n",
      "[CV] END .learning_rate=0.1, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END .learning_rate=0.1, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END .learning_rate=0.1, max_depth=9, min_child_weight=5; total time=   0.6s\n",
      "[CV] END .learning_rate=0.1, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END .learning_rate=0.1, max_depth=9, min_child_weight=5; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "[CV] END learning_rate=0.1, max_depth=9, min_child_weight=10; total time=   0.5s\n",
      "Best RMSE score: 1744.93\n",
      "Best params: {'learning_rate': 0.1, 'max_depth': 9, 'min_child_weight': 1}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid_xb = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'min_child_weight': [1, 5, 10]\n",
    "}\n",
    "\n",
    "model_xg = xgb.XGBRegressor()\n",
    "\n",
    "print('XGBoost:')\n",
    "\n",
    "tune_hyperparameters(model_xg, param_grid_xb, features_train_ord, target_train_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:\n",
      "RMSE score: 1961.57\n",
      "Training time: 0.68s\n",
      "Prediction time: 0.04s\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "params_xg = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 1\n",
    "}\n",
    "\n",
    "print('XGBoost:')\n",
    "\n",
    "evaluate_model(\n",
    "    xgb.XGBRegressor,\n",
    "    features_train_ohe,\n",
    "    target_train_ohe,\n",
    "    features_valid_ohe,\n",
    "    target_valid_ohe,\n",
    "    params_xg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our gradient boosting models, we see the best RMSE score with LightGBM. CatBoost had the second best RMSE score but took the longest time to train.\n",
    "\n",
    "To cut down on training time in general, we made the decision to exclude certain hyperparameters from tuning and testing for this project. E.g. for LightGBM, we did not tune for things like `n_estimators`; for CatBoost we did not tune for things like `learning_rate` or `depth`; and for XGBoost, we did not tune for things like `n_estimators`, `learning_rate`, `max_depth`.\n",
    "\n",
    "Hopefully this does not negatively impact the quality of our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regression model RMSE score: 2032.78\n",
      "Training time: 9.53s\n",
      "Prediction time: 0.17s\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "start_time = time.time()\n",
    "\n",
    "best_model_rf = RandomForestRegressor(max_depth=10, n_estimators=50, random_state=12345)\n",
    "\n",
    "best_model_rf.fit(features_train_ord, target_train_ord)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "start_time_predict = time.time()\n",
    "\n",
    "pred_test_rf = best_model_rf.predict(features_test_ord)\n",
    "\n",
    "end_time_predict = time.time()\n",
    "prediction_time = end_time_predict - start_time_predict\n",
    "\n",
    "print(f'Best regression model RMSE score: {rmse(target_test_ord, pred_test_rf):.2f}')\n",
    "print(f'Training time: {training_time:.2f}s')\n",
    "print(f'Prediction time: {prediction_time:.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the best regression model using random forest and incorporate the test set. The results are similar to our initial evaluation of the model, having the best quality of the prediction. We again note that the speed is the worst, the only downside to this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 212361, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 4409.028032\n",
      "Best gradient boosting model RMSE score: 1762.49\n",
      "Training time: 0.83s\n",
      "Prediction time: 0.09s\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "start_time = time.time()\n",
    "\n",
    "best_model_lgbm = lgb.LGBMRegressor(learning_rate=0.1, max_depth=15, num_leaves=50)\n",
    "\n",
    "best_model_lgbm.fit(features_train_ord, target_train_ord, categorical_feature=cat_features)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "start_time_predict = time.time()\n",
    "\n",
    "pred_test_lgbm = best_model_lgbm.predict(features_test_ord)\n",
    "\n",
    "end_time_predict = time.time()\n",
    "prediction_time = end_time_predict - start_time_predict\n",
    "\n",
    "print(f'Best gradient boosting model RMSE score: {rmse(target_test_ord, pred_test_lgbm):.2f}')\n",
    "print(f'Training time: {training_time:.2f}s')\n",
    "print(f'Prediction time: {prediction_time:.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the best gradient boosting model using LightGBM and incorporate the test set. We note that both the quality via RMSE score and speed for this model is better than our best regression model score using random forest. In choosing between the two, we would recommend LightGBM as the option for our client to go with for predicting car values based on its overall performance.\n",
    "\n",
    "In a future follow-up, we would want to retrain the gradient boosting models with more thorough hyperparameter tuning, as we had to make exclusions for this project to mitigate time and speed of training, and see if we're able to reproduce results and come to the same conclusions (as well as compare against our regression models again)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
